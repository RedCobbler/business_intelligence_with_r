# PARKING LOT



```
tf = tempfile()
download.file("http://archive.ics.uci.edu/ml/machine-learning-databases/00275
  /Bike-Sharing-Dataset.zip", tf)
bike_share_daily_raw = read.table(unz(tf, "day.csv"), header=TRUE, sep=",")
file.remove(tf)
write.table(bike_share_daily_raw, “bike_share_daily.csv”, row.names=FALSE)
rm(bike_share_daily_raw)
```

```
bike_share_daily=read.table("bike_share_daily.csv", sep=",", header=T, 
  colClasses=c("character", "Date", "factor", "factor", "factor", "factor", 
  "factor", "factor", "factor", "numeric", "numeric", "numeric", "numeric",
  "integer", "integer", "integer"))
levels(bike_share_daily$season) = c("Winter", "Spring", "Summer", "Fall")
levels(bike_share_daily$workingday) = c("No", "Yes")
levels(bike_share_daily$holiday) = c("No", "Yes")
bike_share_daily$mnth = as.factor(as.numeric(bike_share_daily$mnth))
levels(bike_share_daily$mnth) = c(month.abb)
levels(bike_share_daily$yr) = c(2011, 2012)
```



##Fake data creation

## Creating fake data to test code
[KEEP? DELETE?]

Creating fake data is useful when you want to test how code will work before doing it "for real" on larger, more complicated datasets. We'll create a few fake dataframes in this recipe as both an example as how to do it, as well as for use in other recipes in this book.  

While creating and using fake data is useful for lots of cases, it is especially useful for merging/joining (which we'll explore in the next few recipes). Sometimes joins may not behave like you expect that they will due to intricacies in your data, so testing it first on known (fake) data helps you determine whether any problems arose because of the code, or because of something in your data. If the fake data merge as expected but the real data don't, then you know that there's something in your data (and not the code!) that is messing with the join process. In large, enterprise-scale databases, it is typically the joins that can cause the most unexpected behavior—that is, can lead to the wrong results, impacting everything downstream of that join for the worse.

Whatever the use case, testing code on fake data can sometimes save considerable time that would have been lost debugging.

Using a variety of sources, we'll develop four dataframes that mimic a customer sales type of database (of course, these datasets are all small, so a database is unnecessary here).

```
# Table A – customer data
customer_id = seq(1:10000)
customer_gender = sample(c("M","F", "Unknown"), 10000, replace=TRUE,
  prob=c(.45, .45, .10))
customer_age = round(runif(10000, 18, 88), 0)
customer_purchases = round(rlnorm(10000)*100, 2)
  
# Table B – city/state/zip and business density lookup table
set.seed(1235813)
temp = tempfile()
download.file("ftp://ftp.census.gov/econ2012/CBP_CSV/zbp12totals.zip", temp)
zipcodes = read.table(unz(temp, "zbp12totals.txt"), header=T, quote="\"",
  sep=",", colClasses="character")
unlink(temp)
zipcodes = zipcodes[,c(1,11:12,10)]
# End of Table B
  
# Table A, continued
customer_zip = zipcodes[sample(1:nrow(zipcodes), 10000, replace=TRUE),]$zip
customers = data.frame(customer_id, customer_gender, customer_age,
  customer_purchases, customer_zip)
  
# TABLE C – results of a product interest survey
ones = seq(1, 1, length.out = 2000)
zeros = seq(0, 0, length.out = 2000)
strongly_agree = c(ones, zeros, zeros, zeros, zeros)
agree = c(zeros, ones, zeros, zeros, zeros)
neutral = c(zeros, zeros, ones, zeros, zeros)
disagree = c(zeros, zeros, zeros, ones, zeros)
strongly_disagree = c(zeros, zeros, zeros, zeros, ones)
survey = data.frame(customer_id, strongly_agree, agree, neutral, disagree,
  strongly_disagree)
  
# TABLE D – lookup table to match states to regions
state_regions = data.frame(datasets::state.abb, datasets::state.name, 
  datasets::state.region, datasets::state.division, colnames=c("abb", "state", 
  "region", "division"))
```

In Table A, we've created a customer demographic and sales dataset. The `sample` function creates a random vector of male of female genders, as well as unknown, proportional to the weights in the `prob` option. The age and purchase values were created using the random number generators based on the uniform and log-normal distributions, respectively. Each customer's zip code was generated from a random selection of the zip codes in Table B.

Table B, we've downloaded and unzipped some 2012 County Business Patterns data from the US Census to create a "locations" lookup table. (Metadata for this dataset is online at https://www.census.gov/econ/cbp/download/noise_layout/ZIP_Totals_Layout10.txt; of relevance to this recipe, in addition to the city/state/zip fields, we've also kept the est field, which is the number of business establishments in this zip code in the first quarter of 2012.)

Table C is just a systematic selection designating a survey response from each customer.

Finally, Table D takes advantage of R's built-in data from the datasets package to generate a lookup table of states and state regions.




##Predictive Modeling set up

## Partitioning the data efficiently for predictive modeling

There's still a fair amount of controversy over whether splitting the dataset into training/test sets (or training/test/validation), bootstrapping, or cross-validation is the best approach to effective predictive modeling. The smaller the sample size, the more difficult the choice becomes, and experience and industry norms become more important in determining which approach works best.

In this recipe, we'll look at setting up partitions; later, we'll see cross-validation in action as part of the modeling process. Of course, if your aim is explanation (and not prediction), this tool is moot: you'll want to use the entire dataset.

Max Kuhn's caret package is a wonderful tool covering basically the whole suite of predictive modeling needs in R. We'll see it later in this book, starting here to select a stratified random sample. We'll also use the base package's functions for a simple random sample. For both approaches, we'll continue to use the fake customer data.

```
require(caret)
```

For simple random sampling, sample and setdiff allow us to split a (large) dataset into partitions of 60\% training, 20\% test, and 20\% validation (for example, based on Andrew Ng's CS229 recommendation):

```
observations = nrow(customer_survey)
customer_partitions = sample(nrow(customer_survey), 0.60 * observations)
test_set = sample(setdiff(seq_len(nrow(customer_survey)), customer_partitions),
  0.20 * observations)
validation_set = setdiff(setdiff(seq_len(nrow(customer_survey)),
  customer_partitions), test_set)
customer_survey_training = customer_survey[customer_partitions, ]
customer_survey_test = customer_survey[test_set,]
customer_survey_validation = customer_survey[validation_set,]

```

If you want to ensure samples from each region are more or less evenly represented in each partition, you can use caret's createDataPartition function twice, and specify the variable to stratify on:

```
customer_partitions = createDataPartition(customer_survey$division, p = 0.60,
  list = FALSE, times = 1)
customer_survey_training = customer_survey[customer_partitions,]
customer_survey_testing = customer_survey[-customer_partitions,]
customer_test_partitions = createDataPartition(customer_survey_testing$division,
  p = 0.50, list = FALSE, times = 1)
customer_survey_test = customer_survey_testing[customer_test_partitions,]
customer_survey_validation = customer_survey_testing[-customer_test_partitions,]
rm(customer_survey_testing)

```

When events or distributions of the outcomes of interest are skewed and/or not evenly distributed (which can be most of the time!), you'll want to use stratified random sampling. The createDataPartition function in the caret package can do this for you.

To make an 80/20 split, you can adjust the value of 0.60 in the code to 0.80, ignore or delete the lines of code referencing a validation set, tweak the test set code, and run it, i.e.,

```
customer_partitions = sample(nrow(customer_survey), 0.80 * observations)
# or
customer_partitions = createDataPartition(customer_survey$division, p = 0.60,
  list = FALSE, times = 1)
```

and then
```
customer_survey_training = customer_survey[customer_partitions,]
customer_survey_testing = customer_survey[-customer_partitions,]
```

## Predictive modeling with Rattle

There are other all-in-one predictive modeling packages in R, of course, but perhaps the most useful among those is Graham Williams' rattle, which provides a GUI-based approach to the major predictive modeling techniques. A great feature is that each executed GUI choice is recorded in the log, allowing you to see the R commands that run the package behind the scenes. In fact, I purposely based the simple random sample code used above on how rattle does it; after loading data into the rattle GUI and executing a partition, you can look at the log tab to see the similarities in the code.

As rattle has an outstanding book (Williams 2011) as well as great online documentation (http://rattle.togaware.com/) and tutorials (http://handsondatascience.com), there's no need to cover its use in this book. But I highly recommend using it no matter what your level of experience with either predictive modeling or with other R packages—it could be all you need for the majority of your work.
