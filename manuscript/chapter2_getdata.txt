# Chapter 2: Getting Data

## Working with Flat Files

### Loading data from files

The tried-and-true method for bringing data into R is via the humble csv file, and in many cases it will be the default approach. But in most professional environments, you'll encounter a wide variety of file formats and production needs, so having a cheat sheet of data import methods is useful.

There are a wide variety of packages and functions to import data from files, but a few provide nearly all the functionality you'll need:

{width="85%"}
| File Type | Function | Package |
| --------- | -------- | ------- |
| csv file | `read.table("file", sep=",", ...)`  | base |
|  | `read.csv("file", ...)` |  | 
| Flat file | `read.table("file", sep=" ", ...)` | base |
|  | `fread("file", sep=" ", ...)` | data.table |
| Excel | `read_excel("Excelfile", sheet, ...)` | readxl |
|  | `read.xls("Excelfile",sheet, ...)` | gdata |
|  | `readWorksheet("Excelfile", sheet, ...)` | XLConnect |
| SPSS | `spss.get("SPSSfile.sav")` | Hmisc |
| Stata | `read.dta("Statafile.dta")` | foreign |
| SAS | `read.sas7bdat("SASfile.sas7bdat")` | sas7bdat |

#### Reading flat files from disk or the web

To import flat files, it's probably best to get in the habit of using read.table instead of read.csv. While comma-delimited files are indeed the most common, tab- and pipe-delimited can also be quite prevalent depending on the context. And, since `read.csv` is just a wrapper for `read.table`, you'll likely save time in the long run by using read.table for any flat file, and changing the `sep=" "` value as needed, such as `\t` for tab, `|` for pipe, and so on.

We saw reading in a .csv file in the first chapter:

```
vanco=read.table(
```

While reading in a local flat file is pretty standard fare for any R user, sometimes reading in online files can require a little extra work. This example reads in a csv file from the Canadian 2011 Public Service Employee Survey (PSES), using UTF-8 encoding to ensure special characters are read in correctly:

```
pses2011 = read.table("http://www.tbs-sct.gc.ca/pses-saff/2011/data/2011_results-resultats.csv", sep=",", encoding="UTF-8", header=FALSE)
```

#### Reading Excel files

To import Excel files, `readxl`, `gdata`, or `XLConnect` provide pretty much anything you'll need; `gdata` is faster and more powerful but requires Perl, so Windows users must ensure that's installed on their system before they can use it. `XLConnect` has a lot of options for those using Windows and/or are in Excel-heavy work environments, and while it can be slow with large files, it is based on Java so it could be easier to start with on Windows platforms (as long as the Java and R architectures match, i.e., if R is x64, Java should be as well). `readxl` is a new addition to the R ecosystem, and if all you need to do is pull in a worksheet as-is, it's by far the easiest method:

```
bob = read_xl(
```

The next example uses `gdata`: it imports the documentation file for the 2011 PSES dataset imported in the previous step, and since the sheet has non-data rows in it (at the bottom), we'll exclude those by using the nrows function. It also contains "multi-string" headers, so we'll use the `header`, `skip`, and `col.name` options to ignore their header and create our own:

```
pses2011_header = read.xls("http://www.tbs-sct.gc.ca/pses-saff/2011/data/PSES2011_Documentation.xls", sheet="Layout-Format", nrows=22, header=FALSE, skip=1, col.names=c("Variables", "Size", "Type", "Description"), stringsAsFactors=FALSE)
```

The `sheet=` option can use the sheet's name or its position, e.g., using `sheet=6` in the above example retrieves the same data.

The raw data we read in from a csv and the metadata we read in from an Excel file can be merged to create the header names using the `colnames` function:

```
colnames(pses2011) = pses2011_header$Variable
```

#### Creating a dataframe from the clipboard

A really useful short-cut to get spreadsheet or table data with a header row into R quickly is copying it into the clipboard and importing it via the "clipboard" option in the read.table function:

```
my_data_frame = read.table("clipboard", sep="\t", header=TRUE)
```

#### Another way to to read SAS files

If you have a SAS license, `Hmisc` provides another means to import SAS files with more options than the `sas7bdat` package:

```
sas.get(libraryName="directoryofsasfile", member="sasfile", sasprog="full/path/to/sas.exe")
```

Longtime SAS users might remember the datalines (or cards, if you're really an old-timer) method to create tables on the fly. While the clipboard method shown above is probably better in most cases, if you're used to using this old SAS method and you only need a quick, small, use-once table, the textConnection function could be handy to have in the toolbox:

```
marketing = read.table(textConnection
("Survey_Response, Send_Email_Ad
Strongly_Agree, Yes
Agree, Yes
Neutral, No
Disagree, No
Strongly_Disagree, No"),
header=TRUE, sep=",")
```

#### Reading big files with data.table

The `data.table` package is extremely useful—and much, much faster than read.table—for larger files.

```
require(data.table)
# If you want to read in a csv directly, use fread
pses2011 = fread("pses2011.csv", sep=",")
# If you already have a dataframe and want to speed up
# reading and accessing it, use data.table
pses2011 = data.table("pses2011")
```

#### Unzipping files into R

Larger files are often zipped up to save space and bandwidth. Setting up a temporary file and then using the `unz` function inside the usual `read.table` takes care of bringing it into R:

[SIMPLIFY THIS]

```
tf = tempfile()
download.file("ftp://ftp.census.gov/econ2012/CBP_CSV/zbp12totals.zip", tf)
zipcodes = read.table(unz(tf, "zbp12totals.txt"), header=T, quote="\"", sep=",", colClasses="character")
file.remove(tf)
```

If the zip file has more than one file in it, you can unzip them from within R in this way:

```
td = tempdir()
tf = tempfile(tmpdir=td, fileext=".zip")
download.file("http://archive.ics.uci.edu/ml/machine-learning-databases/00275/Bike-Sharing-Dataset.zip", tf)
unzip(tf, exdir=td)
bike_share_daily = read.table(unz(tf, "day.csv"), header=T, sep=",")
bike_share_hourly = read.table(unz(tf, "hour.csv"), header=T, sep=",")
bike_share_readme = readLines(unz(tf, "Readme.txt"))
file.remove(tf)
```

Note: although there is an `untar` function, it is no longer necessary—gzipped files can be read directly into R with the standard `read.table` function.  



## Working with Databases

### Connecting to a Database

While those who work with server-based databases will typically manipulate the data in those environments before importing into R, there are times when having a direct link to the database can be useful—for ongoing and/or automated reporting, for one. This recipe covers the basics of `RODBC`, one of the packages that allows you to use ODBC to connect R to your database. I've found the `sqldf` package (see the next few recipes) easier to use for working with database tables once you've connected, but as usual there are a variety of options in R—choose the one that works best for your workflow.

As there are dozens of database platforms, all with their own system-specific drivers, we won't cover Data Source Name (DSN) setup in this book. Your database admin and/or a search on stackoverflow can provide advice on setting up the proper drivers for your local system and adding system DSN connections. Once the appropriate DSNs are available for you, you can use `RODBC` package to access your databases:

```
require(RODBC)
```

Once your local system is configured for the ODBC connection, you need to define that connection within R, replacing the highlighted code with your specific values:

```
connect = odbcConnect("name_of_dsn_connection", uid="your_user_id", pwd="your_password")
# for example: connect = odbcConnect("name_of_dsn_connection", uid="your_user_id", pwd="your_password")
```

We can verify the connection is working:

```
odbcGetInfo(connect)
```

Once the connection is made, data can flow. If you don't already know the names of the tables or you want to verify their spelling, you can get a list by writing the list to an R object and then view that object:

```
sqlTables(connect)
# If there are a lot of tables or you want to make a record of
# their names, create an object and then view it within R:
connect_tables = sqlTables(connect)
connect_tables
```

`RODBC` will add a `$` to each table's name to designate it as a system file.

You can pull in an entire table using the `sqlFetch` function, but if you're using a database, you probably want to query it to get just what you need.

To get a whole table (don't forget the `$` at the end of the table name):

```
whole_table = sqlFetch(connect, "table_name$")
```

To make a SQL query and put it into a dataframe, set up the query in an R object and then use the `sqlQuery` function. The following query is identical to above as it brings in the whole table:

```
# Within the double quotes you can put a valid SQL query
# Using single quotes to name the table
query_1 = "SELECT * FROM 'table_name$'"
query_1_dataframe = sqlQuery(connect, query_1)
```

This query will pull in all columns but only those records specified by the where clause, and will sort it by the order by variable(s):

```
query_2 = "SELECT * FROM 'table_name$' 
  WHERE variable_name = some_condition 
  ORDER BY ordering_variable_name"
query_2_dataframe = sqlQuery(connect, query_2)
```

At the end of the session, close the database connection:

```
odbcCloseAll()
```

The `RODBC` package allows R to connect with any database that allows OBDC connections, and then interact with that database using SQL. Since database platforms, local systems, and server environments vary considerably, it very difficult to specify how to set up the initial DSN connection for any given combination—you'll need to consult with a database administrator or search online for your particular combination of databases, drivers, and system connections. 
You can learn more by viewing: `vignette("RODBC", package="RODBC")`.

### Creating a SQLite database inside R

You don't even need to understand the intricacies of database planning and management to be able to create your own database on the fly from within R. While there's no need to do so if your data fits into R's memory, it can be really useful if you need to subset and merge several large files to obtain a working dataframe that can be comfortably analyzed within R's memory limits.

This recipe walks through creating a SQLite database and adding tables to it; the following recipe will walk through obtaining data from the database for analysis in R.

The `sqldf` package provides more options for database work without dealing with a formal database—everything can be done from within R.

```
require(sqldf)
```


Creating a new database with sqldf is easy:

```
sqldf("attach 'PSES_database.sqlite' as new")
```

It's just a shell at the moment; next create a connection to that database for use in subsequent table import and manipulation:

```
connect = dbConnect(SQLite(), dbname="PSES_database.sqlite ")
```

And now you can read csv files directly into the database (i.e., so it is not throttled by passing through R first). We'll use the PSES 2011 file we created in the There's more... section of the Loading data from files recipe earlier in this chapter.

```
read.csv.sql("pses2011.csv", sql = "CREATE TABLE pses2011 AS SELECT * FROM file",
  dbname = "PSES_database.sqlite")
```

Because Excel files must be loaded into R first, and then passed to the database, if your Excel file has only a few sheets in it, saving them as csvs might be preferable. If you have many sheets to import, using  XLConnect to import the sheets all at once could save a little time. They'll still need to be loaded into the database individually, however, and you should remove the dataframes when you've transferred them to the database.

```
# This loads the Excel file into R
pses2011_xls = loadWorkbook("PSES2011_Documentation.xls")
  
# This reads each worksheet into separate dataframes within a list
pses2011_documentation = readWorksheet(pses2011_xls, 
  sheet=getSheets(pses2011_xls))
  
# This file was downloaded to the working directory from
# http://www.tbs-sct.gc.ca/pses-saff/2011/data/PSES2011_Documentation.xls
# The sheet names have spaces and hyphens, which will cause
# trouble for SQLite; run names(pses2011_documentation) to see
# So, this changes the dataframe names inside the list
names(pses2011_documentation) = c("Questions", "Agency", "LEVELID15", "Demcode",
  "PosNegSpecs", "LayoutFormat")
  
# Add a new row to account for 0 values for LEVEL1ID in
# main pses2011 file
pses2011_documentation$Agency = rbind(pses2011_documentation$Agency,
  c(0,NA,"Other",NA,"OTH"))
  
# Now each sheet can be loaded into the database as a separate table
with(pses2011_documentation, {
  dbWriteTable(conn=connect, name="Questions", value=Questions, row.names=FALSE)
  dbWriteTable(conn=connect, name="Agency", value=Agency, row.names=FALSE)
  dbWriteTable(conn=connect, name="LEVELID15", value=LEVELID15, row.names=FALSE)
  dbWriteTable(conn=connect, name="Demcode", value=Demcode, row.names=FALSE)
  dbWriteTable(conn=connect, name="PosNegSpecs", value=PosNegSpecs, row.names=FALSE)
  dbWriteTable(conn=connect, name="LayoutFormat", value=LayoutFormat, row.names=FALSE)   
} )
  
# Remove the Excel objects
rm(pses2011_xls, pses2011_documentation)
```

To see names of the tables in the database:

```
sqldf("SELECT * FROM sqlite_master", dbname = "PSES_database.sqlite")$tbl_name
```

To see table names and SQL statements that generated them:

```
sqldf("SELECT * FROM sqlite_master", dbname = "PSES_database.sqlite")}
```

To see names of columns in a particular table:

```
# Note: PRAMA and TABLE_INFO are SQLite-specific statements
sqldf("PRAGMA TABLE_INFO(Questions)", dbname = "PSES_database.sqlite")$name       
```

A shortcut for seeing a list of tables and/or fields (which may not work depending on your system):    

```
dbListTables(connect)
dbListFields(connect, "Questions")  
```

The equivalent of `head` to look at the start of a table would be:

```
sqldf("SELECT * FROM Questions LIMIT 6", dbname = "PSES_database.sqlite")}
```

Finally, whenever you're done interacting with the database, close the connection:

```
dbDisconnect(connect)
```

SQLite was built intentionally to a be "lightweight" database, and being able to set it up and access it from entirely within R makes it perfect for occasional to moderate desktop use. As is the case with the many different flavors of SQL, there are nuances in SQLite (as we saw with the PRAGMA statement, above), but most basic uses remain the same across those different types of SQL.

You can see a list of the SQL statements available with: `.SQL92Keywords`.

More info on the way R interfaces with RDBMSs can be found on the homepage for the DBI package (https://github.com/rstats-db/DBI), and details on SQLite can be found at its homepage (http://www.sqlite.org/).  

Those with more sophisticated desktop database needs should consider PostgreSQL (http://www.postgresql.org/), which also plays well with R via the RPostgreSQL package.


### Creating a dataframe from a SQLite database

Once you have a database connection, you're able to access exactly what you need and pull just that into R as a dataframe, conserving memory and saving time when working on projects that will need to access a variety of tables.

Load the sqldf package and then reconnect to the database we created in the previous recipe:

```
require(sqldf)
connect = dbConnect(SQLite(), dbname="PSES_database.sqlite ")
```

Selecting subsets from a database table using `sqldf` is based on (of course) SQL select statements. For example, this statement brings in three of the columns from the LEVEL1ID table:

```
pses_acr = sqldf("SELECT Level1ID, Acronym_PSES_Acronyme_SAFF AS Acronym, DeptNameE as Department FROM Agency", dbname="PSES_database.sqlite")
```

You can join tables within the database as well before bringing it into R. For example, this code merges the main data table (pses2011) with the agency name/abbreviation table to create a dataframe called pses2011-agency:

```
pses2011_agency = sqldf("SELECT
    maintable.*
    , agencytable.DeptNameE as Agency
    , agencytable.Acronym_PSES_Acronyme_SAFF as Abbr
    FROM pses2011 maintable
    LEFT JOIN Agency agencytable
        ON maintable.LEVEL1ID = agencytable.Level1ID",
    dbname="PSES_db.sqlite")
```

Don't forget to close the connection: `dbDisconnect(connect)` or `close(connect)`.

Basically, `sqldf("SQL QUERY", dbname="NAMEOFDATABASE")` is all you need to use SQLite within R, and it works on both databases and dataframes—if the latter, it's just `sqldf("SQL QUERY")`. It is important to note that unlike R, SQLite is not case sensitive, so be careful naming tables and columns—a and A are identical to SQLite. (SQL functions are in CAPS in the code examples in this book simply to distinguish them from table and column names, which are left in the same case that R sees them.)

You can do most basic types of SQL work in this fashion; for example, if you want to count the number of rows in your new dataframe where ANSWER3 ("Neutral") is more than 50% of the total responses, have it grouped by Question and Agency, and order the result by highest to lowest counts, you can use:

```
agency_row_count = sqldf("SELECT
    Question
    , Agency
    , COUNT(ANSWER3) as Answer3_Count
    FROM pses2011_agency
    WHERE ANSWER3 > 50
    GROUP BY Question, Agency
    ORDER BY Answer3_Count desc")  
```

The possibilities are fairly limitless by containing large files inside a database and bringing in only what's needed for analysis using SQL. A basic knowledge of SQL is required, of course, but the learning curve is very small and well worth taking on given its centrality in data access and manipulation. It also serves as a great bridge for those used to SQL who want to use R more extensively but are not yet accomplished in using the R functions that have SQL equivalents (e.g., R's merge vs. SQL's join). For example, the standard SQL CASE WHEN statement could be used to generate new columns that sum the number of cases in which the majority either strongly agreed or strongly disagreed:

```
agency_row_count = sqldf("SELECT
    Question
    , SUM(CASE WHEN ANSWER1 > 51 THEN 1 ELSE 0 END) AS Strong_Agreement
    , SUM(CASE WHEN ANSWER5 > 51 THEN 1 ELSE 0 END) AS Strong_Disagreement
    FROM pses2011_agency
    GROUP BY Question, Agency
    ORDER BY Question desc")  
```

It's worth pointing out that if your data fits in memory, there usually isn't need for a database; merging and manipulation can occur inside R—later recipes in this chapter explore ways to do that.

Finally, remember that SQL is suited for data extraction and manipulation, so save the stats work for R. As one example, out-of-the-box SQLite can calculate means (via the AVG function), but not medians or standard deviations; you'd have to hardcode them by their formulas, which is not a very efficient use of code or time. Using R to do that work (see Chapter 2) is far more efficient and reproducible.


## Getting Data from the Web

### Working through a proxy

If you're working behind a company firewall, you may have to use a proxy to pull data into R from the web. Hadley Wickham's `httr` package makes it simple.

```
require(httr)
```

Enter your proxy address, the port (usually 8080), and your user name/password in place of the CAPS code:

```
set_config(use_proxy(url="YOUR_PROXY_URL", port="YOUR_PORT", username="YOUR_USER_NAME", password="YOUR_PASSWORD")
```

There is a bewildering array of methods to access websites from within R, particularly while having to pass through a proxy, and most of them are obscure to even the most established quants. Thanks to `httr`, all of that complexity has been hidden behind a few simple functions—type `vignette("quickstart", package="httr")` if you want more information.

### Scraping data from a web table

Sometimes, webpages have a treasure trove of data in tables... but don't have an option to download it as a text or Excel file. And while data scraping addins are avilable for modern web browsers like Chrome (Scraper) or Firefox (Table2Clipboard and TableTools2) that make it as easy as point-and-click, if the tables aren't set up for easy download or a simple cut-and-paste, you can use the `XML` package to grab what you need.

While there are a few R packages that allow scraping, the `XML` package is the simplest to begin with, and may serve all your needs anyway.

```
require(XML)
```

As a simple example, we can explore the 2010 National Survey on Drug Use and Health. Like many websites, it has pages that contain multiple tables. Say we just want Table 1.1A. First, read in the raw html:

```
drug_use_2010_table = readLines("http://oas.samhsa.gov/NSDUH/2k10NSDUH/tabs/Sect1peTabs1to46.htm")
```

then read in the first table (`which=1`) part of the html:

```
drug_use_table1_1 = readHTMLTable(druguse, header=T, which=1, stringsAsFactors=FALSE)
```

What if you want more than one table? The `XML` package allows you to read in entire webpages, and then scrape them to extract the information you need. For example, let's say we want tables 1.17A and 1.17B . Using the webpage's table of contents, we see that the tables we want are 31st and 32nd:

```
drug_use_table1_17a = readHTMLTable(druguse, header=T, which=31, stringsAsFactors=FALSE)
drug_use_table1_17b = readHTMLTable(druguse, header=T, which=32, stringsAsFactors=FALSE)
```

#### Scraping tables that cover multiple pages

What about a case where you need a table that goes over many pages? For example, ProPublica has a website that lists deficiencies in nursing home care in the United States (http://projects.propublica.org/nursing-homes/); the subset that includes Texas includes more than 3,400 rows that cover 189 different (web)pages.  

Obviously, cutting and pasting that would seriously suck.

Here's how you can do it in R with a few lines of code. First, set up an R object that will iteratively list every one of the 189 web URLs:

```
allpages = paste("http://projects.propublica.org/nursing-homes/findings/search?page=", 1:189, "&search=&ss=ALL&state=TX", sep="")
```

then read in each page into an R list format:

```
tablelist = list()
for(i in seq_along(allpages)){
  page = allpages[i]
  page = readLines(page)
  homes = readHTMLTable(page, header=T, which=1, stringsAsFactors = FALSE)
  tablelist[[i]] = homes
  }
```

and finally turn the list into a data frame:

```
nursing_home_deficiencies = do.call(rbind, lapply(tablelist, data.frame, stringsAsFactors=FALSE))
```

#### Cleaning up the scrape

In many cases the data frame will require a little clean up. The following optional code does some cleaning for this particular data set; look at `?gsub`, `?strsplit`, `?substr`, and related commands to see more details:

```
colnames(nursing_home_deficiencies) = c("Date", "Home", "City", "State", "DeficiencyCount", "Severity")
nursing_home_deficiencies$State = gsub("Tex.", "TX", nursing_home_deficiencies$State, fixed = TRUE)
nursing_home_deficiencies$Home = gsub(" (REPORT)  Home Info", "", nursing_home_deficiencies$Home, fixed = TRUE)
nursing_home_deficiencies$Severity = substr(nursing_home_deficiencies$Severity, 1, 1)
nursing_home_deficiencies$Date = gsub(".", "", nursing_home_deficiencies$Date, fixed = TRUE)
clean = function(col) {
    col = gsub('Jan', 'January', col, fixed = TRUE)
    col = gsub('Feb', 'February', col, fixed = TRUE)
    col = gsub('Aug', 'August', col, fixed = TRUE)
    col = gsub('Sept', 'September', col, fixed = TRUE)
    col = gsub('Oct', 'October', col, fixed = TRUE)
    col = gsub('Nov', 'November', col, fixed = TRUE)
    col = gsub('Dec', 'December', col, fixed = TRUE)
    return(col)
  }
nursing_home_deficiencies$Date = clean(nursing_home_deficiencies$Date)
nursing_home_deficiencies$Date = as.Date(nursing_home_deficiencies$Date, format="%B %d, %Y")
```

### Working with APIs

STUFF HERE?






## Writing files to disk

There is no reason to save a dataframe for use outside R in any format other than a flat file: [COMMENT ON WHY SAVE AS FLAT FILE]

This will save your dataframe into the working directory as a csv, without the row numbers that R provides by default:

```
write.table(pses2011, "pses2011.csv", sep=",", row.names=FALSE)
```

