# Chapter 3: Cleaning and Preparing Data

- Understanding your data  
- Cleaning up  
- Merging dataframes: Joins, unions, and bindings  
- Subsetting: filter and select from a dataframe  
- Creating a derived column  
- Reshaping a dataframe between wide and long  
- Piping with %>% : Stringing it together in dplyr  


## Understanding your data

### Identifying data types and overall structure

Before you do anything else---every dataset should be evaluated with a few basic functions. It’s funny how often this is overlooked…but it’s not funny how much time you can waste writing code that fails until you realize you don’t have the data in the proper format.

Factors, characters, dates, values that are out-of-range for the measurement, inappropriate dummy values, and NAs tend to be the data types or results that will screw up future analyses—so using the the following functions, every time you first pull in a dataset, will save a lot of time and grief in the long run.

We saw the frequent use of `str` in the *Chapter 1* example to understand our dataset and to ensure our changes worked as expected. `head` (and `tail`), `summary`, and `complete.cases` are a few other important functions that help you understand your data set as seen by R. 

Using Table A from the fake customer data created in Chapter 2, we might explore our data one or more times a session with the following functions:  

```
# View the structure of the dataset, particularly data types
str(customers)

# Look at the first (or last) 6 lines of the data frame
head(customers)
tail(customers)
```

```
# Obtain basic summary stats
summary(customers)

# Look at the rows with missing data
missing_data = customers[!complete.cases(customers),]

missing_data
```

Ironically, a few days before I started this chapter, I was trying to use a function that needed factors on a variable I just *knew* were factors, but R had considered them characters because as is my habit, I had used the `stringsAsFactors=FALSE` option when I imported the data. After banging my head for a while, I pulled up `str` and the problem became clear immediately.

Always check! Even if you’re a seasoned professional!


### Identifying (and ordering) factor levels

Factors can be ordered or unordered; when created they usually are arranged alphabetically. 

```
levels(customers$customer_gender)

"F"       "M"       "Unknown"
```

If you do need to change the order---for example, if you want them to display in plots in a different order---you can use the `ordered` function:

```
customers$customer_gender = ordered(customers$customer_gender, 
    c("Unknown", "F", "M"))

"Unknown" "F"       "M" 
```


### Identifying unique values or duplicates

The `unique` function will provide the distinct values in a variable, in the order that they appear in the data. 

```
unique(customers$customer_age)

 [1] 72 44 83 74 66 42 18 NA 34 35 32 50 76 23 36 54
[17] 47 49 33 70 71 68 43 61 45 41 86 85 60 87 24 19
[33] 31 78 59 48 40 69 22 21 67 80 65 84 39 81 51 58
[49] 30
```

You can use the `sort` function to see the unique values in order (excluding NAs):

```
sort(unique(customers$customer_age))

 [1] 18 19 21 22 23 24 30 31 32 33 34 35 36 39 40 41
[17] 42 43 44 45 47 48 49 50 51 54 58 59 60 61 65 66
[33] 67 68 69 70 71 72 74 76 78 80 81 83 84 85 86 87
```

You can see if any rows are duplicated in the data frame with the `duplicated` function, but by itself just returns TRUE/FALSE. Use this structure instead to find any duplicated rows:

```
customers[duplicated(customers),]

[1] customer_id        customer_gender   
[3] customer_age       customer_purchases
[5] customer_zip      
<0 rows> (or 0-length row.names)
```

There aren't any, so to illustrate what it shows when it finds a duplicate, we'll create a new row that is the duplicate of row 2718:

```
customers[10001,] = customers[2718,]

customers[duplicated(customers),]

      customer_id customer_gender customer_age customer_purchases customer_zip
10001        2718               M           47              75.43        73140
```

Before moving on, make sure you remove the duplicated row:

```
customers = unique(customers)
```

![](images/R_unique_screenshot.png)


### Sorting 

The `dplyr` package is incredibly useful for data manipulation, as we'll see later in this chapter, but it's also useful for understanding your data. We often want to sort on variables and see the results to start to get a sense of the data and its patterns. The `base` R installation includes `sort` and `order` functions, but they are clunky when you try to use this this way. `dplyr`'s `arrange` function makes it easy. 

```
require(dplyr)

# Look at top 10 customers by purchases (descending)
head(arrange(customers, desc(customer_purchases)), 10)

# Look at top 10 customers by age (ascending) and purchases (descending)
head(arrange(customers, customer_age, desc(customer_purchases)), 10)

# NAs are placed last
tail(arrange(customers, customer_age, desc(customer_purchases)), 10)

# You can store the sorted data frame, of course
customers = arrange(customers, customer_age, desc(customer_purchases))
```


## Cleaning up

### Converting between data types

In the previous chapter, we saw a JSON import that turned all of the variables into character data types. While this result is relatively rare, there is almost *always* a need to convert a column from one data type to another. After you've used `str` to figure out what type(s) of data you have, you can use conversion functions to change them as you need. The following are the primary conversion functions:

| Data type you want | Function |
| ------------------ | -------- |
| Character / String | `as.character(OBJECT, ...)` |
| Factor / Category | `as.factor(OBJECT, ...)` |
| Numeric / Double | `as.numeric(OBJECT, ...)` |
| Integer | `as.integer(OBJECT, ...)` |
| Date | `as.Date(OBJECT, format="yyyy-mm-dd", ...)` |
| Datetime | `as.POSIXct(OBJECT, tz="CURRENT TIME ZONE", ...)` |

Sometimes you may need to reduce your data set to only a certain type of variable, say all numeric and none of the other variable types. Use the `sapply` function as follows:

```
customers_numeric = customers[sapply(customers,is.numeric)]
```

W> #### Dates and Times
W> 
W> Note that conversion to and between dates and times can be tricky, depending on the structure of the existing date/time variable. If it's already in standard ISO format (e.g., `yyyy-mm-dd`), these basic conversions will work fine. If not, you'll have to mess with the `format` option and/or use other techniques. See *Chapter 6: Trends and Time* for lots of details on working with and converting date and time values. 


### Cleaning up a web-scraped table: Regular expressions and more

In many cases the data frame will require a little clean up after it's loaded. Clarifying messy or unclear column names, pattern replacement, converting between data types, and concatenating or extracting portions of variables' results are all common cleaning needs before analysis. 

`gsub` is probably the most useful general purpose approach to regular expressions in R. You should look at the help files for `?gsub` for regular expressions, `?strsplit` for splitting a character string, and `?substr` for extracting a portion of a string (as well as the links in their *See Also* sections) to get more details on those and similar or otherwise related functions.

There are many possible approaches to cleaning, so as an example, the following code does some cleaning of the Pro Publica *Nursing Home Deficiencies* data set scraped from the web in the previous chapter, using a few different functions:

```
# Create cleaner column names
colnames(nursing_home_deficiencies) = c("Date", "Home", "City", "State", 
  "Deficiency_Count", "Severity")

# Replace abbreviations and extraneous text
nursing_home_deficiencies$State = gsub("Tex.", "TX", 
  nursing_home_deficiencies$State, fixed = TRUE)

nursing_home_deficiencies$Home = gsub(" (REPORT)  Home Info", "", 
  nursing_home_deficiencies$Home, fixed = TRUE)
 
# Take the first letter (severity score) and remove the 
# duplicate and extraneous text
nursing_home_deficiencies$Severity = substr(nursing_home_deficiencies$Severity,
  1, 1)
    
# Clean the date information and convert to date objcet
nursing_home_deficiencies$Date = gsub(".", "", nursing_home_deficiencies$Date,
  fixed = TRUE)

clean = function(col) {
    col = gsub('Jan', 'January', col, fixed = TRUE)
    col = gsub('Feb', 'February', col, fixed = TRUE)
    col = gsub('Aug', 'August', col, fixed = TRUE)
    col = gsub('Sept', 'September', col, fixed = TRUE)
    col = gsub('Oct', 'October', col, fixed = TRUE)
    col = gsub('Nov', 'November', col, fixed = TRUE)
    col = gsub('Dec', 'December', col, fixed = TRUE)
    return(col)
  }

nursing_home_deficiencies$Date = clean(nursing_home_deficiencies$Date)

nursing_home_deficiencies$Date = as.Date(nursing_home_deficiencies$Date, 
  format="%B %d, %Y")
```

The use of `fixed = TRUE` option above tells `gsub` to use exact matching; the default is `false`, which uses [POSIX 1003.2 extended regular expressions](http://www.cheatography.com/davechild/cheat-sheets/regular-expressions/). 

A> #### Changing a single column's name
A> 
A> The `colnames` function works in the above example to specify the names for all columns at once. If you just want to change a single column's name, the pattern is slightly different:
A> 
A> ```
A> colnames(my_data)[column_number] = "new_column_name"
A> ```


### Missing data: Converting dummy NA values

Sometimes, you’ll find values or placeholders where no data should be; for example, `-99` and `9999` are common among older data sets to represent missing values (represented by `NA` in R). To fix this, you can use `gsub`, but you could use assignment to ensure the values don't go from numeric to character:

```
# This will work 
my_data$my_variable = as.numeric(gsub("9999", NA, my_data$my_variable))

# But this is better, albeit less transparent
my_data$my_variable[my_data$my_variable==9999] = NA 

# And this will do it for the entire data frame
my_data[my_data == 9999] = NA
```

It’s also useful to remember to use `na.rm=TRUE` when including data with NA values in calls to many base installation functions (e.g., `mean`), otherwise R will return `NA` or throw an error. 

```
# This won't work
mean(customers$customer_age)
NA

# This will
mean(customers$customer_age, na.rm=TRUE)
53.42279
```

### Rounding

Sometimes you'll have data that's already summarized in some form (e.g., means) and you need to reduce the number of significant figures or decimal places, either for display purposes or to avoid spurious precision. Use `round` for decimal places, and `signif` for significant figures, using the `digits=` option as necessary:

```
round(mean(customers$customer_age, na.rm=TRUE), 1)
53.4

signif(mean(customers$customer_age, na.rm=TRUE), 1)
50

signif(mean(customers$customer_age, na.rm=TRUE), 3)
53.4
```

`ceiling`, `floor`, and `trunc` provide other ways to round numbers to an integer if you need to round up, down, or remove the decimal portion, respectively.  


### Concatenation

Mashing things together into the same field is done with the `paste` or `paste0` function; the only difference between the two is that `paste` requires a separator (defaults to a space) while `paste0` will concatenate without any separators. 

For example, if you want to attach the first of the month to a `yyyy-mm` field, these would be equivalent:

```
my_data$date = as.Date(paste(mydata$yyyy_mm, "01", sep="-"))

my_data$date = as.Date(paste0(mydata$yyyy_mm, "-01"))
```

`paste` works for anything, but you'll have to pay attention to the data type it creates. The commands above would have created character variables if we hadn't specified that we wanted a date object, for example. 

It can also be useful for including soft-coded values in a title or a plot, axis, annotation, etc.:

```
plot(customers$customer_age, customers$customer_purchase, 
  main=paste("Age vs Purchase Amount for", 
  length(customers$customer_age), "Customers"))
```

![](images/pasteplot.png)

A> #### Commas in Numbers
A> 
A> Every once in a while you'll get a file that has commas in a numeric field (cough Excel cough). A data type conversion, `paste`, and `gsub` can be used together to clean that up in one line of code:
A> 
A> ```
A> my_data$my_variable = as.numeric(paste(gsub(",", "", my_data$my_variable)))
A> ```


## Merging dataframes

We've already touched on merging tables inside a SQLite database to create a dataframe in Chapter 2, but more often you usually need to merge two dataframes inside R.

The best package to do this in R is `dplyr`, and part of its greatness is that it works on dataframes in the same way as it does on databases. However, SQL jockies may still prefer to use `sqldf`. I tend to use the former in R-heavy projects and the latter in projects where I need to work from a database or want to streamline the join results. 

Of course, `base` R has native joining abilities as well via the `merge` function, but it tends to be somewhat clunkier than `dplyr` or `sqldf`. 

### Joins

The recipe provides an overview of joining tables via dplyr's `*_join` functions, shows a little more on doing joins using `sqldf`, and reviews R's built-in `merge` function for comparison. We'll also cover the concepts of how the types of joins work to make it easy to visualize what we are trying to accomplish.

First, load the libraries:

```
require(dplyr)
require(sqldf)
```

We'll use the fake customer data we created in the previous chapter to illustrate these methods.

Our objective is to merge the four tables so that we have the customer data alongside an expanded view of their location and their answer from the survey. Using `dplyr`:  

```
customer_locations = left_join(customers, zipcodes, by = 
  c("customer_zip" = "zip"))
  
customer_regions = left_join(customer_locations, state_regions, by= 
  c("stabbr" = "abb"))
  
customer_survey = left_join(customer_regions, survey, by = 
  c("customer_id" = "customer_id"))
```

The base package's `merge` function is the old standby, but is considerably slower than `dplyr`, and the type of join is implied by the `all` option, which reduces code clarity.

```
customer_locations = merge(customers, zipcodes, by.x = "customer_zip", 
  by.y = "zip", all.x = TRUE)
  
customer_regions = merge(customer_locations, state_regions, by.x = "stabbr", 
  by.y = "abb", all.x = TRUE)
  
customer_survey = merge(customer_regions, survey, by.x="customer_id", 
  by.y="customer_id", all.x=TRUE)
```

W> R will match on columns with the same names, so while you can usually get away without specifying the `by` variables with `merge` or `dplyr`, you should *always* specify them anyway---remember your downstream users!

With both the `dplyr` joins and the `base` merges, you get everything in both dataframes. You either have to select only the columns you want in a new dataframe before the merge, or remove the excess columns after it. Of course, using SQL you can choose just the columns you want, which is illustrated below.

```
customer_survey = sqldf("
  SELECT
    customers.*
    , zipcodes.city
    , zipcodes.stabbr
    , state_regions.state
    , state_regions.region
    , state_regions.division
    , survey.strongly_agree
    , survey.agree
    , survey.neutral
    , survey.disagree
    , survey.strongly_disagree
  FROM customers
  LEFT JOIN zipcodes
    ON customers.customer_zip = zipcodes.zip
  LEFT JOIN state_regions
    ON zipcodes.stabbr = state_regions.abb
  LEFT JOIN survey
    ON customers.customer_id = survey.customer_id"
  , stringsAsFactors=FALSE)
```

If you're coming to R with some SQL in your background, you might wonder why this wasn't the method to start with---after all, we can do it all in one step, we get exactly what we want, and we can sort (`ORDER BY`) or add any other SQL statement here to get whatever else we need. The reason is that showing each join/merge in a single step allows other R coders to see exactly what was done more clearly. SQL merge code can get ugly fast, so it's useful to consider whether the project (=code) is going to be passed on to others in the future and whether they are familiar with SQL, and code accordingly.

That said, I love using this approach; SQL provides you many more options and ability to customize your output to be exactly what you need. As long as your downstream dependencies are fine with SQL, you might consider this as your *primary* approach to joining.

Obviously, there's more to life than left joins. The following table lays out the different types of joins, and how `dplyr`, `merge`, and `sqldf` perform each, given data frames/tables *A* and *B*.

{width="90%"}
| Type of Join | Visualization | dplyr | merge | sqldf |
| ------------ | ------------- | ----- | ----- | ----- |
| Left | ![Left join](images/Ch3_sql_01.png) | `left_join(A, B, by=c("A.key" ="B.key"))` | `merge(A, B, by.x="A.key", by.y="B.key", all.x=TRUE)` | `SELECT * FROM A LEFT JOIN B ON A.key = B.key` |
| Inner | ![Inner join](images/Ch3_sql_02.png) | `inner_join(A, B, by=c("A.key" ="B.key"))` | `merge(A, B, by.x="A.key", by.y="B.key", all=FALSE)` | `SELECT * FROM A INNER JOIN B ON A.key = B.key` |
| Full (Outer) | ![Full join](images/Ch3_sql_03.png) | *Not yet implemented* | `merge(A, B, by.x="A.key", by.y="B.key", all=TRUE)` | *Not yet implemented* | 
| Semi | ![Semi join](images/Ch3_sql_04.png) | `semi_join(A, B, by=c("A.key" ="B.key"))` | *n/a* | `SELECT * FROM A LEFT JOIN B ON A.key = B.key WHERE B.key iS NULL` | 
| Anti | ![Anti join](images/Ch3_sql_05.png) | `anti_join(A, B, by=c("A.key" ="B.key"))` | *n/a* | *Not yet implemented* | 
| Right** | ![Right join](images/Ch3_sql_06.png) | `left_join(B, A, by=c("B.key" ="A.key"))` | `merge(A, B, by.x="A.key", by.y="B.key", all.y=TRUE)` | `SELECT * FROM B LEFT JOIN A ON B.key = A.key` |
| Right Anti** | ![Right anti join](images/Ch3_sql_07.png) | `semi_join(B, A, by=c("B.key" ="A.key"))` | *n/a* | `SELECT * FROM B LEFT JOIN A ON B.key = A.key WHERE A.key iS NULL` |

\*\* Although `dplyr` doesn't have a formal right join function, simply reversing the table order and using left join serves the same purpose. Though why you'd want to use a right join anyway is beyond me---right joins are just backwards left joins and it makes more sense to focus on your primary dataset as the "A" table.

You can merge large files with `data.tables`, but the way you specify the joins is opaque---e.g., `datatable_AB = datatable_A[datatable_B]` is a left join, given you've specified the keys ahead of time---so for now it's probably best employed for single-user/casual use and not for production code unless speed needs to trump clarity.


### Unions and bindings

Concatenating two tables can be handled two ways: via `sqldf` or through R's native binding functions. 

It's often easier just to use the native R functions if you already have the two tables ready to concatenate: `rbind` for stacking and `cbind` for concatenating sideways. "Being ready" means that the columns are identically named, and that there are the same number of columns in each table. The `sqldf` approach is best when you're pulling specific columns from each table to stack into a single data frame.  

We'll split the `customer_survey` data frame in half row-wise to illustrate unions:

```
customers_first_half = customer_survey[1:5000,]
customers_second_half = customer_survey[5001:10000,]
```

If you need to stack two tables with the same columns and prefer the SQL approach (and/or already have `sqldf` loaded), just include it in your query, e.g.:

```
customers_whole = sqldf(
    "SELECT * FROM customers_first_half
    UNION
    SELECT * FROM customers_second_half")
```

The `UNION` above would be done with `rbind` as follows:

```
customers_whole = rbind(customers_first_half, customers_second_half)
```

If you have some variables in one table that don't occur in the other table, you can assign them in the other table as a new variable of `NA` values, and then `rbind` them together; we saw an example of this in Chapter 1 when combining the actual and forecast values together into a single data frame for plotting:

```
# Create a data frame with the original data
# and space for the forecast details
use_df = data.frame(Total_Use = power_monthly$Total_Use_kWh, 
  Forecast = NA, Upper_80 = NA, 
  Lower_80 = NA, Upper_95 = NA, Lower_95 = NA)

# Create a data frame for the forecast details
# with a column for the original data
use_fc = data.frame(Total_Use = NA, Forecast = total_use_fc$mean, Upper_80 = 
  total_use_fc$upper[,1], Lower_80 = total_use_fc$lower[,1], 
  Upper_95 = total_use_fc$upper[,2], Lower_95 = total_use_fc$lower[,2])

# "Union" the two data frames into one
use_ts_fc = rbind(use_df, use_fc)
```

The `sqldf` version of the above would be considerably larger, as the `total_use_fc` is actually an R list object, so it would have to be converted to a data frame before you could `UNION` or `rbind`.

Binding two tables together side-by-side is done with the `cbind` or `data.frame` functions (`cbind` is just a wrapper for `data.frame`). This approach is *row-blind*: it will line up the two tables *in the order that they occur*. It is up to you to ensure the observations (rows) match across the two data frames. The two data frames must also have the same numbers of observations (i.e., the same length). 

Again, we'll split the `customer_survey` data column-wide in two for illustration:

```
customers_left_half = customer_survey[,1:7]

customers_right_half = customer_survey[,8:14]
```

We can put them back together again either way:

```
customers_back_together = cbind(customers_left_half, customers_right_half)

customers_back_together = data.frame(customers_left_half, customers_right_half)
```

An alternative to the `cbind` is a join with `sqldf`; a natural or an inner join will work if you have an appropriate key in each table (i.e., `customer_id` in this example). Then you don't need to ensure the order is correct, either. 

```
# Keep the ID in the second table
customers_right_half = customer_survey[,c(1,8:14)]

# Use sqldf to make a natural join
customers_back_together = sqldf("
  SELECT *
  FROM customers_left_half A
  JOIN customers_right_half B
    ON A.customer_id = B.customer_id")
```    

As in the last recipe, here's a visualization and function summary for unions and concatenated tables:  

{width="90%"}
| Type of Join | Visualization | r/cbind | sqldf |
| ------------ | ------------- | ------- | ----- |
| Union | ![union](images/rbind_union.png) | `rbind(A, B)` | `SELECT * FROM A UNION SELECT * FROM B` |
| Semi-union | ![semi union](images/rbind_semi_union.png) | *see above* | *see below*\*\* |
| Concatenate | ![concat](images/cbind.png) | `data.frame(A, B)` | `SELECT * FROM A JOIN B ON A.key = B.key` |
| | | `cbind(A, B)`|  |

\*\* Use `UNION` to bind the two tables, but you'll need to cast columns as null in the tables where they don't already occur, and ensure the column names match in both places, e.g., `CAST(NULL as numeric(8)) AS Forecast` could be used as one line in the table with the actual data so it would read NA when unioned with the forecast values. 


## Subsetting: filter and select from a dataframe

Even after joining, the dataset we have is rarely the dataset we need to complete a project. The `dplyr` package has a few really useful data manipulation functions that work on data frames just like they do on databases, and tend to be simpler, faster, and more intuitive than comparable `base` installation functions like `merge`, `order`, or `subset`.

We'll use the same fake data generated previously to illustrate filtering (subsetting rows) and selecting (subsetting columns) a data frame.

```
require(dplyr)
```

Need to subset on a couple of conditions? Use `filter`, using `&` for AND and `|` (pipe) for OR:

```
# Subset the data to only those customers who agree or strongly agree 
# with the survey question
customers_marketing = filter(customer_survey, strongly_agree == 1 | agree == 1)

# Subset the data to customers who are Californians over age 64 who have spent 
# at least $200 OR Oregonians of any age who have spent more than $500
customers_CA_65_200_OR_500 = filter(customer_survey, state == "California" &
  customer_age >= 65 & customer_purchases >= 200 | state == "Oregon" &
  customer_purchases > 500)
```

You can use regular expressions in filtering with the `grepl` function:
```
customers_in_lake_cities = filter(customer_survey, grepl('lake', tolower(city)))
```

Need to reduce the data to just some of the variables without having to spell them all out? Use `select`, which provides some built-in SQL "like" type of access to make selection of columns from a large dataset more managable:

```
# Subset the data to only columns with names that start with "customer"
customers_only = select(customer_survey, starts_with("customer"))

# Subset the data to only ID and columns with names that contain "agree"
customers_only_agreement = select(customer_survey, customer_id, matches("agree"))
```

`select` can also remove variables by appending a minus sign (`–`) to the front of the criteria:  

```
# Subset the data to only columns with names that DO NOT start with "customer"
customers_excluded = select(customer_survey, -starts_with("customer"))
```

Definitely explore the vignettes in the `dplyr` package (start with `vignette("introduction", package="dplyr"))`, as they provide a wide variety of examples and use cases—there's almost certainly going to be an example of what you need to do there.

I> RStudio has a great `dplyr` cheat sheet you can download [here](http://www.rstudio.com/wp-content/uploads/2015/02/data-wrangling-cheatsheet.pdf).


## Creating a derived column

You can create a new column based on other columns using either `dplyr`'s `mutate` function or simply by assignment in `base` R. Both create a new column, but `mutate` can perform slower on large datasets and/or be more awkward to type than simply making a new column in the usual way with `$` and `=`. 

Both of these commands create a new column that combine city and the state names. 

```
customer_survey$Location = paste(customer_survey$city, customer_survey$stabbr, 
    sep=", ")
# or
customer_survey = mutate(customer_survey, Location=paste(city, stabbr, sep=", "))
```

Of course, math operations work with both methods, e.g., to get the proportion an observation is of the whole:

```
customer_survey$proportion = customer_survey$customer_purchases / 
  sum(customer_survey$customer_purchases)
# or
customer_survey = mutate(customer_survey, proportion2 = customer_purchases / 
  sum(customer_purchases))
```


## Peeking at the outcome with `dplyr`

If you want to check on how one or more of the `dplyr` actions will perform in terms of output before updating or creating a dataframe, you can wrap it inside the `head` function. For example:

```
head(select(customer_survey, contains("customer", ignore.case=TRUE)))
```

![](images/head1full.png)

```
head(arrange(customer_survey, region, desc(customer_age), 
  desc(customer_purchases)))
```

![](images/head2full.png)

```
head(mutate(customer_survey, Location = paste(city, state, sep=", "), 
  purchase_proportion=customer_purchases/sum(customer_purchases)))
```

![](images/head3full.png)


## Reshaping a dataframe between wide and long

A lot of analysis and plotting functions require that data be organized in *long* format, where the variable of interest is in one column and the id/categorical portions of the data are repeated as necessary in other columns. Then again, other tools need the data in *wide* format, so being able to move between the two formats is essential. The `reshape2` package provides just that ability.  

### Reshaping: melt and cast

We'll use a small dataset to illustrate some of the possibilities `reshape2` offers—this shows the verbal and nonverbal IQ test scores for children before and after neurosurgery (and in which hemisphere, i.e. "Side" of the brain; data from *Table 2* in [this paper](http://thejns.org/doi/10.3171/2015.3.PEDS14359)). Load the package and the data:

```
require(reshape2)

surgery_outcomes = read.table("https://raw.githubusercontent.com/Rmadillo/
  business_intelligence_with_r/master/manuscript/code/surgery_outcomes.tsv", 
  header=T, sep="\t")

head(surgery_outcomes, 4)

row  ID  Side   Phase  Verbal  Nonverbal
1    1   Right  Post   146     115
2    1   Right  Pre    129     103
3    2   Right  Post   126     115
4    2   Right  Pre    138     115
```

As is, this data is both wide and long. If you're only interested in looking at either Verbal score or Nonverbal scores, it's long, as the other variables are already long. If you want to look at Verbal and Nonverbal together, it's wide. 

Suppose each measurement needs its own row, that is, we need to make this a long data frame so we can compare Verbal and Nonverbal scores. We need to contain those two scores within a single column, such as `IQ_score`. To do this, you `melt` a dataframe from wide into long format:

```
surgery_outcomes_melted = melt(surgery_outcomes, id.vars=c(1:4),
  measure.vars=c(5:6), variable.name="Test_type", value.name="IQ_score")

head(surgery_outcomes_melted, 4)

row  ID  Side   Phase  Test_type  IQ_score
1    1   Right  Post   Verbal     146
2    1   Right  Pre    Verbal     129
3    2   Right  Post   Verbal     126
4    2   Right  Pre    Verbal     138
```

We now have a completely long form data set, as we have the primary variable of analytic interest contained in a single column. 

Any data in long form can be `cast` into a variety of wide forms, with a `d` or an `a` tacked on to the front depending on whether you need a dataframe (`d`), or a vector, matrix, or array (`a`). To put this data back into the wide/long shape it started as, use `dcast`:

```
surgery_outcomes_original = dcast(surgery_outcomes_melted, 
  ID + Side + Phase ~ Test_type, value.var="IQ_score")

head(surgery_outcomes_original, 4)

row  ID  Side   Phase  Verbal  Nonverbal
1    1   Right  Post   146     115
2    1   Right  Pre    129     103
3    2   Right  Post   126     115
4    2   Right  Pre    138     115
```

Being able to move back and forth between data shapes makes meeting the needs of the R packages you're working with considerably easier, especially if you're moving between some that work best with long form structures and others that work well with wide.

The important part to remember in melting data is to ensure you've designated the id and measurement variables appropriately. While `melt` can operate with neither or only one of these designated, proper coding etiquette requires that you designate each specifically to avoid ambiguity.

I> The `tidyr` package is a new, simpler way to reshape data. At the time of this writing, it can accomplish any simple `melt`/`cast` (called `gather`/`spread`) task. However, more complicated data set transformations can fail in `tidyr`; the current consensus is to keep `reshape2` in the toolbox until `tidyr` is mature enough to accomplish the full suite of transformation needs. A useful cheat sheet on `tidyr` is co-packaged with `dplyr` by RStudio [here](http://www.rstudio.com/wp-content/uploads/2015/02/data-wrangling-cheatsheet.pdf), and a more complete treatment is available [here](http://vita.had.co.nz/papers/tidy-data.html) or by checking out the vignette: `vignette("tidy-data")`.


### Reshaping: Crossing variables with ~ and +

There is tremendous variety in reshaping options using the formula structure in `dcast`. The tilde sign (`~`) separates the variables you want representing the rows (left of the ~) and those you want representing the columns (right of the ~). For example, we can cross phase and the test type in the columns:

```
head(dcast(surgery_outcomes_melted, ID ~ Test_type + Phase, 
  value.var="IQ_score"), 4)

     ID  Verbal_Post  Verbal_Pre  Nonverbal_Post  Nonverbal_Pre
1    1   146          129         115             103
2    2   126          138         115             115
3    3   110          100         104             100
4    4   104          NA          106             NA
```

We could also bring back the `Side` variable into the same wide shape:

```
head(dcast(surgery_outcomes_melted, ID + Side ~ Test_type + Phase, 
  value.var='IQ_score'), 4)

   ID  Side   Verbal_Post  Verbal_Pre  Nonverbal_Post  Nonverbal_Pre
1  1   Right  146          129         115             103
2  2   Right  126          138         115             115
3  3   Left   110          100         104             100
4  4   Right  104          NA          106             NA
```

Perhaps you only want to use `Phase` to look at Verbal and Nonverbal separately:

```
head(dcast(surgery_outcomes_melted, ID + Phase ~ Test_type, 
  value.var='IQ_score'), 4)

  ID Phase Verbal Nonverbal
1  1  Post    146       115
2  1   Pre    129       103
3  2  Post    126       115
4  2   Pre    138       115
```

And so on... the possibilities are as many as your variable set...


### Summarizing while reshaping

While `dplyr` provides you with all you need for summarizing data once your data structure is set, `reshape2` offers the option to summarize while reshaping. The summarization works based on which operation you choose as well as which side of the `~` you place the variables (as described above).  

Get the mean for each `Test_type`, by `Phase`:

```
dcast(surgery_outcomes_melted, Phase ~ Test_type, 
  value.var='IQ_score', mean, na.rm=T)

  Phase   Verbal Nonverbal
1  Post 104.6667 102.26667
2   Pre 103.0909  98.90909
```

Get the mean for each `Side`, by combination of `Test_type` and `Phase`:

```
dcast(surgery_outcomes_melted, Side ~ Test_type + Phase,
  value.var='IQ_score', mean, na.rm=T)

   Side Verbal_Post Verbal_Pre Nonverbal_Post Nonverbal_Pre
1  Left    102.4444   97.71429       100.7778      97.42857
2 Right    108.0000  112.50000       104.5000     101.50000
```

Get the standard deviation for each combination of `Test_type`, `Side`, and `Phase`:

```
dcast(surgery_outcomes_melted, Test_type + Side + Phase ~ ., 
  value.var='IQ_score', sd, na.rm=T)

  Test_type  Side Phase        .
1    Verbal  Left  Post 14.78269
2    Verbal  Left   Pre 12.51285
3    Verbal Right  Post 23.80756
4    Verbal Right   Pre 24.93324
5 Nonverbal  Left  Post 16.09952
6 Nonverbal  Left   Pre 13.96253
7 Nonverbal Right  Post 11.46734
8 Nonverbal Right   Pre 10.47219
```

## Piping with %>%: Stringing it together in dplyr
  
A useful additional feature of `dplyr` is that you can string together a bunch of functions with the `%>%` ("then") operator, a feature that is growing in popularity across a variety of R packages. So, if you want to filter, group, do some summary stats, create a new column, etc., you can pipe them all together instead of nesting them in parentheses or over multiple lines. 

```
# For customers who've purchased more than $500, get count, mean, 
# sd, and sum for each state, and calculate the coeffient of
# variation, rounded to 2 decimal places
customer_500_piped = customer_survey %>%  
  filter(customer_purchases >= 500) %>%   
  group_by(state) %>%  
  summarize(count_purchases = n(),  
    mean_purchases = mean(customer_purchases, na.rm=T),  
    sd_purchases = sd(customer_purchases, na.rm=T),  
    sum_purchases = sum(customer_purchases)) %>%
  mutate(cv_purchases = round(sd_purchases / mean_purchases, 2))  

head(customer_500_piped, 4)

Source: local data frame [4 x 6]

     state count_purchases mean_purchases sd_purchases sum_purchases cv_purchases
     (chr)           (int)          (dbl)        (dbl)         (dbl)        (dbl)
1  Alabama               7       879.2857     301.6824       6155.00         0.34
2   Alaska               4       928.5350     234.7453       3714.14         0.25
3  Arizona               8       707.6425     186.9487       5661.14         0.26
4 Arkansas               5       767.0300     124.2218       3835.15         0.16
```

This is also a good example where `mutate` is more convenient than traditional assignment.  
 
However, not everyone is enamored with this new "piping" trend; when you need to provide verbose code or examples for beginners to understand the processing steps it [may not be the best coding choice](http://www.fromthebottomoftheheap.net/2015/06/03/my-aversion-to-pipes/). As with all "coding standards" advice, do as you think best for you and your context. 


|  |
|  |
|  |
|  |
