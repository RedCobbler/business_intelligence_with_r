# Chapter 3: Cleaning and Preparing Data

- Understanding Your Data
- Cleaning up
- Merging dataframes
- Filtering, selecting, and arranging a dataframe
- Reshaping: Transforming a dataframe from wide to long and back again


## Understanding Your Data

### Identifying data types and overall structure

Before you do anything else—every dataset should be evaluated with a few basic functions. It’s funny how often this is overlooked…but it’s not funny how much time you can waste writing code that fails until you realize you don’t have the data in the proper format.

Factors, characters, dates, values that are out-of-range for the measurement, inappropriate dummy values, and NAs tend to be the data types or results that will screw up future analyses—so using the the following functions, every time you first pull in a dataset, will save a lot of time and grief in the long run.

We saw the frequent use of `str` in the Chapter 1 example to understand our dataset and to ensure our changes worked as expected. `head` (and `tail`), `summary`, and `complete.cases` are a few other important functions that help you understand your data set as seen by R. 

Using Table A from the fake customer data created in Chapter 2, we might explore our data one or more times a session with the following functions

```
# View the structure of the dataset, particularly data types
str(customers)

# Look at the first (or last) 6 lines of the data frame
head(customers)
tail(customers)

# Obtain basic summary stats
summary(customers)

# Look at the rows with missing data
missing_data = customers[!complete.cases(customers),]
missing_data
```

Ironically, a few days before I started this chapter, I was trying to use a function that needed factors on a variable I just *knew* were factors, but R had considered them characters because as I had used the `stringsAsFactors=FALSE` option when I imported the data. After banging my head for a while, I pulled up `str` and the problem became clear immediately.

Always check! Even if you’re a seasoned professional!


### Identifying (and ordering) factor levels

Factors can be ordered or unordered; when created they usually are arranged alphabetically. 

```
levels(customers$customer_gender)

[1] "F"       "M"       "Unknown"
```

If you do need to change the order for example, if you want them to display in plots in a different order, you can use the `ordered` function:

```
customers$customer_gender = ordered(customers$customer_gender, 
    c("Unknown", "F", "M"))

[1] "Unknown" "F"       "M" 
```


### Identifying unique values or duplicates

The `unique` function will provide the distinct values in a variable, in the order that they appear in the data. 

```
unique(customers$customer_age)

 [1] 72 44 83 74 66 42 18 NA 34 35 32 50 76 23 36 54 47 49 33 70 71 68 43 61 
 45 41 86 85 60 87 24 19 31 78 59
[36] 48 40 69 22 21 67 80 65 84 39 81 51 58 30
```

You can use the `sort` function to see the unique values in order:

```
sort(unique(customers$customer_age))

[1] 18 19 21 22 23 24 30 31 32 33 34 35 36 39 40 41 42 43 44 45 47 48 49 50
51 54 58 59 60 61 65 66 67 68 69
[36] 70 71 72 74 76 78 80 81 83 84 85 86 87
```

You can see if any rows are duplicated in the data frame with the `duplicated` function, but by itself just returns TRUE/FALSE. Use this structure to find any duplicated rows:

```
customers[duplicated(customers), ]

[1] customer_id        	customer_gender    customer_age       
customer_purchases 	customer_zip      
<0 rows> (or 0-length row.names)
```

There aren't any, so to illustrate what it shows when it finds a duplicate, we'll create a new row that is the duplicate of row 2718:

```
customers[10001,] = customers[2718,]

      customer_id customer_gender customer_age customer_purchases customer_zip
10001        2718               M           47              75.43        73140
```

Before moving on, make sure you remove the duplicated row:

```
customers = unique(customers)
```

![unique customers](/images/R_unique_screenshot.png)


### Sorting 

The `dplyr` package is incredibly useful for data manipulation, as we'll see later in this chapter, but it's also useful for understanding your data. Even in Excel, lots of us want to sort on variables and see the results to start to get a sense of the data and its patterns. The `base` R installation includes `sort` and `order` functions, but they are clunky when you try to use this this way. `dplyr`'s `arrange` function makes it easy. 

```
require(dplyr)

# Look at top 10 customers by purchases (descending)
head(arrange(customers, desc(customer_purchases)), 10)

# Look at top 10 customers by age (ascending) and purchases (descending)
head(arrange(customers, customer_age, desc(customer_purchases)), 10)

# NAs are placed last
tail(arrange(customers, customer_age, desc(customer_purchases)), 10)

# You can store the sorted data frame, of course
customers = arrange(customers, customer_age, desc(customer_purchases))
```


## Cleaning up

### Cleaning up a web-scraped table

In many cases the data frame will require a little clean up after it's loaded. Clarifying messy or unclear column names, pattern replacement, converting between data types, and concatenating or extracting portions of variables' results are all common cleaning needs before analysis. 

`gsub` is probably the most useful general purpose approach to regular expressions in R. You should look at the help files for `?gsub` for regular expressions, `?strsplit` for splitting a character string, and `?substr` for extracting a portion of a string (as well as the links in their *See Also* sections) to get more details on those and similar or otherwise related functions.

There are many possible approaches to cleaning, so as an example, the following code does some cleaning of the Pro Publica *Nursing Home Deficiencies* data set scraped from the web in the previous chapter, using a few different functions:

```
# Create column names
colnames(nursing_home_deficiencies) = c("Date", "Home", "City", "State", 
    "Deficiency_Count", "Severity")

# Replace abbreviations and extraneous text
nursing_home_deficiencies$State = gsub("Tex.", "TX", 
    nursing_home_deficiencies$State, fixed = TRUE)
nursing_home_deficiencies$Home = gsub(" (REPORT)  Home Info", "", 
    nursing_home_deficiencies$Home, fixed = TRUE)
    
# Take the first 
nursing_home_deficiencies$Severity = substr(nursing_home_deficiencies$Severity,
    1, 1)
    
# Clean the date data
nursing_home_deficiencies$Date = gsub(".", "", nursing_home_deficiencies$Date,
    fixed = TRUE)
clean = function(col) {
    col = gsub('Jan', 'January', col, fixed = TRUE)
    col = gsub('Feb', 'February', col, fixed = TRUE)
    col = gsub('Aug', 'August', col, fixed = TRUE)
    col = gsub('Sept', 'September', col, fixed = TRUE)
    col = gsub('Oct', 'October', col, fixed = TRUE)
    col = gsub('Nov', 'November', col, fixed = TRUE)
    col = gsub('Dec', 'December', col, fixed = TRUE)
    return(col)
  }
nursing_home_deficiencies$Date = clean(nursing_home_deficiencies$Date)
nursing_home_deficiencies$Date = as.Date(nursing_home_deficiencies$Date, 
    format="%B %d, %Y")
```

The use of `fixed = TRUE` option above tells `gsub` to use exact matching; the default is `false`, which uses [POSIX 1003.2 extended regular expressions](http://www.cheatography.com/davechild/cheat-sheets/regular-expressions/). 


### Missing values: Dummy NA values and dropping the observations with NAs

Sometimes, you’ll find values or placeholders where no data should be; for example, -99 and 9999 are common among older data sets to represent NAs. To fix this, you can use `gsub`, but you could also do something like:

```
my_data$my_variable[my_data$my_variable==9999] = NA 
```

There are a variety of ways to deal with missing values—and we’ll explore some later in this chapter, under *Imputation*—but the simplest is to just remove the rows that contain them:

```
my_data = na.omit(my_data)
```

This is usually a bad idea. You lose a lot of information this way, so unless you data set is massive, eitehr work with the NAs as-is, or perform imputation on the NAs using the best possible method relative to your data set and your analytic aims. 

It’s also useful to remember to use `na.rm=TRUE` when including data with NA values in calls to many base installation functions (e.g., `mean`); they will throw an error otherwise. 

```
mean(customers$customer_age)

mean(customers$customer_age, na.rm=TRUE)

```


### Converting between data types

In the previous chapter, we saw a JSON import that turned all of the variables into character data types. While this result is relatively rare, there is almost *always* a need to convert a column from one data type to another. After you've used `str` to figure out what type(s) of data you have, you can use conversion functions to change them as you need. The following are the primary conversion functions:

| Data type you want | Function |
| ------------------ | -------- |
| Character / String | `as.character(OBJECT, ...)` |
| Factor / Category | `as.factor(OBJECT, ...)` |
| Numeric / Double | `as.numeric(OBJECT, ...)` |
| Integer | `as.integer(OBJECT, ...)` |
| Date | `as.Date(OBJECT, format="yyyy-mm-dd", ...)` |
| Datetime | `as.POSIXct(OBJECT, tz="CURRENT TIME ZONE", ...)` |

Note that conversion to and between dates and times can be tricky, depending on the structure of the existing date/time variable. If it's already in standard ISO format (e.g., `yyyy-mm-dd`), these basic conversions will work fine. If not, you'll have to mess with the `format` option and/or use other techniques. See Chapter 8 (*Trends and Time*) for lots of details on working with and converting date and time values. 


### Rounding

Sometimes you'll have data that's already summarized in some form (e.g., means) and you need to reduce the number of significant figures, either for display purposes or to avoid spurious precision. Use `round`, using the :

```
round(mean(customers$customer_age, na.rm=TRUE), )
```


### Concatenation

Mashing things together into the same field is done with the `paste` or `paste0` function; the only difference between the two is that `paste` requires a separator (defaults to a space) while `paste0` will concatenate without any separators between what you're pasting together. 

For example, if you want to attach the first of the month to a `yyyy-mm` field, these would be equivalent:

```
my_data$date = as.Date(paste(mydata$yyyy_mm, "01", sep="-"))
my_data$date = as.Date(paste0(mydata$yyyy_mm, "-01"))
```

`paste` works for anything, but you'll have to pay attention to the data type it creates. The commands above would have created character variables without specifying you wanted a date object, for example. 

It can also be useful for including variables or changing values in a title or a plot, axis, annotation, etc.:

```
EXAMPLE OF BASE PLOT WITH A VARIABLE IN TITLE
```

Every once in a while you'll get a file that has commas in a numeric field (cough Excel cough). A data type conversion, `paste`, and `gsub` can be used together to clean that up in one line:

```
my_data$my_variable = as.numeric(paste(gsub(",", "", my_data$my_variable)))
```


## Imputation

When you disregard cases with any missing variables, you lose useful information that the nonmissing values in that case convey. You may sometimes want to impute reasonable values (those that will not skew the results of analyses very much) for the missing values.

Read data and replace missing values with mean:

> dat <- read.csv("missing-data.csv", na.strings = "")
> dat$Income.imp.mean <- ifelse(is.na(dat$Income), mean(dat$Income, na.rm=TRUE), dat$Income)


Imputing random values sampled from nonmissing values

If you want to impute random values sampled from the nonmissing values of the variable, you can use the following two functions:

rand.impute <- function(a) {
  missing <- is.na(a)
  n.missing <- sum(missing)
  a.obs <- a[!missing]
  imputed <- a
  imputed[missing] <- sample (a.obs, n.missing, replace=TRUE)
  return (imputed)
}

random.impute.data.frame <- function(dat, cols) {
  nms <- names(dat)
  for(col in cols) {
    name <- paste(nms[col],".imputed", sep = "")
    dat[name] <- rand.impute(dat[,col])
  }
  dat
}

dat <- read.csv("missing-data.csv", na.strings="")
> random.impute.data.frame(dat, c(1,2))










## Scaling

The scale() function takes two optional arguments, center and scale, whose default values are TRUE. The following table shows the effect of these arguments:

Argument
	

Effect

center = TRUE, scale = TRUE
	

Default behavior described earlier

center = TRUE, scale = FALSE
	

From each value, subtract the mean of the concerned variable

center = FALSE, scale = TRUE
	

Divide each value by the root mean square of the associated variable, where root mean square is sqrt(sum(x^2)/(n-1))

center = FALSE, scale = FALSE
	

Return the original values unchanged



***DOUBLE CHECK ON REDUCING TO ONLY NUMERIC***  


## Merging dataframes

We've already touched on merging tables inside a SQLite database to create a dataframe, but more often you usually need to merge two dataframes inside R.

Perhaps the best package to do this in R is `dplyr`, and part of its greatness is that it works on dataframes in the same way as it does on databases. However, SQL jockies will prefer to use `sqldf`. I tend to use the former in R-heavy projects and the latter in projects where I need to work on lots of database tables or want to streamline the join results. Of course, `base` R has joining abilities as well via the `merge` function, but it tends to be less useful than `dplyr` or `sqldf`.    

### Joins

The recipe provides an overview of joining tables via dplyr's `*_join` functions, shows a little more on doing joins using `sqldf`, and reviews R's built-in `merge` function for comparison. We'll also cover the concepts of how the types of joins work to make it easy to visualize what we are trying to accomplish.

First, load the libraries:

```
require(dplyr)
require(sqldf)
```

We'll use the fake customer data we created in the previous chapter to illustrate these methods.

Our objective is to merge the four tables so that we have the customer data alongside an expanded view of their location and their answer from the survey.

```
customer_locations = left_join(customers, zipcodes, by = 
  c("customer_zip" = "zip")
customer_regions = left_join(customer_locations, state_regions, by= 
  c("stabbr" = "abb")
customer_survey = left_join(customer_regions, survey, by = 
  c("customer_id" = "customer_id")
```

The base package's `merge` function is the old standby, but is considerably slower than `dplyr`, and the type of join is implied by the all option, which reduces code clarity.

```
customer_locations = merge(customers, zipcodes, by.x = "customer_zip", 
  by.y = "zip", all.x = TRUE)
customer_regions = merge(customer_locations, state_regions, by.x = "stabbr", 
  by.y = "abb", all.x = TRUE)
customer_survey = merge(customer_regions, survey, by.x="customer_id", 
  by.y="customer_id", all.x=TRUE)
```

With both the `dplyr` `*_join` and the `base` `merge` functions, you get everything in both dataframes. You either have to select only the columns you want in a new dataframe before the merge, or remove the excess columns after it. Of course, using SQL you can choose just the columns you want, which is illustrated below.

```
customer_survey = sqldf("SELECT
    customers.*
    , zipcodes.city
    , state_regions.state
    , state_regions.region
    , state_regions.division
    , survey.strongly_agree
    , survey.agree
    , survey.neutral
    , survey.disagree
    , survey.strongly_disagree
    FROM customers
    LEFT JOIN zipcodes
        ON customers.customer_zip = zipcodes.zip
    LEFT JOIN state_regions
        ON zipcodes.stabbr = state_regions.abb
    LEFT JOIN survey
        ON customers.customer_id = survey.customer_id"
    , stringsAsFactors=FALSE)
```

If you're coming to R with some SQL in your background, you might wonder why this wasn't the primary method of this recipe—after all, we can do it all in one step, and we get exactly what we want, and we can sort (`ORDER BY`) or add any other SQL statement here to get what we need. The reason is that showing each join/merge in a single step allows other R coders to see exactly what was done more clearly. SQL merge code can get ugly fast, so it's useful to consider whether the project (=code) is going to be passed on to others in the future and whether they are familiar with SQL, and code accordingly.

That said, I love using this approach; SQL provides you many more options and ability to customize your output to be exactly what you need. As long as your downstream dependencies are fine with SQL, you might consider this as your *primary* approach to joining.

Obviously, there's more to life than left joins. The following table lays out the different types of joins, and how `dplyr`, `merge`, and `sqldf` perform each, given data frames/tables *A* and *B*.

{width="90%"}
| Type of Join | Visualization | dplyr | merge | sqldf |
| ------------ | ------------- | ----- | ----- | ----- |
| Left | ![Left join](/images/Ch3_sql_01.png) | `left_join(A, B, by=c("A.key" ="B.key"))` | `merge(A, B, by.x="A.key", by.y="B.key", all.x=TRUE)` | `SELECT * FROM A LEFT JOIN B ON A.key = B.key` |
| Inner | ![Inner join](/images/Ch3_sql_02.png) | `inner_join(A, B, by=c("A.key" ="B.key"))` | `merge(A, B, by.x="A.key", by.y="B.key", all=FALSE)` | `SELECT * FROM A INNER JOIN B ON A.key = B.key` |
| Full (Outer) | ![Full join](/images/Ch3_sql_03.png) | *Not yet implemented* | `merge(A, B, by.x="A.key", by.y="B.key", all=TRUE)` | `SELECT * FROM A FULL OUTER JOIN B ON A.key = B.key` | 
| Semi | ![Semi join](/images/Ch3_sql_04.png) | `semi_join(A, B, by=c("A.key" ="B.key"))` | *n/a* | `SELECT * FROM A LEFT JOIN B ON A.key = B.key WHERE B.key iS NULL` | 
| Anti | ![Anti join](/images/Ch3_sql_05.png) | `anti_join(A, B, by=c("A.key" ="B.key"))` | *n/a* | `SELECT * FROM A FULL OUTER JOIN B ON A.key = B.key WHERE A.key is NULL OR B.key iS NULL` | 
| Right[^*] | ![Right join](/images/Ch3_sql_06.png) | `left_join(B, A, by=c("B.key" ="A.key"))` | `merge(A, B, by.x="A.key", by.y="B.key", all.y=TRUE)` | `SELECT * FROM A RIGHT JOIN B ON A.key = B.key`|
| Right Anti* | ![Right anti join](/images/Ch3_sql_07.png) | `semi_join(B, A, by=c("B.key" ="A.key"))` | *n/a* | `SELECT * FROM A RIGHT JOIN B ON A.key = B.key WHERE A.key IS NULL`

[^*] Although `dplyr` doesn't have a right join function, simply reversing 
    the table order and using left join serves the same purpose. Though why
    you'd want to use a right join anyway is beyond me—right joins are just 
    backwards left joins and it makes more sense to focus on your primary 
    dataset as the "A" table.

W> R will match on columns with the same names, so while you can get away without specifying the `by` variables, you should always specify them—remember your downstream users!

I> You can merge large files with `data.tables`, but at the time of this writing, the way you specify the joins is opaque—e.g., `datatable_AB = datatable_A[datatable_B]` is a left join, given you've specified the keys ahead of time—so for now we recommend that it only be employed for single-user/casual use and not for production code.


### Unions and bindings

Concatenating two tables can be handled two ways: via `sqldf` or through R's native binding functions. 

If you need to stack two tables with the same columns—`UNION` in SQL terms—and prefer the SQL approach (and/or already have `sqldf` loaded), just include it in your query, e.g.:

```
EXAMPLE SQL QUERY WITH UNION
```

It's often easier just to use the native R functions if you already have the two tables ready to concatenate: `rbind` for stacking and `cbind` for concatenating *sideways*. Being ready means that the columns are identically named, and that there are the same number of columns in each table. 

The above SQL `UNION` above would be done with `rbind` as follows:

```
rbind(A, B)
```

Binding two tables together side-by-side is done with the `cbind` function. `cbind` is *row-blind*: it will line up the two tables *in the order that they occur*. It is up to you to ensure the observations (rows) match across the two data frames. The two data frames must also have the same numbers of observations (i.e., the same length). 

```
cbind(A,B)
```

As in the last recipe, here's a visualization summary:  

{width="90%"}
| Type of Join | Visualization | *bind | sqldf |
| ------------ | ------------- | ----- | ----- |
| Union | ![union](/images/rbind_union.png) | `rbind(A, B)` | `SELECT * FROM A UNION SELECT * FROM B` |
| Semi-union | ![semi union](/images/rbind_semi_union.png) | *see below* | *see below* |
| Concatenate | ![concat](/images/cbind.png) | `cbind(A, B)` | *n/a* |


Add NA columns to allow stacking with different #s of columns

See example in Chapter 1


## Filtering and Selecting a Dataframe: Subseting Rows and Columns

Even after joining, the dataset we have is rarely the dataset we need to complete a project. The `dplyr` package has a few really useful data manipulation functions that work on data frames just like they do on databases, and tend to be simpler, faster, and more intuitive than comparable `base` installation functions like `merge`, `order`, or `subset`.

We'll use the same fake data generated previously to illustrate filtering (subsetting rows) and selecting (subsetting columns) a data frame.

```
require(dplyr)
```

Need to subset on a couple of conditions? Use `filter`, stringing together criteria using `&` for AND and `|` (pipe) for OR:

```
# Subset the data to only those customers who agree or strongly agree with the survey question
customers_marketing = filter(customer_survey, strongly_agree == 1 | agree == 1)

# Subset the data to customers who are Californians over age 64 who have spent at 
# least $200 OR Oregonians of any age who have spent more than $500
customers_CA_65_200_OR_500 = filter(customer_survey, state == "California" &
  customer_age >= 65 & customer_purchases >= 200 | state == "Oregon" &
  customer_purchases > 500)
```

Need to reduce the data to just some of the variables without having to spell them all out? Use `select`, which provides some "regular expression" type of access to make selection of columns from a large dataset more managable:

```
# Subset the data to only columns with names that start with "customer"
customers_only = select(customer_survey, starts_with("customer"))

# Subset the data to only ID and columns with names that contain "agree"
customers_only_agreement = select(customer_survey, customer_id, matches("agree"))
```

`select` can also remove variables by appending a minus sign (`–`) to the front of the criteria:  

```
# Subset the data to only columns with names that DO NOT start with "customer"
customers_excluded = select(customer_survey, -starts_with("customer"))
```

Definitely explore the vignettes in the `dplyr` package (start with `vignette("introduction", package="dplyr"))`, as they provide a wide variety of examples and use cases—there's almost certainly going to be an example of what you need to do there.

T> RStudio has a great `dplyr` cheat sheet you can download [here](http://www.rstudio.com/wp-content/uploads/2015/02/data-wrangling-cheatsheet.pdf).


## Stringing it Together in `dplyr`

A potentially useful additional feature of `dplyr` is that you can string together a bunch of functions with the `%>%` ("then") operator, a feature that is growing in popularity across a variety of R packages.

```
customer_500_dplyred = customer_survey %>%
    filter(customer_purchases >= 500) %>%
    group_by(state) %>%
    summarize(count_purchases = n(),
              mean_purchases = mean(customer_purchases, na.rm=T),
              sd_purchases = sd(customer_purchases, na.rm=T),
              sum_est = sum(est)) %>%
    mutate(cv_purchases = round(sd_purchases / mean_purchases, 2))
```

W> Not everyone is enamored with this new piping trend; when you need to provide verbose code or examples for beginners to understand the processing steps it [may not be the best coding choice](http://www.fromthebottomoftheheap.net/2015/06/03/my-aversion-to-pipes/). As with all "coding standards" advice, do as you think best for you and your context. 

At the end of this piping string we see dplyr's `mutate` function. This creates a new column, but can perform slower on large datasets and/or be more awkward to type than simply making a new column in the usual way with `$` and `=` in `base` R. 

```
customer_survey = mutate(customer_survey, Location=paste(city, stabbr, sep=", "))
# vs
customer_survey$Location = paste(city, stabbr, sep=", ")
```

Still, it can be pretty convenient if you're stringing functions together, as above (so a `%>%` operation would simply be `mutate(Location=paste(city, stabbr, sep=", ")`, making it as easy to type as the normal way), or want to peek at the outcome before running it for results, as below.


## Peeking at the Outcome with `dplyr`

If you want to check on how one or more of the `dplyr` actions will perform in terms of output before updating or creating a dataframe, you can wrap it inside the `head` function. For example:

```
head(select(customer_survey, contains("customer", ignore.case=TRUE)))
```

![](/images/IMAGE.png)

```
head(arrange(customer_survey, region, desc(customer_age), 
    desc(customer_purchases)))
```

![](/images/IMAGE.png)

```
head(mutate(customer_survey, Location = paste(city, stabbr, sep=", "), 
    purchase_proportion=round(customer_purchases/length(customer_purchases),2)))
```

![](/images/IMAGE.png)


## Transforming a Dataframe Between Wide and Long

A lot of analysis and plotting functions require that data be organized in *long* format, where the variable of interest is in one column and the id/categorical portions of the data are repeated as necessary in other columns. Then again, other tools need the data in *wide* format, so being able to move between the two formats is essential. The `reshape2` package provides just that ability.  

### Reshaping: `melt` and `cast`

We'll use a small dataset to illustrate some of the possibilities `reshape2` offers—this shows the verbal and nonverbal IQ test scores for children before and after neurosurgery (and in which hemisphere, i.e. "Side" of the brain). Load the package and the data:

```
require(reshape2)
surgery_outcomes = read.table("surgery_outcomes.csv", header=T, sep=",")
head(surgery_outcomes, 4)

row  ID  Side   Phase  Verbal  Nonverbal
1    1   Right  Post   146     115
2    1   Right  Pre    129     103
3    2   Right  Post   126     115
4    2   Right  Pre    138     115
```

As is, this data is both wide and long. If you're only interested in looking at either Verbal score or Nonverbal scores, it's long, as the other variables are already long. If you want to look at Verbal and Nonverbal together, it's wide. 

Suppose each measurement needs its own row, that is, we need to make this a long data frame so we can compare Verbal and Nonverbal scores. We need to contain those two scores within a single column, such as `IQ_score`. To do this, you `melt` a dataframe from wide into long format:

```
surgery_outcomes_melted = melt(surgery_outcomes, id.vars=c(1:3),
    measure.vars=c(4:5), variable.name="Test_type", value.name="IQ_score")
head(surgery_outcomes_melted, 4)

row  ID  Side   Phase  Test_type  IQ_score
1    1   Right  Post   Verbal     146
2    1   Right  Pre    Verbal     129
3    2   Right  Post   Verbal     126
4    2   Right  Pre    Verbal     138
```

We now have a completely long form data set, as we have the primary variable of analytic interest contained in a single column. 

Any data in long form can be `cast` into a variety of wide forms, with a `d` or an `a` tacked on to the front depending on whether you need a dataframe (`d`) or vector, matrix, or array (`a`). To put this data back into the wide/long shape it started as, use `dcast`:

```
surgery_outcomes_original = dcast(surgery_outcomes_melted, ID + Side + 
    Phase ~ Test_type)
head(surgery_outcomes_original, 4)

row  ID  Side   Phase  Verbal  Nonverbal
1    1   Right  Post   146     115
2    1   Right  Pre    129     103
3    2   Right  Post   126     115
4    2   Right  Pre    138     115
```

Being able to move back and forth between data shapes makes meeting the needs of the R packages you're working with considerably easier, especially if you're moving between some that work best with long form structures and others that work well with short.

The important part to remember in melting data is to ensure you've designated the id and measurement variables appropriately. While `melt` can operate with neither or only one of these designated, proper coding etiquette requires that you designate each specifically to avoid ambiguity.

I> The `tidyr` package is a new, simpler way to reshape data. At the time of this writing, it can accomplish any simple `melt`/`cast` (called `gather`/`spread`) task. However, more complicated data set transformations can fail in `tidyr`; the current consensus is to keep `reshape2` in the toolbox until `tidyr` is mature enough to accomplish the full suite of transformation needs. A useful cheat sheet on `tidyr` is co-packaged with `dplyr` by RStudio [here](http://www.rstudio.com/wp-content/uploads/2015/02/data-wrangling-cheatsheet.pdf), and a more complete treatment is available [here](http://vita.had.co.nz/papers/tidy-data.html) or by checking out the vignette: `vignette("tidy-data")`.


### Reshaping: Crossing Variables with ~ and +

There is tremendous variety in reshaping options using the formula structure in `dcast`. The tilde sign (`~`) separates the variables you want representing the rows (left of the ~) and those you want representing the columns (right of the ~). For example, we can cross phase and the test type in the columns:

```
head(dcast(surgery_outcomes_melted, ID ~ Test_type + Phase), 4)

     ID  Verbal_Post  Verbal_Pre  Nonverbal_Post  Nonverbal_Pre
1    1   146          129         115             103
2    2   126          138         115             115
3    3   110          100         104             100
4    4   104          NA          106             NA
```

We could also return the `Side` variable to the same shape:

```
head(dcast(surgery_outcomes_melted, ID + Side ~ Test_type + Phase), 4)

   ID  Side   Verbal_Post  Verbal_Pre  Nonverbal_Post  Nonverbal_Pre
1  1   Right  146          129         115             103
2  2   Right  126          138         115             115
3  3   Left   110          100         104             100
4  4   Right  104          NA          106             NA
```

Perhaps you only want to use `Phase` to look at Verbal and Nonverbal separately:

```
head(dcast(surgery_outcomes_melted, ID + Phase ~ Test_type), 4)

  ID Phase Verbal Nonverbal
1  1  Post    146       115
2  1   Pre    129       103
3  2  Post    126       115
4  2   Pre    138       115
```

And so on... the possibilities are as many as your variable set...


### Summarizing while Reshaping

While `dplyr` might provide you with all you need for summarizing data once your data structure is set, `reshape2` offers the option to summarize while reshaping. The summarization works based on which operation you choose as well as which side of the `~` you place the variables (as described above).  

```
dcast(surgery_outcomes_melted, Phase ~ Test_type, mean, na.rm=T)

  Phase   Verbal Nonverbal
1  Post 104.6667 102.26667
2   Pre 103.0909  98.90909
```

```
dcast(surgery_outcomes_melted, Side ~ Test_type + Phase, mean, na.rm=T)

   Side Verbal_Post Verbal_Pre Nonverbal_Post Nonverbal_Pre
1  Left    102.4444   97.71429       100.7778      97.42857
2 Right    108.0000  112.50000       104.5000     101.50000
```

```
dcast(surgery_outcomes_melted, Test_type + Side + Phase ~ ., sd, na.rm=T)

  Test_type  Side Phase        .
1    Verbal  Left  Post 14.78269
2    Verbal  Left   Pre 12.51285
3    Verbal Right  Post 23.80756
4    Verbal Right   Pre 24.93324
5 Nonverbal  Left  Post 16.09952
6 Nonverbal  Left   Pre 13.96253
7 Nonverbal Right  Post 11.46734
8 Nonverbal Right   Pre 10.47219
```

We'll explore summarization with `dplyr` in the next chapter. 
