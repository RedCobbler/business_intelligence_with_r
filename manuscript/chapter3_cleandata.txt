# Chapter 3: Cleaning and Preparing Data

- Identifying basic structure
- Cleaning up
- Merging dataframes
- Filtering, selecting, and arranging a dataframe
- Transforming a dataframe from wide to long and back again
- Partitioning the data efficiently for predictive modeling
- Censored data (here or in survival analysis type stuff?)


## Understanding Your Data

### Identifying data types and overall structure

Before anything else—every dataset should be evaluated with a few basic functions. It’s funny how often this is overlooked…but it’s not funny how much time you can waste writing code that fails until you realize you don’t have the data in the proper format.

We saw the frequent use of `str` in the Chapter 1 example to understand our dataset and to ensure our changes worked as expected. `head` (and `tail`), `summary`, and `complete.cases` are a few other important functions that help you understand your data set as seen by R. 

Factors, characters, dates, values that are out-of-range for the measurement, inappropriate dummy values, and NAs tend to be the data types or results that will screw up future analyses—so using the the following functions, every time you first pull in a dataset, will save a lot of time and grief in the long run.

Using the same power usage data from Chapter 1, we might explore our data one or more times a session with the following functions

```
# View the structure of the dataset, particularly data types
str(power)

# Look at the first (or last) 6 lines of the data frame
head(power)
tail(power)

# Obtain basic summary stats
summary(power)

# Look at the rows with missing data
power[!complete.cases(power),]
```

It may seem like this is too basic to be included in a cookbook aimed at people who are already R users—but to illustrate the point ironically, a few days before I started this chapter, I was trying to use a function that needed factors on a variable I just knew were factors, but R had considered them characters because as I had used the `stringsAsFactors=FALSE` option when I imported the data. After banging my head for a while, I pulled up `str` and the problem became clear immediately.

Always check! Even if you’re a seasoned professional!


### Sorting 

order (one or more variables)
order(..., na.last = TRUE, decreasing = FALSE)


sort (one variable only)
sort(x, decreasing = FALSE, na.last = NA, ...)


### Identifying (and ordering) factor levels

levels
ordered


### Identifying unique values or duplicates

unique

duplicated (GIVES TRUE/FALSE)




duplicated(d2)

d2[duplicated(d2), ]

[1] FOR_MONTH                   DISCHARGES                 
[3] IP_CENSUS_OR_DISCHARGE_UNIT
<0 rows> (or 0-length row.names)


QUOTE: Some analytics questions don't require unique identifiers, and you end up with a dataset that has more rows than you really need. Deleting duplicate rows is easy—first, delete the columns you don't need (as otherwise `unique` will consider them as part of the comparison process), then use `unique` to remove the remaining duplicates, e.g.: 

```
deduped_dataframe = unique(dataframe)
```


## Cleaning up

### Cleaning up a web-scraped table

In many cases the data frame will require a little clean up after it's loaded. Messy or unclear column names, pattern replacement, converting between data types, and extracting portions of variables' results are all common cleaning needs before analysis. 

`gsub` is probably the most useful general purpose approach to regular expressions in R. You should look at the help files for `?gsub` for regular expressions, `?strsplit` for splitting a character string, and `?substr` for extracting a portion of a string (as well as the links in their *See Also* sections) to get more details on those and similar or otherwise related functions.

There are many possible approaches to cleaning, so as an example, the following code does some cleaning of the Pro Publica *Nursing Home Deficiencies* data set scraped from the web in the previous chapter, using a few different functions:

```
colnames(nursing_home_deficiencies) = c("Date", "Home", "City", "State", 
    "Deficiency_Count", "Severity")
nursing_home_deficiencies$State = gsub("Tex.", "TX", 
    nursing_home_deficiencies$State, fixed = TRUE)
nursing_home_deficiencies$Home = gsub(" (REPORT)  Home Info", "", 
    nursing_home_deficiencies$Home, fixed = TRUE)
nursing_home_deficiencies$Severity = substr(nursing_home_deficiencies$Severity,
    1, 1)
nursing_home_deficiencies$Date = gsub(".", "", nursing_home_deficiencies$Date,
    fixed = TRUE)
clean = function(col) {
    col = gsub('Jan', 'January', col, fixed = TRUE)
    col = gsub('Feb', 'February', col, fixed = TRUE)
    col = gsub('Aug', 'August', col, fixed = TRUE)
    col = gsub('Sept', 'September', col, fixed = TRUE)
    col = gsub('Oct', 'October', col, fixed = TRUE)
    col = gsub('Nov', 'November', col, fixed = TRUE)
    col = gsub('Dec', 'December', col, fixed = TRUE)
    return(col)
  }
nursing_home_deficiencies$Date = clean(nursing_home_deficiencies$Date)
nursing_home_deficiencies$Date = as.Date(nursing_home_deficiencies$Date, 
    format="%B %d, %Y")
```

The use of `fixed = TRUE` option above tells `gsub` to use exact matching; the default is `false`, which uses POSIX 1003.2 extended regular expressions. 


### Missing values: Dummy NA values and dropping the observations with NAs

Sometimes, you’ll find values or placeholders where no data should be; for example, -99 and 9999 are common among older data sets to represent NAs. To fix this, you can use `gsub`, but you could also do something like:

```
my_data$my_variable[my_data$my_variable==9999] = NA 
```

There are a variety of ways to deal with missing values—and we’ll explore some later in this chapter, under *Imputation*—but the simplest is to just remove the rows that contain them:

```
my_data = na.omit(my_data)
```

It’s also useful to remember to use `na.rm=TRUE` when including data with NA values in calls to many base installation functions (e.g., `mean`).


### Rounding

```
round(OBJECT, DIGITS)
```


### Concatenation

paste 
paste0


### Converting between data types

In the previous chapter, we saw a JSON import that turned all of the variables into character data types. While this result is relatively rare, there is almost *always* a need to convert a column from one data type to another. After you've used `str` to figure out what type(s) of data you have, you can use conversion functions to change them as you need. The following are the primary conversion functions:

| Data type you want | Function |
| ------------------ | -------- |
| Character / String | `as.character(OBJECT, ...)` |
| Factor / Category | `as.factor(OBJECT, ...)` |
| Numeric / Double | `as.numeric(OBJECT, ...)` |
| Integer | `as.integer(OBJECT, ...)` |
| Date | `as.Date(OBJECT, format="yyyy-mm-dd", ...)` |
| Datetime | `as.POSIXct(OBJECT, tz="CURRENT TIME ZONE", ...)` |

Note that conversion to and between dates and times can be tricky, depending on the structure of the existing date/time variable. If it's already in standard ISO format (e.g., `yyyy-mm-dd`), these basic conversions will work fine. If not, you'll have to mess with the `format` option and/or use other techniques. See Chapter 8 (*Trends and Time*) for lots of details on working with and converting date and time values. 

Commas in values (Excel, looking at you). 
dataset$Variable2=as.numeric(paste(gsub(",", "", dataset$Variable)))




## Imputation

When you disregard cases with any missing variables, you lose useful information that the nonmissing values in that case convey. You may sometimes want to impute reasonable values (those that will not skew the results of analyses very much) for the missing values.

Read data and replace missing values with mean:

> dat <- read.csv("missing-data.csv", na.strings = "")
> dat$Income.imp.mean <- ifelse(is.na(dat$Income), mean(dat$Income, na.rm=TRUE), dat$Income)



Imputing random values sampled from nonmissing values

If you want to impute random values sampled from the nonmissing values of the variable, you can use the following two functions:

rand.impute <- function(a) {
  missing <- is.na(a)
  n.missing <- sum(missing)
  a.obs <- a[!missing]
  imputed <- a
  imputed[missing] <- sample (a.obs, n.missing, replace=TRUE)
  return (imputed)
}

random.impute.data.frame <- function(dat, cols) {
  nms <- names(dat)
  for(col in cols) {
    name <- paste(nms[col],".imputed", sep = "")
    dat[name] <- rand.impute(dat[,col])
  }
  dat
}

dat <- read.csv("missing-data.csv", na.strings="")
> random.impute.data.frame(dat, c(1,2))










## Scaling

The scale() function takes two optional arguments, center and scale, whose default values are TRUE. The following table shows the effect of these arguments:

Argument
	

Effect

center = TRUE, scale = TRUE
	

Default behavior described earlier

center = TRUE, scale = FALSE
	

From each value, subtract the mean of the concerned variable

center = FALSE, scale = TRUE
	

Divide each value by the root mean square of the associated variable, where root mean square is sqrt(sum(x^2)/(n-1))

center = FALSE, scale = FALSE
	

Return the original values unchanged



## DOUBLE CHECK ON REDUCING TO ONLY NUMERIC


## Merging dataframes

We've already touched on merging tables inside a SQLite database to create a dataframe, but more often you usually need to merge two dataframes inside R.

Perhaps the best package to do this in R is `dplyr`, and part of its greatness is that it works on dataframes in the same way as it does on databases. However, SQL jockies will prefer to use `sqldf`. I tend to use the former in R-heavy projects and the latter in projects where I need to work on lots of database tables or want to streamline the join results. Of course, `base` R has joining abilities as well via the `merge` function.   

### Joins

The recipe provides an overview of joining tables via dplyr's `\*_join` functions, shows a little more on doing joins using `sqldf`, and reviews R's built-in `merge` function. We'll also cover the concepts of how the types of joins work to make it easy to visualize what we are trying to accomplish.

First, load the libraries:

```
require(dplyr)
require(sqldf)
```

We'll use the fake data we created in the previous recipe to illustrate these methods.

Our objective is to merge the four tables so that we have the customer data alongside an expanded view of their location and their answer from the survey.

```
customer_locations = left_join(customers, zipcodes, by = 
  c("customer_zip" = "zip")
customer_regions = left_join(customer_locations, state_regions, by= 
  c("stabbr" = "abb")
customer_survey = left_join(customer_regions, survey, by = 
  c("customer_id" = "customer_id")
```

The base package's `merge` function is the old standby, but is considerably slower than `dplyr`, and the type of join is implied by the all option, which reduces code clarity.

```
customer_locations = merge(customers, zipcodes, by.x = "customer_zip", 
  by.y = "zip", all.x = TRUE)
customer_regions = merge(customer_locations, state_regions, by.x = "stabbr", 
  by.y = "abb", all.x = TRUE)
customer_survey = merge(customer_regions, survey, by.x="customer_id", 
  by.y="customer_id", all.x=TRUE)
```

With both the `dplyr` `*_join` and the `base` `merge` functions, you get everything in both dataframes. You either have to select only the columns you want in a new dataframe before the merge, or remove the excess columns after it. Of course, using SQL you can choose just the columns you want, which is illustrated below.

```
customer_survey = sqldf("SELECT
    customers.*
    , zipcodes.city
    , state_regions.state
    , state_regions.region
    , state_regions.division
    , survey.strongly_agree
    , survey.agree
    , survey.neutral
    , survey.disagree
    , survey.strongly_disagree
    FROM customers
    LEFT JOIN zipcodes
        ON customers.customer_zip = zipcodes.zip
    LEFT JOIN state_regions
        ON zipcodes.stabbr = state_regions.abb
    LEFT JOIN survey
        ON customers.customer_id = survey.customer_id"
    , stringsAsFactors=FALSE)
```

If you're coming to R with some SQL in your background, you might wonder why this wasn't the primary method of this recipe—after all, we can do it all in one step, and we get exactly what we want, and we can sort (`ORDER BY`) or add any other SQL statement here to get what we need. The reason is that showing each join/merge in a single step allows other R coders to see exactly what was done more clearly. SQL merge code can get ugly fast, so it's useful to consider whether the project (=code) is going to be passed on to others in the future and whether they are familiar with SQL, and code accordingly.

That said, I love using this approach. With a small set of tables, there's probably no reason not to do it this way. With a large set, you might consider this as your *primary* approach to joining if downstream users are familiar with SQL and/or there's a chance the code will be ported or used in a SQL environment.

R will match on columns with the same names, so while you can get away without specifying the by variables, you should always specify them—remember your downstream users!

Obviously, there's more to life than left joins. The following table lays out the different types of joins, and how `dplyr`, `merge`, and `sqldf` perform each, given data frames/tables *A* and *B*.

| Type of Join | Visualization | dplyr | merge | sqldf |
| ------------ | ------------- | ----- | ----- | ----- |
| Left | ![Left join](/images/Ch3_sql_01.png) | `left_join(A, B, by=c("A.key" ="B.key"))` | `merge(A, B, by.x="A.key", by.y="B.key", all.x=TRUE)` | `SELECT * FROM A LEFT JOIN B ON A.key = B.key` |
| Inner | ![Inner join](/images/Ch3_sql_02.png) | `inner_join(A, B, by=c("A.key" ="B.key"))` | `merge(A, B, by.x="A.key", by.y="B.key", all=FALSE)` | `SELECT * FROM A INNER JOIN B ON A.key = B.key` |
| Full (Outer) | ![Full join](/images/Ch3_sql_03.png) | *Not yet implemented* | `merge(A, B, by.x="A.key", by.y="B.key", all=TRUE)` | `SELECT * FROM A FULL OUTER JOIN B ON A.key = B.key` | 
| Semi | ![Semi join](/images/Ch3_sql_04.png) | `semi_join(A, B, by=c("A.key" ="B.key"))` | *n/a* | `SELECT * FROM A LEFT JOIN B ON A.key = B.key WHERE B.key iS NULL` | 
| Anti | ![Anti join](/images/Ch3_sql_05.png) | `anti_join(A, B, by=c("A.key" ="B.key"))` | *n/a* | `SELECT * FROM A FULL OUTER JOIN B ON A.key = B.key WHERE A.key is NULL OR B.key iS NULL` | 
| Right* | ![Right join](/images/Ch3_sql_06.png) | `left_join(B, A, by=c("B.key" ="A.key"))` | `merge(A, B, by.x="A.key", by.y="B.key", all.y=TRUE)` | `SELECT * FROM A RIGHT JOIN B ON A.key = B.key`|
| Right Anti* | ![Right anti join](/images/Ch3_sql_07.png) | `semi_join(B, A, by=c("B.key" ="A.key"))` | *n/a* | `SELECT * FROM A RIGHT JOIN B ON A.key = B.key WHERE A.key IS NULL`

\* Although `dplyr` doesn't have a right join function, simply reversing the table order and using left join serves the same purpose. Though why you'd want to use a right join anyway is beyond me—right joins are just backwards left joins and it makes more sense to focus on your primary dataset as the "A" table.

*Note*: you can merge large files with `data.tables`, but at the time of this writing, the way you specify the joins is opaque—e.g., `datatable_AB = datatable_A[datatable_B]` is a left join, given you've specified the keys ahead of time—so for now we recommend that it only be employed for single-user/casual use and not for production code.


### Unions and bindings

Concatenating two tables can be handled two ways: via `sqldf` or through R's native binding functions. 

TABLE WITH SQUARE VENNS
| Type of Join | Visualization | *bind | sqldf |
| ------------ | ------------- | ----- | ----- |
| Union | ![union](/images/Ch3_sql_01.png) | `rbind(A, B)` | `SELECT * FROM A UNION SELECT * FROM B` |
| Semi-union | ![semi union](/images/Ch3_sql_01.png) | *see below* | *see below* |
| Concatenate | ![concat](/images/Ch3_sql_01.png) | `cbind(A, B)` | *n/a* |

If you need to stack two tables with the same columns—`UNION` in SQL terms—and prefer the SQL approach (and/or already have `sqldf` loaded), just include it in your query, e.g.:

```
EXAMPLE SQL QUERY WITH UNION
```

It's often easier just to use the native R functions if you already have the two tables ready to concatenate: `rbind` for stacking and `cbind` for concatenating *sideways*. Being ready means that the columns are identically named, and that there are the same number of columns in each table. 

The above SQL `UNION` above would be done with `rbind` as follows:

```
rbind(A, B)
```

Binding two tables together side-by-side is done with the `cbind` function. `cbind` is *row-blind*: it will line up the two tables *in the order that they occur*. It is up to you to ensure the observations (rows) match across the two data frames. The two data frames must also have the same numbers of observations (i.e., the same length). 

```
cbind(A,B)
```

Add NA columns to allow stacking with different #s of columns

See example in Chapter 1


## Filtering, selecting, and arranging a dataframe

Even after joining, the dataset we have is rarely the dataset we need to complete a project. The `dplyr` package has a few really useful data manipulation functions that work on data frames just like they do on databases, and tend to be simpler, faster, and more intuitive than comparable `base` installation functions like `merge`, `order`, or `subset`.

We'll use the same fake data generated previously to illustrate subsetting rows, selecting columns, and arranging the dataframe based on custom sorting order.

```
require(dplyr)
```

As we saw above in the *Joins* section, `dplyr` contains SQL-like agility with much less coding for common R tasks.

Need to subset on a couple of conditions? Use `filter`, stringing together criteria using `&` for AND and `|` (pipe) for OR:

```
customers_marketing = filter(customer_survey, strongly_agree == 1 | agree == 1)
customers_CA_65_200_OR_500 = filter(customer_survey, state == "California" &
  customer_age >= 65 & customer_purchases >= 200 | state == "Oregon" &
  customer_purchases > 500)
```

Need to reduce the data to just some of the variables without having to spell them all out? Use `select`, which provides some "regular expression" type of access to make selection from a large dataset more managable:

```
customers_only = select(customer_survey, starts_with("customer"))
customers_only_agreement = select(customer_survey, customer_id, matches("agree"))
```

`select` can also remove variables by appending a minus sign (`–`) to the front of the criteria:  

```
customers_excluded = select(customer_survey, -starts_with("customer"))
```

Want to sort by a set of variables? Use `arrange`:

```
customers_sorted = arrange(customers_CA_65_200_OR_500, state, customer_age, 
  desc(customer_purchases))
```

Definitely explore the vignettes in the `dplyr` package (start with `vignette("introduction", package="dplyr"))`, as they provide a wide variety of examples and use cases—there's almost certainly going to be an example of what you need to do there.

## Stringing it together in dplyr

One of the really useful additional features of `dplyr` is that you can string together a bunch of functions with the `%>%` ("then") operator, a feature that is growing in popularity across a variety of R packages.

```
customer_500_dplyred = customer_survey %>%
    filter(customer_purchases >= 500) %>%
    group_by(state) %>%
    summarize(count_purchases = n(),
              mean_purchases = mean(customer_purchases, na.rm=T),
              sd_purchases = sd(customer_purchases, na.rm=T),
              sum_est = sum(est)) %>%
    mutate(cv_purchases = round(sd_purchases / mean_purchases, 2))
```

dplyr's `mutate` function creates a new column, but can perform slower on large datasets and/or be more awkward to type than simply making a new column in the usual way with `$` and `=`. For example:

```
customer_survey = mutate(customer_survey, Location=paste(city, stabbr, sep=", "))
# vs
customer_survey$Location = paste(city, stabbr, sep=", ")
```

Still, it can be pretty convenient if you're stringing functions together, as above (so a `%>%` operation would simply be `mutate(Location=paste(city, stabbr, sep=", ")`, making it as easy to type as the normal way), or want to peek at the outcome before running it for results, as below.

## Peeking at the outcome

If you want to check on how one or more of the `dplyr` actions will perform in terms of output before updating or creating a dataframe, you can wrap it inside the head function:

```
head(select(customer_survey, contains("customer", ignore.case=TRUE)))
head(arrange(customer_survey, region, desc(customer_age), desc(customer_purchases)))
head(mutate(customer_survey, Location = paste(city, stabbr, sep=", "), 
  purchase_proportion=round(customer_purchases/length(customer_purchases),2)))
```

## Transforming a dataframe from wide to long and back again

A lot of analyses and plotting require that data be organized in long format, where the variable of interest is in one column and the id/categorical portions of the data are repeated as necessary in other columns. Then again, other tools need the data in wide format, so being able to move between the two formats is essential. The `reshape2` package provides just that ability.  

We'll use a different dataset to illustrate some of the possibilities `reshape2` offers—this shows the verbal and nonverbal IQ test scores for children before and after neurosurgery (and in which hemisphere, i.e. "Side" of the brain). Load the package and the data:

```
require(reshape2)
surgery_outcomes = read.table("surgery_outcomes.csv", header=T, sep=",")
head(surgery_outcomes, 4)

row  ID  Side   Phase  Verbal  Nonverbal
1    1   Right  Post   146     115
2    1   Right  Pre    129     103
3    2   Right  Post   126     115
4    2   Right  Pre    138     115
```

If each measurement needs its own row, you melt a dataframe from wide into long format:

```
surgery_outcomes_melted = melt(surgery_outcomes, id.vars=c(1:3),
    measure.vars=c(4:5), variable.name="Test_type", value.name="IQ_score")
head(surgery_outcomes_melted, 4)

row  ID  Side   Phase  Test_type  IQ_score
1    1   Right  Post   Verbal     146
2    1   Right  Pre    Verbal     129
3    2   Right  Post   Verbal     126
4    2   Right  Pre    Verbal     138
```

Any data in long form can be cast into a variety of wide forms, with a `d` or an `a` tacked on to the front depending on whether you need a dataframe (`d`) or vector, matrix, or array (`a`). To put this data back in the shape it started as:

```
surgery_outcomes_original = dcast(surgery_outcomes_melted, ID + Side + 
    Phase ~ Test_type)
head(surgery_outcomes_original, 4)

row  ID  Side   Phase  Verbal  Nonverbal
1    1   Right  Post   146     115
2    1   Right  Pre    129     103
3    2   Right  Post   126     115
4    2   Right  Pre    138     115
```

Being able to move back and forth between data shapes makes meeting the needs of the R packages you're working with considerably easier, especially if you're moving between some that work best with long form structures and others that work well with short.

The important part to remember in melting data is to ensure you've designated the id and measurement variables appropriately. While `melt` can operate with neither or only one of these designated, proper coding etiquette requires that you designate each specifically to avoid ambiguity.

### Crossing variables with ~ and +

There is tremendous variety in reshaping options using the formula structure in `dcast`. The tilde sign (`~`) separates the variables you want representing the rows (left of the ~) and those you want representing the columns (right of the ~). For example, we can cross phase and the test type in the columns:

```
head(dcast(surgery_outcomes_melted, ID ~ Test_type + Phase), 4)

     ID  Verbal_Post  Verbal_Pre  Nonverbal_Post  Nonverbal_Pre
1    1   146          129         115             103
2    2   126          138         115             115
3    3   110          100         104             100
4    4   104          NA          106             NA
```

We could also return the `Side` variable to the same shape:

```
head(dcast(surgery_outcomes_melted, ID + Side ~ Test_type + Phase), 4)

   ID  Side   Verbal_Post  Verbal_Pre  Nonverbal_Post  Nonverbal_Pre
1  1   Right  146          129         115             103
2  2   Right  126          138         115             115
3  3   Left   110          100         104             100
4  4   Right  104          NA          106             NA
```

And so on... the possibilities are as many as your variable set...

```
head(dcast(surgery_outcomes_melted, ID + Phase ~ Test_type), 4)

  ID Phase Verbal Nonverbal
1  1  Post    146       115
2  1   Pre    129       103
3  2  Post    126       115
4  2   Pre    138       115
```

### Summarizing while reshaping

While `dplyr` might provide you with all you need for summarizing data once your data structure is set, `reshape2` offers the option to summarize while reshaping. The summarization works based on which operation you choose as well as which side of the `~` you place the variables (as described above).  

```
dcast(surgery_outcomes_melted, Phase ~ Test_type, mean, na.rm=T)

  Phase   Verbal Nonverbal
1  Post 104.6667 102.26667
2   Pre 103.0909  98.90909
```

```
dcast(surgery_outcomes_melted, Side ~ Test_type + Phase, mean, na.rm=T)

   Side Verbal_Post Verbal_Pre Nonverbal_Post Nonverbal_Pre
1  Left    102.4444   97.71429       100.7778      97.42857
2 Right    108.0000  112.50000       104.5000     101.50000
```

```
dcast(surgery_outcomes_melted, Test_type + Side + Phase ~ ., sd, na.rm=T)

  Test_type  Side Phase        .
1    Verbal  Left  Post 14.78269
2    Verbal  Left   Pre 12.51285
3    Verbal Right  Post 23.80756
4    Verbal Right   Pre 24.93324
5 Nonverbal  Left  Post 16.09952
6 Nonverbal  Left   Pre 13.96253
7 Nonverbal Right  Post 11.46734
8 Nonverbal Right   Pre 10.47219
```
