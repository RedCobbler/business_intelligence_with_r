# Chapter 3: Cleaning and Preparing Data

- Identifying basic structure
- Merging dataframes
- Filtering, selecting, and arranging a dataframe
- Transforming a dataframe from wide to long and back again
- Partitioning the data efficiently for predictive modeling
- Censored data (here or in survival analysis type stuff?)

It's often tempting to jump into analysis as soon as possible, but it is a cardinal sin to not evaluate your data with the basics first. This chapter covers the evaluations that must be done before anything else.

In this chapter, we'll primarily use the same bike share data we downloaded from the UCI Machine Learning Repository in the Loading data recipe of Chapter 2. To make it easy, the code below downloads the particular file we'll use throughout this chapter and writes the data to a csv file saved in your working directory. The first recipe then loads the csv, adjusts the column types appropriately, and runs through the essential first step of any analysis.

```
tf = tempfile()
download.file("http://archive.ics.uci.edu/ml/machine-learning-databases/00275
  /Bike-Sharing-Dataset.zip", tf)
bike_share_daily_raw = read.table(unz(tf, "day.csv"), header=TRUE, sep=",")
file.remove(tf)
write.table(bike_share_daily_raw, “bike_share_daily.csv”, row.names=FALSE)
rm(bike_share_daily_raw)
```

## Every time, every dataset: IDENTIFY ITS BASIC STRUCTURE

Before anything else—every dataset should be evaluated with a few basic functions. It’s funny how often this is overlooked…but it’s not funny how much time you can waste writing code that fails until you realize you don’t have the data in the proper format.

We'll use the the Washington DC bike share data downloaded and written to a csv in the introduction to this chapter, then apply some meaningful values for most of the summary variables:

```
bike_share_daily=read.table("bike_share_daily.csv", sep=",", header=T, 
  colClasses=c("character", "Date", "factor", "factor", "factor", "factor", 
  "factor", "factor", "factor", "numeric", "numeric", "numeric", "numeric",
  "integer", "integer", "integer"))
levels(bike_share_daily$season) = c("Winter", "Spring", "Summer", "Fall")
levels(bike_share_daily$workingday) = c("No", "Yes")
levels(bike_share_daily$holiday) = c("No", "Yes")
bike_share_daily$mnth = as.factor(as.numeric(bike_share_daily$mnth))
levels(bike_share_daily$mnth) = c(month.abb)
levels(bike_share_daily$yr) = c(2011, 2012)
```

Understand the dataset's structure and basic statistics first:

```
str(bike_share_daily)
head(bike_share_daily)
summary(bike_share_daily)
bike_share_daily[!complete.cases(bike_share_daily),]
```

Factors, characters, dates, values that are out-of-range for the measurement, inappropriate dummy values, and NAs tend to be the data types or results that will screw up future analyses—so using the these functions, every time you first pull in a dataset, will save a lot of time and grief in the long run.

It may seem like this is too basic to be included in a cookbook aimed at people who are already R users—but to illustrate the point ironically, a few days before I started this chapter, I was trying to use a function that needed factors on a variable I just knew were factors, but R had considered them characters because as I had used the `stringsAsFactors=F` option when I imported the data. After banging my head for a while, I pulled up `str` and the problem became clear immediately.

Always check! Even if you’re a seasoned professional!

## Missing values: Dummy NA values and dropping the NAs

Sometimes, you’ll find values or placeholders where no data should be; for example, -99 and 9999 are common among older data sets to represent NAs. To fix this, you can use something like:

```
my_data$my_variable[my_data$my_variable==9999] = NA 
```

There are a variety of ways to deal with missing values—and we’ll explore some later in this book—but the simplest is to just remove the rows that contain them:

```
my_data = na.omit(my_data)
```

It’s also useful to remember to use `na.rm=TRUE` when including data with NA values in calls to many base installation functions (e.g., mean).

## Imputation

> dat <- read.csv("missing-data.csv", na.strings = "")
> dat$Income.imp.mean <- ifelse(is.na(dat$Income), mean(dat$Income, na.rm=TRUE), dat$Income)

rand.impute <- function(a) {
Replacing missing values with the mean

When you disregard cases with any missing variables, you lose useful information that the nonmissing values in that case convey. You may sometimes want to impute reasonable values (those that will not skew the results of analyses very much) for the missing values.
Getting ready

Download the missing-data.csv file and store it in your R environment's working directory.
How to do it...

Read data and replace missing values:

> dat <- read.csv("missing-data.csv", na.strings = "")
> dat$Income.imp.mean <- ifelse(is.na(dat$Income), mean(dat$Income, na.rm=TRUE), dat$Income)

After this, all the NA values for Income will now be the mean value prior to imputation.
How it works...

The preceding ifelse() function returns the imputed mean value if its first argument is NA. Otherwise, it returns the first argument.
There's more...

You cannot impute the mean when a categorical variable has missing values, so you need a different approach. Even for numeric variables, we might sometimes not want to impute the mean for missing values. We discuss an often used approach here.
Imputing random values sampled from nonmissing values

If you want to impute random values sampled from the nonmissing values of the variable, you can use the following two functions:

rand.impute <- function(a) {
  missing <- is.na(a)
  n.missing <- sum(missing)
  a.obs <- a[!missing]
  imputed <- a
  imputed[missing] <- sample (a.obs, n.missing, replace=TRUE)
  return (imputed)
}

random.impute.data.frame <- function(dat, cols) {
  nms <- names(dat)
  for(col in cols) {
    name <- paste(nms[col],".imputed", sep = "")
    dat[name] <- rand.impute(dat[,col])
  }
  dat
}

dat <- read.csv("missing-data.csv", na.strings="")
> random.impute.data.frame(dat, c(1,2))




## Dropping duplicates

Some analytics questions don't require unique identifiers, and you end up with a dataset that has more rows than you really need. Deleting duplicate rows is easy—first, delete the columns you don't need, then use unique to remove the remaining duplicates, e.g.: 



duplicated(DF)
prospect[duplicated(prospect), ]

```
deduped_df = unique(df)
```





## Scaling

The scale() function takes two optional arguments, center and scale, whose default values are TRUE. The following table shows the effect of these arguments:

Argument
	

Effect

center = TRUE, scale = TRUE
	

Default behavior described earlier

center = TRUE, scale = FALSE
	

From each value, subtract the mean of the concerned variable

center = FALSE, scale = TRUE
	

Divide each value by the root mean square of the associated variable, where root mean square is sqrt(sum(x^2)/(n-1))

center = FALSE, scale = FALSE
	

Return the original values unchanged



## DOUBLE CHECK ON REDUCING TO ONLY NUMERIC


## Merging dataframes

We've already touched on merging tables inside a SQLite database to create a dataframe, but more often you usually need to merge two dataframes inside R.

Hadley Wickham is well known in the data science world largely for his `ggplot2` package, which many professionals (and this book) use extensively. But he and other collaborators have developed a suite of other packages that significantly reduce the mental overhead for a wide variety of common R uses. One of these is `dplyr` for data manipulation, and part of its greatness is that it works on dataframes in the same way as it does on databases.

The recipe provides an overview of dplyr's `_join` functions, shows a little more on doing joins using `sqldf`, and reviews R's built-in `merge` function. It also covers the concepts of how the types of joins work to make it easy to visualize what you are trying to accomplish.

First, load the libraries:

```
require(dplyr)
require(sqldf)
```

We'll use the fake data we created in the previous recipe to illustrate these methods.

Our objective is to merge the four tables so that we have the customer data alongside an expanded view of their location and their answer from the survey.

```
customer_locations = left_join(customers, zipcodes, by = 
  c("customer_zip" = "zip")
customer_regions = left_join(customer_locations, state_regions, by= 
  c("stabbr" = "abb")
customer_survey = left_join(customer_regions, survey, by = 
  c("customer_id" = "customer_id")
```

The base package's `merge` function is the old standby, but is considerably slower than `dplyr`, and the type of join is implied by the all option, which reduces code clarity.

```
customer_locations = merge(customers, zipcodes, by.x = "customer_zip", 
  by.y = "zip", all.x = TRUE)
customer_regions = merge(customer_locations, state_regions, by.x = "stabbr", 
  by.y = "abb", all.x = TRUE)
customer_survey = merge(customer_regions, survey, by.x="customer_id", 
  by.y="customer_id", all.x=TRUE)
```

With both the `dplyr` `*_join` and `merge` functions, you get everything in both dataframes. You either have to select only the columns you want in a new dataframe before the merge, or remove the excess columns after it. Of course, using SQL you can choose just the columns you want, which is illustrated below.

```
customer_survey = sqldf("SELECT
    customers.*
    , zipcodes.city
    , state_regions.state
    , state_regions.region
    , state_regions.division
    , survey.strongly_agree
    , survey.agree
    , survey.neutral
    , survey.disagree
    , survey.strongly_disagree
    FROM customers
    LEFT JOIN zipcodes
        ON customers.customer_zip = zipcodes.zip
    LEFT JOIN state_regions
        ON zipcodes.stabbr = state_regions.abb
    LEFT JOIN survey
        ON customers.customer_id = survey.customer_id"
    , stringsAsFactors=FALSE)
```

If you're coming to R with some SQL in your background, you might wonder why this wasn't the primary method of this recipe—after all, we can do it all in one step, and we get exactly what we want, and we can sort (`ORDER BY`) or add any other SQL statement here to get what we need. The reason is that showing each join/merge in a single step allows other R coders to see exactly what was done more clearly. SQL merge code can get ugly fast, so it's useful to consider whether the project (=code) is going to be passed on to others in the future and whether they are familiar with SQL, and code accordingly.

That said, I love using this approach. With a small set of tables, there's probably no reason not to do it this way. With a large set, you might consider this as your *primary* approach to joining if downstream users are familiar with SQL and/or there's a chance the code will be ported or used in a SQL environment.

R will match on columns with the same names, so while you can get away without specifying the by variables, you should always specify them—remember your downstream users!

Obviously, there's more to life than left joins. The following table lays out the different types of joins, and how `dplyr`, `merge`, and `sqldf` perform each, given data frames/tables *A* and *B*.

| Type of Join | Visualization | dplyr | merge | sqldf |
| ------------ | ------------- | ----- | ----- | ----- |
| Left | ![Left join](/images/Ch3_sql_01.png) | `left_join(A, B, by=c("A.key" ="B.key"))` | `merge(A, B, by.x="A.key", by.y="B.key", all.x=TRUE)` | `SELECT * FROM A LEFT JOIN B ON A.key = B.key` |
| Inner | ![Inner join](/images/Ch3_sql_02.png) | `inner_join(A, B, by=c("A.key" ="B.key"))` | `merge(A, B, by.x="A.key", by.y="B.key", all=FALSE)` | `SELECT * FROM A INNER JOIN B ON A.key = B.key` |
| Full (Outer) | ![Full join](/images/Ch3_sql_03.png) | *Not yet implemented* | `merge(A, B, by.x="A.key", by.y="B.key", all=TRUE)` | `SELECT * FROM A FULL OUTER JOIN B ON A.key = B.key` | 
| Semi | ![Semi join](/images/Ch3_sql_04.png) | `semi_join(A, B, by=c("A.key" ="B.key"))` | *n/a* | `SELECT * FROM A LEFT JOIN B ON A.key = B.key WHERE B.key iS NULL` | 
| Anti | ![Anti join](/images/Ch3_sql_05.png) | `anti_join(A, B, by=c("A.key" ="B.key"))` | *n/a* | `SELECT * FROM A FULL OUTER JOIN B ON A.key = B.key WHERE A.key is NULL OR B.key iS NULL` | 
| Right* | ![Right join](/images/Ch3_sql_06.png) | `left_join(B, A, by=c("B.key" ="A.key"))` | `merge(A, B, by.x="A.key", by.y="B.key", all.y=TRUE)` | `SELECT * FROM A RIGHT JOIN B ON A.key = B.key`|
| Right Anti* | ![Right anti join](/images/Ch3_sql_07.png) | `semi_join(B, A, by=c("B.key" ="A.key"))` | *n/a* | `SELECT * FROM A RIGHT JOIN B ON A.key = B.key WHERE A.key IS NULL`

\* Although `dplyr` doesn't have a right join function, simply reversing the table order and using left join serves the same purpose. Though why you'd want to use a right join anyway is beyond me—right joins are just backwards left joins and it makes more sense to focus on your primary dataset as the "A" table.

*Note*: you can merge large files with `data.tables`, but at the time of this writing, the way you specify the joins is opaque—e.g., `datatable_AB = datatable_A[datatable_B]` is a left join, given you've specified the keys ahead of time—so for now we recommend that it only be employed for single-user/casual use and not for production code.

## Filtering, selecting, and arranging a dataframe

Even after joining, the dataset we have is rarely the dataset we need to complete a project. The `dplyr` package has a few really useful data manipulation functions that work on data frames just like they do on databases, and tend to be simpler, faster, and more intuitive than comparable `base` installation functions like `merge`, `order`, or `subset`.

We'll use the same fake data generated previously to illustrate subsetting rows, selecting columns, and arranging the dataframe based on custom sorting order.

```
require(dplyr)
```

dplyr contains SQL-like agility with much less coding for common R tasks.

Need to subset on a couple of conditions? Use `filter`, stringing together criteria using `&` for AND and `|` (pipe) for OR:

```
customers_marketing = filter(customer_survey, strongly_agree == 1 | agree == 1)
customers_CA_65_200_OR_500 = filter(customer_survey, state == "California" &
  customer_age >= 65 & customer_purchases >= 200 | state == "Oregon" &
  customer_purchases > 500)
```

Need to reduce the data to just some of the variables without having to spell them all out? Use `select`, which provides some "regular expression" type of access to make selection from a large dataset more managable:

```
customers_only = select(customer_survey, starts_with("customer"))
customers_only_agreement = select(customer_survey, customer_id, matches("agree"))
```

`select` can also remove variables by appending a minus sign (`–`) to the front of the criteria:  

```
customers_excluded = select(customer_survey, -starts_with("customer"))
```

Want to sort by a set of variables? Use `arrange`:

```
customers_sorted = arrange(customers_CA_65_200_OR_500, state, customer_age, 
  desc(customer_purchases))
```

Definitely explore the vignettes in the `dplyr` package (start with `vignette("introduction", package="dplyr"))`, as they provide a wide variety of examples and use cases—there's almost certainly going to be an example of what you need to do there.

## Stringing it together in dplyr

One of the really useful additional features of `dplyr` is that you can string together a bunch of functions with the `%>%` ("then") operator, a feature that is growing in popularity across a variety of R packages.

```
customer_500_dplyred = customer_survey %>%
    filter(customer_purchases >= 500) %>%
    group_by(state) %>%
    summarize(count_purchases = n(),
              mean_purchases = mean(customer_purchases, na.rm=T),
              sd_purchases = sd(customer_purchases, na.rm=T),
              sum_est = sum(est)) %>%
    mutate(cv_purchases = round(sd_purchases / mean_purchases, 2))
```

dplyr's `mutate` function creates a new column, but can perform slower on large datasets and/or be more awkward to type than simply making a new column in the usual way with `$` and `=`. For example:

```
customer_survey = mutate(customer_survey, Location=paste(city, stabbr, sep=", "))
# vs
customer_survey$Location = paste(city, stabbr, sep=", ")
```

Still, it can be pretty convenient if you're stringing functions together, as above (so a `%>%` operation would simply be `mutate(Location=paste(city, stabbr, sep=", ")`, making it as easy to type as the normal way), or want to peek at the outcome before running it for results, as below.

## Peeking at the outcome

If you want to check on how one or more of the `dplyr` actions will perform in terms of output before updating or creating a dataframe, you can wrap it inside the head function:

```
head(select(customer_survey, contains("customer", ignore.case=TRUE)))
head(arrange(customer_survey, region, desc(customer_age), desc(customer_purchases)))
head(mutate(customer_survey, Location = paste(city, stabbr, sep=", "), 
  purchase_proportion=round(customer_purchases/length(customer_purchases),2)))
```

## Transforming a dataframe from wide to long and back again

A lot of analyses and plotting require that data be organized in long format, where the variable of interest is in one column and the id/categorical portions of the data are repeated as necessary in other columns. Then again, other tools need the data in wide format, so being able to move between the two formats is essential. The `reshape2` package provides just that ability.  

We'll use a different dataset to illustrate some of the possibilities `reshape2` offers—this shows the verbal and nonverbal IQ test scores for children before and after neurosurgery (and in which hemisphere, i.e. "Side" of the brain). Load the package and the data:

```
require(reshape2)
surgery_outcomes = read.table("surgery_outcomes.csv", header=T, sep=",")
head(surgery_outcomes, 4)

row  ID  Side   Phase  Verbal  Nonverbal
1    1   Right  Post   146     115
2    1   Right  Pre    129     103
3    2   Right  Post   126     115
4    2   Right  Pre    138     115
```

If each measurement needs its own row, you melt a dataframe from wide into long format:

```
surgery_outcomes_melted = melt(surgery_outcomes, id.vars=c(1:3),
    measure.vars=c(4:5), variable.name="Test_type", value.name="IQ_score")
head(surgery_outcomes_melted, 4)

row  ID  Side   Phase  Test_type  IQ_score
1    1   Right  Post   Verbal     146
2    1   Right  Pre    Verbal     129
3    2   Right  Post   Verbal     126
4    2   Right  Pre    Verbal     138
```

Any data in long form can be cast into a variety of wide forms, with a `d` or an `a` tacked on to the front depending on whether you need a dataframe (`d`) or vector, matrix, or array (`a`). To put this data back in the shape it started as:

```
surgery_outcomes_original = dcast(surgery_outcomes_melted, ID + Side + 
    Phase ~ Test_type)
head(surgery_outcomes_original, 4)

row  ID  Side   Phase  Verbal  Nonverbal
1    1   Right  Post   146     115
2    1   Right  Pre    129     103
3    2   Right  Post   126     115
4    2   Right  Pre    138     115
```

Being able to move back and forth between data shapes makes meeting the needs of the R packages you're working with considerably easier, especially if you're moving between some that work best with long form structures and others that work well with short.

The important part to remember in melting data is to ensure you've designated the id and measurement variables appropriately. While `melt` can operate with neither or only one of these designated, proper coding etiquette requires that you designate each specifically to avoid ambiguity.

### Crossing variables with ~ and +

There is tremendous variety in reshaping options using the formula structure in `dcast`. The tilde sign (`~`) separates the variables you want representing the rows (left of the ~) and those you want representing the columns (right of the ~). For example, we can cross phase and the test type in the columns:

```
head(dcast(surgery_outcomes_melted, ID ~ Test_type + Phase), 4)

     ID  Verbal_Post  Verbal_Pre  Nonverbal_Post  Nonverbal_Pre
1    1   146          129         115             103
2    2   126          138         115             115
3    3   110          100         104             100
4    4   104          NA          106             NA
```

We could also return the `Side` variable to the same shape:

```
head(dcast(surgery_outcomes_melted, ID + Side ~ Test_type + Phase), 4)

   ID  Side   Verbal_Post  Verbal_Pre  Nonverbal_Post  Nonverbal_Pre
1  1   Right  146          129         115             103
2  2   Right  126          138         115             115
3  3   Left   110          100         104             100
4  4   Right  104          NA          106             NA
```

And so on... the possibilities are as many as your variable set...

```
head(dcast(surgery_outcomes_melted, ID + Phase ~ Test_type), 4)

  ID Phase Verbal Nonverbal
1  1  Post    146       115
2  1   Pre    129       103
3  2  Post    126       115
4  2   Pre    138       115
```

### Summarizing while reshaping

While `dplyr` might provide you with all you need for summarizing data once your data structure is set, `reshape2` offers the option to summarize while reshaping. The summarization works based on which operation you choose as well as which side of the `~` you place the variables (as described above).  

```
dcast(surgery_outcomes_melted, Phase ~ Test_type, mean, na.rm=T)

  Phase   Verbal Nonverbal
1  Post 104.6667 102.26667
2   Pre 103.0909  98.90909
```

```
dcast(surgery_outcomes_melted, Side ~ Test_type + Phase, mean, na.rm=T)

   Side Verbal_Post Verbal_Pre Nonverbal_Post Nonverbal_Pre
1  Left    102.4444   97.71429       100.7778      97.42857
2 Right    108.0000  112.50000       104.5000     101.50000
```

```
dcast(surgery_outcomes_melted, Test_type + Side + Phase ~ ., sd, na.rm=T)

  Test_type  Side Phase        .
1    Verbal  Left  Post 14.78269
2    Verbal  Left   Pre 12.51285
3    Verbal Right  Post 23.80756
4    Verbal Right   Pre 24.93324
5 Nonverbal  Left  Post 16.09952
6 Nonverbal  Left   Pre 13.96253
7 Nonverbal Right  Post 11.46734
8 Nonverbal Right   Pre 10.47219
```
