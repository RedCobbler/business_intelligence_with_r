# Chapter 4: Know Thy Data

- Creating summary plots
- Missing data  
- Obtaining summary statistics
- Inference on summary statistics
- Plotting univariate distributions
- Plotting bivariate and comparative distributions

It's often tempting to jump into analysis as soon as possible, but it is a cardinal sin to not evaluate your data with the basics first. This chapter covers the evaluations that must be done before anything else.

In this chapter, we'll primarily use the same bike share data we downloaded from the UCI Machine Learning Repository in the Loading data recipe of Chapter 2. To make it easy, the code below downloads the particular file we'll use throughout much of this chapter and adjusts the column types and levels.

```
download.file("http://archive.ics.uci.edu/ml/machine-learning-databases/00275/Bike-Sharing-Dataset.zip", 
    "Bike-Sharing-Dataset.zip")
bike_share_daily = read.table(unz("Bike-Sharing-Dataset.zip", "day.csv"),
    sep=",", header=T,
    colClasses=c("character", "Date", "factor", "factor", "factor", "factor",
        "factor", "factor", "factor", "numeric", "numeric", "numeric", "numeric",
        "integer", "integer", "integer"))
levels(bike_share_daily$season) = c("Winter", "Spring", "Summer", "Fall")
levels(bike_share_daily$workingday) = c("No", "Yes")
levels(bike_share_daily$holiday) = c("No", "Yes")

levels(bike_share_daily$mnth) = c(month.abb)
levels(bike_share_daily$yr) = c(2011, 2012)
```

## Creating summary plots

If you were to poll data scientists on the "Three Most Important Steps in Any Analysis," ***Plot Thy Data*** should account for all three. Far too many business professionals rely on tables and simple summary statistics to make decisions, which are useful but can never provide the richness of information that graphs can. Further, decisions based on single statistic (usually the mean) are made much too often, usually with either good results reached by luck or accident, or—more often—you end up with (predictably) poor results. These days, I only rarely provide tables of summary stats to decision makers, often choosing instead to provide distributional or other EDA plots with corresponding ranges of variation (standard deviation, IQR, etc.) or uncertainty (confidence limits, credibility intervals, etc.) instead of “naked” single values like a mean or median.  

The recipes below are aimed at giving you a quick visual summary of the data, not to produce final plots—and so they're rough and use functions that can be difficult to modify. We'll see recipes for production plots later in this chapter and throughout the book, but we often just need to see the data before doing anything else.

The `ggpairs` function in the `GGally` package has a relatively slow but visually wonderful way to plot your dataset, so load that library before continuing.

```
require(GGally)
```

GGally is built over `ggplot2` (which we'll use often in this book), and its syntax works in more or less the same manner as `qplot`. You can create the summary plot of a subset of the bike share data for an at-a-glance overview:

```
ggpairs(data=bike_share_daily, columns=c(14:15, 10, 13, 3, 7), 
  title="Daily Bike Sharing Data", axisLabels="show", color="season")
```

![ggpairs](/images/Ch4_01.png)

It’s helpful to use a non-parametric correlation method if you don’t know a priori whether the quantitative variables are linearly related in their raw forms. Unfortunately, `ggpairs` doesn’t yet provide this functionality, so know that for continuous pairs of variables that don’t show a linear point cloud, the correlation coefficient in this graph will be *larger than it should be*.

`GGally` is still in active development, so currently it's pretty difficult to customize the `ggpairs` plot, and it may rarely be useful to anyone other than you. Even then, it's a great way to get a sense of the data at a glance. I use this function with almost any dataset I work with.

## Create histograms of all numeric variables in one plot

Sometimes you just want to see histograms, but coding and laying out a bunch of them in a grid is more work than it needs to be. The `psych` package's `multi.hist` function can do this in a single line. Note that if you specify variables that aren't numeric, it will throw an error.

```
require(psych)
multi.hist(bike_share_daily[,sapply(bike_share_daily, is.numeric)])
```

![multi.hist](/images/Ch4_02.png)

## A better "pairs" plot

The `psych` package also has a modification of the built-in `graphics` package's `pairs` function that's more useful out of the box. As mentioned above, until you have evidence that your paired variables are indeed linear in raw form, you should modify the default calculation option to `method="spearman"` for continuous data or `method="kendall"` for ordinal data or continuous data in which you have some known outliers:

```
pairs.panels(bike_share_daily[,sapply(bike_share_daily, is.numeric)], 
  ellipses=FALSE, pch=".", las=2, cex.axis=0.7, method="kendall")
```

![pairs.panels](/images/Ch4_03.png)

## Mosaic plots: "Scatterplots" for categorical data

Once you've loaded the `vcd` package, the `pairs` function can also create a matrix of mosaic plots, either by using the table function on a dataframe, or by referencing a table object.

```
require(vcd)
pairs(table(bike_share_daily[c(3, 9)])) # not shown
```

The dataset we've used so far in this recipe isn't a great one for mosaic plots since most of those variables are time-related, so a better view of the outcome of plotting categorical pairs can be seen with the built-in `Titanic` dataset (`?Titanic`), using the highlighting option to make the categories more clearly distinct at a glance:

```
pairs(Titanic, highlighting=2)
```

![mosaic pairs](/images/Ch4_04.png)


## Plotting univariate distributions

We saw the quick-and-dirty way to plot a bunch of distributions at once in an earlier recipe; this one focuses on a production-ready single-distribution plot using the `ggplot2` package.

This package can seem daunting at first; the power of its immense flexibility and beauty comes at the cost of a bewildering variety of options. But it's well worth the effort to learn, as you'll be able to customize virtually anything you decide to plot. The documentation is very thorough, but if you need a cheat-sheet to simply the possibilities down to the common needs and uses, Sharon Machlis has a *great* introduction on [Computer World](http://www.computerworld.com/article/2935394/business-intelligence/my-ggplot2-cheat-sheet-search-by-task.html) (see aside), and Zev Ross has a more thorough but also well done overview on his [blog](http://zevross.com/blog/2014/08/04/beautiful-plotting-in-r-a-ggplot2-cheatsheet-3/). 

Load the bike share data we've been using, as well as the packages we'll use in this recipe (if they're not already loaded).

```
require(ggplot2)
require(scales)
```

For quantitative data, business users are used to seeing histograms; quants prefer density plots. Here's how to do both, separately...  

```
ggplot(bike_share_daily, aes(casual)) +
    geom_density(col="blue", fill="blue", alpha=0.3) +
    xlab("Casual Use") +
    theme_bw()

ggplot(bike_share_daily, aes(casual)) +
    geom_histogram(col="blue", fill="blue", alpha=0.3) +
    xlab("Casual Use") +
    theme_bw()
```

![density and histogram](/images/0815OS_02_06a.png)

```
ggplot(bike_share_daily, aes(casual)) +
    ylab("density and count") +
    xlab("Casual Use") +
    geom_histogram(aes(y=..density..), col="blue", fill="blue", alpha=0.3) +
    geom_density(col="blue", fill="blue", alpha=0.2) +
    theme_bw() +
    theme(axis.ticks.y = element_blank(), axis.text.y = element_blank())
```

...as well as together:

![density and histogram](/images/0815OS_02_06b.png)

When asked, I’ve usually explained density plots as “more objectives histograms,” which, while not quite true, is accurate enough for business users. If you do need to explain things, see `?density` for more details on the methods and useful additional references; `?stat_density` provides an overview of calculation methods used in `ggplot2`, while `?geom_density` provides information on graphical options. 

That pattern carries over to many `ggplot2` options—find details of the method by replacing the `x` with the stat or geom in `?stat_x` and `?geom_x` for calculation and plotting information, respectively.

When you have categorical data, bar or dot plots serve the same purpose:  

```
ggplot(bike_share_daily, aes(weathersit)) +
    geom_bar(col="blue", fill="blue", alpha=0.3) +
    xlab("Weather Pattern") +
    scale_x_discrete(breaks=c(1, 2, 3), labels=c("Clear", "Cloudy/Rainy", 
    "Stormy")) +
    theme_bw()
ggplot(bike_share_daily, aes(x=weathersit, y=..count.. )) +
    geom_point(stat = "bin", size = 3, pch=15, col="blue") +
    xlab("Weather Pattern") +
    scale_x_discrete(breaks=c(1, 2, 3), labels=c("Clear", "Cloudy/Rainy", 
    "Stormy")) +
    theme_bw()
```

![bar and dot](/images/0815OS_02_07a.png)

You can also use `coord_flip()` to move from a normal bar chart to a horizontal one:
```
ggplot(bike_share_daily, aes(weathersit)) +
    geom_bar(col="blue", fill="blue", alpha=0.3) +
    xlab("") +
    scale_x_discrete(breaks=c(1, 2, 3), labels=c("Clear", "Cloudy/Rainy", 
    "Stormy")) +
    coord_flip() +
    theme_bw()
```

![horizontal bar](/images/0815OS_02_07b.png)

I> #### `ggplot` cheat sheet
I> You can scrape Sharon Machlis' wonderful `ggplot` cheat sheet into a local pipe-delimited text file, to format and display as works best for you.  
I> ~~~~~~~~  
I> require(XML)  
I> machlis_cw_ggplot_page = readLines("http://www.computerworld.com/article/  
I>    2935394/business-intelligence/my-ggplot2-cheat-sheet-search-by-task.html")   
I> gg_cheat_sheet = readHTMLTable(machlis_cw_ggplot_page, header=T, which=1,  
I>    stringsAsFactors=F)   
I> write.table(gg_cheat_sheet, "gg_cheat_sheet.txt", sep="|", row.names=F)   
I> ~~~~~~~~  


## Plotting multiple univariate distributions with faceting

We saw the quick-and-dirty way to plot multiple histograms with multi.hist a few recipes ago; here's how to use ggplot2 to create the same thing more elegantly:

```
ggplot(bike_share_daily, aes(casual, fill=season)) +
    geom_histogram(aes(y = ..density..), alpha=0.2, color="gray50") +
    geom_density(alpha=0.5, size=0.5) +
    facet_wrap(~season) +
    theme_light() +
    xlab("Daily Bike Use Count") +
    ylab("") +
    theme(legend.position="none") +
    theme(axis.ticks.y = element_blank(), axis.text.y = element_blank(),
          axis.title = element_text(size=9, face=2, color="gray30"),
          axis.title.x = element_text(vjust=-0.5))
```

![ggplot multi.hist](/images/0815OS_02_08.png)

The same can be done for categorical distributions, too, of course:

```
ggplot(bike_share_daily, aes(weathersit, fill=season)) +
    geom_bar(alpha=0.5) +
    xlab("") +
    ylab("Number of Days") +
    scale_x_discrete(breaks=c(1, 2, 3), labels=c("Clear", "Cloudy/Rainy", 
    "Stormy")) +
    coord_flip() +
    facet_wrap(~season, ncol=1) +
    theme_light()
```

![ggplot multi.bar](/images/0815OS_02_09.png)

A> #### What about pie charts?
A> 
A> With apologies to Hunter S. Thompson, if you use pie charts for anything outside of GraphJam, you deserve whatever happens to you.

## Plotting bivariate and comparative distributions

Most of the time, we want to understand how one thing compares to something else. There’s no better way to do this by plotting each group’s distribution against the other’s.

We continue using the bike share data, along with `ggplot2`. Make sure to load the `vcd` package as well:  

```
require(ggplot2)
require(ggExtra)
require(scales)
require(vcd)
```

### Double density plots

For quantitative data, plotting the two distributions over each other provides a clear, direct comparison:

```
ggplot(bike_share_daily, aes(casual, fill=workingday, color=workingday)) +
    geom_density(alpha=0.4) +
    theme_minimal() +
    xlab("Daily Casual Bike Use Count") +
    ylab("") +
    scale_fill_discrete(name="Work Day?") +
    scale_color_discrete(name="Work Day?") +
    theme(axis.ticks.y = element_blank(), axis.text.y = element_blank(), 
    legend.position="top")
```

![double density plot](/images/0815OS_02_10.png)


### Scatterplots and marginal distributions

The `ggExtra` package provides a simple way to plot scatterplots with marginal distributions with the `ggMarginal` function. It defaults to density curves, but the histogram option presents a clearer picture. First, build the scatterplot and assign it to an object.

```
bike_air_temp = ggplot(bike_share_daily, aes(x=atemp, y=casual)) +
    xlab("Daily Mean Normalized Air Temperature") +
    ylab("Number of Total Casual Bike Uses") +
    geom_point(col="gray50") +
    theme_bw()
```

Then, use `ggMarginal` to create the final product:

```
bike_air_temp_mh = ggMarginal(bike_air_temp, type="histogram")
```

![marginal histograms](/images/scatter_marginal_hist.png)

Sometimes density plots can be useful, or perhaps you want to keep it simple and show a boxplot. Just change the `type` option:

![marginal density](/images/scatter_marginal_density.png)  
![marginal boxplot](/images/scatter_marginal_boxplot.png)  


### Mosaic plots

The `vcd` package has a nice implementation of mosaic plots for categorical data called, as you might expect, `mosaic`:

```
mosaic(~ weathersit + season, data=bike_share_daily, shade=T, 
  legend=F, labeling_args = list(set_varnames = c(season = "Season", 
  weathersit = "Primary Weather Pattern"), set_labels = list(weathersit = 
  c("Clear", "Cloudy/Rainy",  "Stormy"))))
```

![mosaic](/images/0815OS_02_11.png)

The first plot is simply a plot of the distributions of each group, i.e., a set of overplotted density histograms.
If you have categorical data, you'll probably want to use mosaic plots to visualize differences between groups; if you're not already familiar with them, think of mosaics plots as contingency tables made with shapes—the larger the shape, the higher the cell count.

For comparison, here's the table that the mosaic plot represents:  

| Winter | Spring | Summer | Fall |
|:------:|:------:|:------:|:----:|
| 111 | 113 | 136 | 103 |
| 66 | 68 | 48 | 65 |
| 4 | 3 | 4 | 10 |


## Multiple bivariate comparisons with faceting

As with univariate plots, we can facet on another variable for comparison’s sake; for example, if we take the bike share data shown above and facet by season:

```
ggplot(bike_share_daily, aes(casual, fill=workingday, color=workingday)) +
    geom_density(alpha=0.4) +
    theme_minimal() +
    xlab("Daily Casual Bike Use Count") +
    ylab("") +
    scale_fill_discrete(name="Work Day?") +
    scale_color_discrete(name="Work Day?") +
    facet_wrap(~season, ncol=2) +
    theme(axis.ticks.y = element_blank(), axis.text.y = element_blank(), 
    legend.position="top")
```

***NEED PLOT***

## Plotting survey data

There are a variety of ways you can plot survey data, but the `likert` package makes things really simple. We can use the `mass` dataset (Math Anxiety Scale Survey) built into `likert` to demonstrate:

```
require(likert)
mathiness = likert(mass[2:15])
plot(mathiness)
```

![basic likert plot](/images/0815OS_02_13.png)

E> There's a known bug in `likert` where you have to load the `reshape` package manually (not reshape2) to make the `grouping` option work.

```
require(reshape)
gender_math = likert(items=mass[,c(4,6,15), drop=FALSE], grouping=mass$Gender)
plot(gender_math, include.histogram=TRUE)
```

![grouped likert plot](/images/0815OS_02_14.png)

Because the `reshape` package masks functions in `reshape2` and `dplyr`, you should detach `likert` and `reshape` when you're done plotting.

```
detach("package:likert", unload=TRUE)
detach("package:reshape", unload=TRUE)
```


## Obtaining summary statistics

Once you've got plots, it's good to have the summary statistics to go along with the variables you’ve plotted. Functions to calculate summary statistics are in several packages, but I’ve found that the `psych` package probably has the simplest way to obtain them, particularly for groupings or when you want to condition on a particular value or threshold.

It’s important to remember that each type of data can reasonably use only certain kinds of summary statistics. The following table should help when, for example, a decision maker asks you to calculate the mean of an ordinal variable (such as a Likert scale)—just show them this table so you don’t have to go on record as the one who said it was a stupid idea:

[BRING BACK MD TABLE]

\* Ratio variables have a "true zero" while interval variables do not (e.g., temperature in °C).
\** Discrete variables can have means/variances based on non-normal probability distributions (e.g., binomial for proportions, Poisson for counts or rates, etc.), but this should not be confused with an arithmetic mean or variance.

Load the `psych` package and the daily bike share data if they're not already loaded from the previous recipe.

```
require(psych)
bike_share_daily=read.table("bike_share_daily.csv", sep=",", header=T)
```

For numeric variables, the describe and describeBy functions do all the work for you, although you do need to specify which columns are quantitative—or which columns you want summary stats on—or it will return (non-sensically) all of them:

```
describe(bike_share_daily[10:16]) # not shown
describeBy(bike_share_daily[10:16], bike_share_daily$holiday)

group: No
           vars   n    mean      sd median trimmed     mad min  max range skew kurtosis    se
casual        1 710  841.77  680.53  711.5  739.69  584.14   2 3410  3408 1.27     1.34 25.54
registered    2 710 3685.33 1553.70 3691.0 3672.14 1695.35  20 6946  6926 0.04    -0.71 58.31
---------------------------------------------------------------------------------------------
group: Yes
           vars  n    mean      sd median trimmed     mad min  max range skew kurtosis     se
casual        1 21 1064.71  860.05    874  965.88  658.27 117 3065  2948 0.88    -0.41 187.68
registered    2 21 2670.29 1492.86   2549 2604.47 1599.73 573 5172  4599 0.27    -1.26 325.77
```

The `table` and `prop.table` functions from the `base` package provide the simplest summary stats for categorical and ordinal variables:

```
table(bike_share_daily$holiday)
No Yes
710  21

prop.table(table(bike_share_daily$holiday))

        No        Yes
0.97127223 0.02872777
```

There are a variety of approaches and options using these functions.

For example, you can subset the quantitative stats based on factors or conditions:

```
describeBy(bike_share_daily[14:16], bike_share_daily$season == "Winter") # not shown
describeBy(bike_share_daily[14:16], bike_share_daily$temp > 0.5) # not shown
describeBy(bike_share_daily$casual, bike_share_daily$windspeed > 
  mean(bike_share_daily$windspeed))

group: FALSE
  vars   n   mean     sd median trimmed    mad min  max range skew kurtosis    se
1    1 409 914.61 673.18    773  827.61 598.97   9 3410  3401 1.11     0.96 33.29
---------------------------------------------------------------------------------
group: TRUE
  vars   n  mean     sd median trimmed   mad min  max range skew kurtosis    se
1    1 322 763.8 695.26  613.5  642.12 579.7   2 3283  3281  1.5     1.94 38.75
```

You can create two- , three-, or *n*-way tables by adding variables to the table call, and use `addmargins` to get marginal sums. 

A three-way table:

```
table(bike_share_daily[c(3,6:7)]) 
, , weekday = 0

        holiday
season   No Yes
  Winter 27   0
  Spring 26   0
  Summer 26   0
  Fall   26   0

, , weekday = 1

        holiday
season   No Yes
  Winter 20   6
  Spring 24   3
  Summer 23   3
  Fall   23   3
…
```

A three-way table with marginals:

```
addmargins(table(bike_share_daily[c(3,6:7)]))
, , weekday = 0

        holiday
season    No Yes Sum
  Winter  27   0  27
  Spring  26   0  26
  Summer  26   0  26
  Fall    26   0  26
  Sum    105   0 105

, , weekday = 1

        holiday
season    No Yes Sum
  Winter  20   6  26
  Spring  24   3  27
  Summer  23   3  26
  Fall    23   3  26
  Sum     90  15 105
…
```

A table of proportions:

```
prop.table(table(bike_share_daily[c(3,9)]))

        weathersit
season             1           2           3
  Winter 0.151846785 0.090287278 0.005471956
  Spring 0.154582763 0.093023256 0.004103967
  Summer 0.186046512 0.065663475 0.005471956
  Fall   0.140902873 0.088919289 0.013679891
```

Sometimes you may want the counts to be in a data frame, either for use in plotting or because tables with more than 2 or 3 dimensions can be difficult to understand at a glance. You can quickly create any *n*-way table as a data frame using `dplyr`:

```
require(dplyr)
summarise(group_by(bike_share_daily, season, holiday, weekday), count=n())
Source: local data frame [37 x 4]
Groups: season, holiday

   season holiday weekday count
1  Winter      No       0    27
2  Winter      No       1    20
3  Winter      No       2    24
4  Winter      No       3    25
…
```


## Inference on summary statistics

### Confidence intervals

When we don’t have a full population to assess, we often need to determine how uncertain our estimates of the population parameters are—we need confidence intervals on those point estimates. There are `base` installation functions for most of the ones we typically need:

| Statistic | Data type | Distribution | Function |
| --------- | --------- | ------------ | -------- |
| Median | Any | None | `wilcox.test(x, conf.int=TRUE)$conf.int` |
| Mean| Continuous | Normal, t, “normal enough” | `t.test(x)$conf.int` |
| Proportion | Percentage | Binomial | `binom.test(x, n)$conf.int` |
| Count | Count | Poisson | `poisson.test(x)$conf.int` |
| Rate | Count/*n* | Poisson | `poisson.test(x, n)$conf.int` |

Sometimes there isn’t a `base` function for the confidence interval we need, or our data depart considerably from one of the typical distributions. For example, the confidence interval of a standard deviation is very sensitive to departures from normality, so R doesn’t have a built-in function for it. Non-parametric bootstrapping is the way to get these intervals; the `boot` package provides the most options but can be a pain to work with since its syntax is fairly different from most everything else in R. Still, it does what we need it to with a relative minimum of code for most cases. In this example we’ll use the `CO2` dataset that comes with R for simplicity’s sake:

```
require(boot)
sd_boot_function = function(x,i){sd(x[i])
sd_boot = boot(CO2$conc, sd_boot_function, R=10000)
sd(CO2$conc)
boot.ci(sd_boot, type="bca")$bca[4:5]

295.9241
258.8950 333.7344
```

The BCa method is generally the appropriate calculation unless you have strong reason to believe that another method is best in the particular context. For example, you might want to use `type="norm"`, i.e., use the normal approximation for very small samples that you expect are “normal enough,” realizing that due to sampling error you could be estimating intervals that are wildly incorrect.   

We can also bootstrap confidence intervals for any summary statistic. For example, to get the bootstrapped 75th percentile, run the following:

```
q75_function = function(x,i){quantile(x[i], probs=0.75)
q75_boot = boot(CO2$conc, q75_function, R=10000)
quantile(CO2$conc,0.75)
boot.ci(q75_boot, type="bca")$bca[4:5]

675
500 675
```

### Credible intervals

STUFF ON BCIs here

The single sample functions available to acquire simple CIs as of this writing include:

| Statistic | Data type | Distribution | Function |
| --------- | --------- | ------------ | -------- |
| Mean | Continuous | Normal, t, “normal enough” | `bayes.t.test(x)`|
| Proportion/Percent | Binomial | `bayes.binom.test(x, n)` |
| Count | Count | Poisson | `bayes.poisson.test(x)` |
| Rate | Count/Unit | Poisson | `bayes.poisson.test(x, T)` |




### Tolerance intervals

While confidence and credible limits tend to be more often used in data science, tolerance intervals are more useful in engineering or industrial contexts: there’s a profound conceptual and practical difference between, for example, a good estimate of average commuting time to work (using a 95% *confidence* interval) and how long you can expect 95% of those trips will take (using a 95% *tolerance* interval). While the former type of value tends to be used more by statisticians and data miners, engineers and industrial analysts—and sometimes business users—find the latter more useful. 

Essentially, a good way to remember the difference is that as the sample size approaches the population size, the confidence interval will approach zero, while a tolerance interval will approach the population percentiles.

The `tolerance` package contains functions for estimating tolerance intervals for distributions typically used in most engineering or industrial processes. As an example, assume you have data on commuting time for 40 randomly chosen workdays. The distribution is oddly-shaped, so we’ll use non-parametric tolerance intervals to estimate (at a 95% confidence level) the number of minutes that 75% of commutes will take:

```
require(tolerance)
commute_time = c(68, 42, 40, 69, 46, 37, 68, 68, 69, 38, 51, 36, 50, 37, 41, 
  68, 59, 65, 67, 42, 67, 62, 48, 52, 52, 44, 65, 65, 46, 67, 62, 66, 43, 58, 
  45, 65, 60, 55, 48, 46)
commute_time_npti = nptol.int(commute_time, alpha=0.05, P=0.75, side=2)
commute_time_npti

  alpha    P 2-sided.lower 2-sided.upper
1  0.05 0.75            37            68
```

You can also obtain a graphical summary with the plottout function, which shows a control chart on the left and a histogram on the right:

```
plottol(commute_time_npti, commute_time, side="two", plot.type="both")
```

![tolerance interval plot](/images/0815OS_02_05.png)


If you plotted both tolerance graphs (plot.type="both"), you'll need to return par to normal manually: 

```
par(mfrow=c(1,1))
```

The following table provides a short overview of the primary univariate tolerance interval functions; see `?tolerance` for more details. I recommend you set `alpha` (confidence level), `P` (the coverage proportion, i.e., tolerance interval), and `side` (whether to calculate upper, lower, or both bounds) manually for best results; if not chosen, `alpha` defaults to 0.05, `P` defaults to 0.99, and `side` defaults to 1 (you’ll probably want 2 in many cases). Other settings depend on the distribution used, and each distribution sometimes has several algorithms to choose from, so it’s worth exploring the documentation before diving in here.


| Data type | Distribution | Function |
| --------- | ------------ | -------- |
| Percent | Binomial | `bintol.int(x, n, m, …)` |
| Count or Rate | Poisson | `poistol.int(x, n, m, side, …)` |
| Nonparametric | None | `nptol.int(x, …)` |
| Continuous | Normal, t, “normal enough” | `normtol.int(x, side, …)`|
| Continuous | Uniform | `uniftol.int(x, …)` |
| Lifetime/survival | Exponential | `exptol.int(x, type.2, …)` |
| Score | Laplace | `laptol.int(x, …)` |
| Indicies | Gamma | `gamtol.int(x, …)` |
| Reliability, extreme values | Weibull, Gumbel | `extol.int(x, dist, …)` |


## Dealing with missing data

### Visualizing missing data

The `VIM` package has some great tools for visualizing missing data patterns. As the bike share data doesn't have any missing values, we'll use a dataset derived from the [Tropical Atmosphere Ocean](http://www.pmel.noaa.gov/tao/) project for this recipe.

```
require(VIM)
data(tao)
summary(tao)
```

![tao summary](/images/tao_summary_screenshot.png)

You can get an instant numeric and visual summary of missing values across the fields with the `aggr` function:

```
tao_aggr = aggr(tao)
tao_aggr

 Missings in variables:
         Variable Count
 Sea.Surface.Temp     3
         Air.Temp    81
         Humidity    93
```

![aggr image](/images/tao_aggr.png)

You can compare missingness across fields with histograms, such as the distribution of missing humidity values by corresponding air temperature:

```
histMiss(tao[5:6])
```

![histMiss image](/images/histmiss1.png)

You can flip the variables to see the opposite distribution:

```
histMiss(tao[c(6,5)])
```

![histMiss reversed image](/images/histmiss2.png)

You can show the same data in a scatterplot for more details:

```
marginplot(tao[5:6])
```

![marginplot image](/images/marginplot.png)

This can be expanded to pairwise visual comparisons with the `marginmatrix` function:

```
marginmatrix(tao[4:6])
```

![marginmatrix image](/images/marginmatrix.png)

I> `VIM` also has a separate GUI for interactive/on-the-fly exploration called `VIMGUI`. Even if you prefer coding, it contains a [vignette](http://cran.r-project.org/web/packages/VIMGUI/vignettes/VIM-Imputation.pdf) from which these examples are drawn, so it's worth installing to learn more about how it works and the many options you can control in the imputation process. 

### Imputation for missing values

`VIM` also performs imputation on missing values using a variety of methods. Generally, replacing values with a constant is a bad idea, as it will bias the distribution (see below for an example). Using a method that accounts for more variables or adds some noise or randomness is often a better approach. But like all things in stats, the best approach will depend on the entire analytics context, from the variables' data types to the business question itself. There is no best way, so proceed with caution whenever you need to impute data. 

The *k*-nearest neighbors approach can provide reasonable imputed values for many situations. Using the same data as above:

```
# Perform k Nearest Neighbors imputation
# Result is new dataframe with imputed values
tao_knn = kNN(tao)
```

This function appends a set of indicator variables to the data that show TRUE/FALSE for whether a particular observation was imputed. This becomes useful in the subsequent plotting of the results, e.g., for the `Air.Temp` and `Humidity` variables:

```
marginplot(tao_knn[c(5:6, 13:14)], delimiter="_imp")
```

![marginplot imputed](/images/marginplot_imputed.png)

Light and dark orange show results imputed for the variable on that axis; black points show observations for which both were imputed. Blue continues to show the distribution of the non-imputed values. 

Viewing a matrix works here as well:

```
marginmatrix(tao_knn[c(4:6, 12:14)], delimiter="_imp")
```

![marginmatrix imputed](/images/marginmatrix_imputed.png)

There are other methods in `VIM`, including a model-based approach:

```
# Perform standard Iterative Robust Model-based Imputation
tao_irmi = irmi(tao)

# Perform robust Iterative Robust Model-based Imputation
tao_irmi_robust = irmi(tao, robust=TRUE)
```

By subsetting the results of different methods, you can plot their densities to see how each approach might influence subsequent analysis. In addition to the original data and the three methods above, we'll add a mean imputation to show why a constant can be a bad choice for imputation.

```
# Create a mean-imputed air temp variable
tao$tao_airtemp_mean = ifelse(is.na(tao$Air.Temp), mean(tao$Air.Temp, 
    na.rm=TRUE), tao$Air.Temp)

# Make a data frame of each air temp result
tao_compare_airtemp = data.frame(tao=tao[,5], tao_knn=tao_knn[,5], 
    tao_irmi=tao_irmi[,5], tao_irmi_robust=tao_irmi_robust[,5], mean=tao[,9])

# Melt the various air temp results into a long data frame
require(reshape2)
tao_compare_melt = melt(tao_compare_airtemp, value.name="Air.Temp")

# Plot density histograms of each option and 
# add black dotted line to emphasize the original data
ggplot(tao_compare_melt, aes(Air.Temp, color=variable)) +
    geom_density(lwd=1.25) + 
    geom_density(data=subset(tao_compare_melt, variable=="tao"), 
        aes(Air.Temp), lty=3, lwd=1.5, color="black") +
    theme_minimal()
```

![imputed results](/images/imputed_results.png)

I> #### More imputation in R: other packages
I> 
I> Since imputation is a complex subject, there are of course a lot of different R packages to do it; explore the *Imputation* section of the CRAN Task View [Official Statistics & Survey Methodology](http://cran.r-project.org/web/views/OfficialStatistics.html) for a short overview of those packages and their methods.
