# Chapter 4: Know Thy Data

- Creating summary plots
- Obtaining summary statistics
- Plotting univariate distributions
- Plotting bivariate and comparative distributions
- Effect sizes: Measuring differences between groups
- Effect sizes: Measuring relationships between groups

It's often tempting to jump into analysis as soon as possible, but it is a cardinal sin to not evaluate your data with the basics first. This chapter covers the evaluations that must be done before anything else.

In this chapter, we'll primarily use the same bike share data we downloaded from the UCI Machine Learning Repository in the Loading data recipe of Chapter 2. To make it easy, the code below downloads the particular file we'll use throughout much of this chapter and writes the data to a csv file saved in your working directory. The first recipe then loads the csv, adjusts the column types appropriately, and runs through the essential first step of any analysis.

```
tf = tempfile()
download.file("http://archive.ics.uci.edu/ml/machine-learning-databases/00275
  /Bike-Sharing-Dataset.zip", tf)
bike_share_daily_raw = read.table(unz(tf, "day.csv"), header=TRUE, sep=",")
file.remove(tf)
write.table(bike_share_daily_raw, “bike_share_daily.csv”, row.names=FALSE)
rm(bike_share_daily_raw)
```

## Creating summary plots

If you were to poll data scientists on the "Three Most Important Steps in Any Analysis," Plot Thy Data should account for all three. Far too many business professionals rely on tables and simple summary statistics to make decisions, which are useful but can never provide the richness of information that graphs can. Further, decisions based on single statistic (usually the mean) are made much too often, usually with either good results reached by luck or accident, or—more often—you end up with (predictably) poor results. These days, I only rarely provide tables of summary stats to decision makers, often choosing instead to provide distributional or other EDA plots with corresponding ranges of variation (standard deviation, IQR, etc.) or uncertainty (confidence limits, credibility intervals, etc.) instead of “naked” single values like a mean or median.  

The recipes below are aimed at giving you a quick visual summary of the data, not to produce final plots—and so they're rough and use functions that can be difficult to modify. We'll see recipes for production plots later in this chapter and throughout the book, but we often just need to see the data before doing anything else.

The `ggpairs` function in the `GGally` package has a relatively slow but visually wonderful way to plot your dataset, so load that library before continuing.

```
require(GGally)
```

GGally is built over `ggplot2` (which we'll use often in this book), and its syntax works in more or less the same manner as `qplot`. You can create the summary plot of a subset of the bike share data for an at-a-glance overview:

```
ggpairs(data=bike_share_daily, columns=c(14:15, 10, 13, 3, 7), 
  title="Daily Bike Sharing Data", axisLabels="show", color="season")
```
![ggpairs](/images/Ch4_01.png)

It’s helpful to use a non-parametric correlation method if you don’t know a priori whether the quantitative variables are linearly related in their raw forms. Unfortunately, `ggpairs` doesn’t yet provide this functionality, so know that for continuous pairs of variables that don’t show a linear point cloud, the correlation coefficient in this graph will be *larger than it should be*.

`GGally` is still in active development, so currently it's pretty difficult to customize the `ggpairs` plot, and it may rarely be useful to anyone other than you. Even then, it's a great way to get a sense of the data at a glance. I use this function with almost any dataset I work with.

## Create histograms of all numeric variables in one plot

Sometimes you just want to see histograms, but coding and laying out a bunch of them in a grid is more work than it needs to be. The `psych` package's `multi.hist` function can do this in a single line. Note that if you specify variables that aren't numeric, it will throw an error.

```
require(psych)
multi.hist(bike_share_daily[,sapply(bike_share_daily, is.numeric)])
```
![multi.hist](/images/Ch4_02.png)

## A better "pairs" plot

The `psych` package also has a modification of the built-in `graphics` package's `pairs` function that's more useful out of the box. As mentioned above, until you have evidence that your paired variables are indeed linear in raw form, you should modify the default calculation option to `method="spearman"` for continuous data or `method="kendall"` for ordinal data or continuous data in which you have some known outliers:

```
pairs.panels(bike_share_daily[,sapply(bike_share_daily, is.numeric)], 
  ellipses=FALSE, pch=".", las=2, cex.axis=0.7, method="kendall")
```
![pairs.panels](/images/Ch4_03.png)

## Mosaic plots—"Scatterplots" for categorical data

Once you've loaded the `vcd` package, the `pairs` function can also create a matrix of mosaic plots, either by using the table function on a dataframe, or by referencing a table object.

```
require(vcd)
pairs(table(bike_share_daily[c(3, 9)])) # not shown
```

The dataset we've used so far in this recipe isn't a great one for mosaic plots since most of those variables are time-related, so a better view of the outcome of plotting categorical pairs can be seen with the built-in `Titanic` dataset (`?Titanic`), using the highlighting option to make the categories more clearly distinct at a glance:

```
pairs(Titanic, highlighting=2)
```
![mosaic pairs](/images/Ch4_04.png)

## Obtaining summary statistics

Once you've got plots, it's good to have the summary statistics to go along with the variables you’ve plotted. Functions to calculate summary statistics are in several packages, but I’ve found that the `psych` package probably has the simplest way to obtain them, particularly for groupings or when you want to condition on a particular value or threshold.

It’s important to remember that each type of data can reasonably use only certain kinds of summary statistics. The following table should help when, for example, a decision maker asks you to calculate the mean of an ordinal variable (such as a Likert scale)—just show them this table so you don’t have to go on record as the one who said it was a stupid idea:

[BRING BACK MD TABLE]

Type of Data

Categorical
Ranked
Discrete
Continuous
Statistic / Parameter
Nominal
Ordinal
Interval/Ratio\text{*
Interval/Ratio\text{*
data set size (n)
Y
Y
Y
Y
percent
Y
Y
Y
Y
count/rate
Y
Y
Y
Y
category (levels)
Y
Y
Y
Y
mode
Y
Y
Y
Y
median
N
Y
Y
Y
interquartile range
N
Y
Y
Y
median absolute deviation
N
Y
Y
Y
range
N
Y
Y
Y
minimum/maximum
N
Y
Y
Y
quantiles
N
Y
Y
Y
mean
N
N
Y\text{**
Y
variance
N
N
Y\text{**
Y
standard deviation
N
N
Y\text{**
Y
coefficient of variation
N
N
Y\text{**
Y

\* Ratio variables have a "true zero" while interval variables do not (e.g., temperature in °C).
\** Discrete variables can have means/variances based on non-normal probability distributions (e.g., binomial for proportions, Poisson for counts or rates, etc.), but this should not be confused with an arithmetic mean or variance.

Load the `psych` package and the daily bike share data if they're not already loaded from the previous recipe.

```
require(psych)
bike_share_daily=read.table("bike_share_daily.csv", sep=",", header=T)
```

For numeric variables, the describe and describeBy functions do all the work for you, although you do need to specify which columns are quantitative—or which columns you want summary stats on—or it will return (non-sensically) all of them:

```
describe(bike_share_daily[10:16]) # not shown
describeBy(bike_share_daily[10:16], bike_share_daily$holiday)
```
```
group: No
           vars   n    mean      sd median trimmed     mad min  max range skew kurtosis    se
casual        1 710  841.77  680.53  711.5  739.69  584.14   2 3410  3408 1.27     1.34 25.54
registered    2 710 3685.33 1553.70 3691.0 3672.14 1695.35  20 6946  6926 0.04    -0.71 58.31
---------------------------------------------------------------------------------------------
group: Yes
           vars  n    mean      sd median trimmed     mad min  max range skew kurtosis     se
casual        1 21 1064.71  860.05    874  965.88  658.27 117 3065  2948 0.88    -0.41 187.68
registered    2 21 2670.29 1492.86   2549 2604.47 1599.73 573 5172  4599 0.27    -1.26 325.77
```

The `table` and `prop.table` functions from the `base` package provide the simplest summary stats for categorical and ordinal variables:

```
table(bike_share_daily$holiday)
No Yes
710  21

prop.table(table(bike_share_daily$holiday))

        No        Yes
0.97127223 0.02872777
```

There are a variety of approaches and options using these functions.

For example, you can subset the quantitative stats based on factors or conditions:

```
describeBy(bike_share_daily[14:16], bike_share_daily$season == "Winter") # not shown
describeBy(bike_share_daily[14:16], bike_share_daily$temp > 0.5) # not shown
describeBy(bike_share_daily$casual, bike_share_daily$windspeed > 
  mean(bike_share_daily$windspeed))

group: FALSE
  vars   n   mean     sd median trimmed    mad min  max range skew kurtosis    se
1    1 409 914.61 673.18    773  827.61 598.97   9 3410  3401 1.11     0.96 33.29
---------------------------------------------------------------------------------------------
group: TRUE
  vars   n  mean     sd median trimmed   mad min  max range skew kurtosis    se
1    1 322 763.8 695.26  613.5  642.12 579.7   2 3283  3281  1.5     1.94 38.75
```

You can create two- , three-, or *n*-way tables by adding variables to the table call, and use `addmargins` to get marginal sums:

```
table(bike_share_daily[c(3,6:7)]) # not shown
addmargins(table(bike_share_daily[c(3,6:7)])) # not shown
prop.table(table(bike_share_daily[c(3,9)]))

        weathersit
season             1           2           3
  Winter 0.151846785 0.090287278 0.005471956
  Spring 0.154582763 0.093023256 0.004103967
  Summer 0.186046512 0.065663475 0.005471956
  Fall   0.140902873 0.088919289 0.013679891
```

Sometimes you may want the counts to be in a data frame, either for use in plotting or because tables with more than 2 or 3 dimensions can be difficult to understand at a glance. You can create any *n*-way table as a data frame using the `plyr` package's `count` function:

```
require(plyr)
count(bike_share_daily[c(3,6:7)])

    season holiday weekday freq
1  Winter      No       0   27
2  Winter      No       1   20
3  Winter      No       2   24
4  Winter      No       3   25
…
```

## Inference on summary statistics: confidence intervals

When we don’t have a full population to assess, we often need to determine how uncertain our estimates of the population parameters are—we need confidence intervals on those point estimates. There are `base` installation functions for most of the ones we typically need:

| Statistic | Data type | Distribution | Function |
| --------- | --------- | ------------ | -------- |
| Median | Any | None | `wilcox.test(x, conf.int=TRUE)$\textdollar$conf.int` |
| Mean| Continuous | Normal, t, “normal enough” | `t.test(x)$\textdollar$conf.int` |
| Proportion | Percentage | Binomial | `binom.test(x, n)$\textdollar$conf.int` |
| Count | Count | Poisson | `poisson.test(x)$\textdollar$conf.int` |
| Rate | Count/*n* | Poisson | `poisson.test(x, n)$\textdollar$conf.int` |

Sometimes there isn’t a `base` function for the confidence interval we need, or our data depart considerably from one of the typical distributions. For example, the confidence interval of a standard deviation is very sensitive to departures from normality, so R doesn’t have a built-in function for it. Non-parametric bootstrapping is the way to get these intervals; the `boot` package provides the most options but can be a pain to work with since its syntax is fairly different from most everything else in R. Still, it does what we need it to with a relative minimum of code for most cases. In this example we’ll use the CO2 dataset that comes with R for simplicity’s sake:

```
require(boot)
sd_boot_function = function(x,i){sd(x[i])
sd_boot = boot(CO2$conc, sd_boot_function, R=10000)
sd(CO2$conc)
boot.ci(sd_boot, type="bca")$bca[4:5]

295.9241
258.8950 333.7344
```

The BCa method is generally the appropriate calculation unless you have strong reason to believe that another method is best in the particular context. For example, you might want to use `type="norm"`, i.e., use the normal approximation for very small samples that you expect are “normal enough,” realizing that due to sampling error you could be estimating intervals that are wildly incorrect.   

We can also bootstrap confidence intervals for any summary statistic. For example, to get the bootstrapped 75th percentile, run the following:

```
q75_function = function(x,i){quantile(x[i], probs=0.75)
q75_boot = boot(CO2$conc, q75_function, R=10000)
quantile(CO2$conc,0.75)
boot.ci(q75_boot, type="bca")$bca[4:5]

675
500 675
```

## Inference on summary statistics: tolerance intervals

While confidence limits tend to be more often used in data science, tolerance intervals are more useful in engineering or industrial contexts: there’s a profound conceptual and practical difference between, for example, a good estimate of average commuting time to work (using a 95% *confidence* interval) and how long you can expect 95% of those trips will take (using a 95% *tolerance* interval). While the former type of value tends to be used more by statisticians and data miners, engineers and industrial analysts—and sometimes business users—find the latter more useful. 

Essentially, a good way to remember the difference is that as the sample size approaches the population size, the confidence interval will approach zero, while a tolerance interval will approach the population percentiles.

The `tolerance` package contains functions for estimating tolerance intervals for distributions typically used in most engineering or industrial processes. As an example, assume you have data on commuting time for 40 randomly chosen workdays. The distribution is oddly-shaped, so we’ll use non-parametric tolerance intervals to estimate (at a 95% confidence level) the number of minutes that 75% of commutes will take:

```
require(tolerance)
commute_time = c(68, 42, 40, 69, 46, 37, 68, 68, 69, 38, 51, 36, 50, 37, 41, 
  68, 59, 65, 67, 42, 67, 62, 48, 52, 52, 44, 65, 65, 46, 67, 62, 66, 43, 58, 
  45, 65, 60, 55, 48, 46)
commute_time_npti = nptol.int(commute_time, alpha=0.05, P=0.75, side=2)
commute_time_npti

  alpha    P 2-sided.lower 2-sided.upper
1  0.05 0.75            37            68
```

You can also obtain a graphical summary with the plottout function, which shows a control chart on the left and a histogram on the right:

```
plottol(commute_time_npti, commute_time, side="two", plot.type="both")
```



If you plotted both tolerance graphs (plot.type="both"), you'll need to return par to normal manually: 

```
par(mfrow=c(1,1))
```

The following table provides a short overview of the primary univariate tolerance interval functions; see ?tolerance for more details. I recommend you set alpha (confidence level), P (the coverage proportion, i.e., tolerance interval), and side (whether to calculate upper, lower, or both bounds) manually for best results; if not chosen, alpha defaults to 0.05, P defaults to 0.99, and side defaults to 1 (you’ll probably want 2 in many cases). Other settings depend on the distribution used, and each distribution sometimes has several algorithms to choose from, so it’s worth exploring the documentation before diving in here.



Data type
Distribution
Function
Percent
Binomial
bintol.int(x, n, m, …)
Count
Poisson
poistol.int(x, n, m, side, …)
Continuous
None
nptol.int(x, …)
Continuous
Normal, t, “normal enough”
normtol.int(x, side, …)
Continuous
Uniform
uniftol.int(x, …)
Lifetime/survival
Exponential
exptol.int(x, type.2, …)
Score
Laplace
laptol.int(x, …)
Indicies
Gamma
gamtol.int(x, …)
Reliability, extreme values
Weibull, Gumbel
extol.int(x, dist, …)

## Plotting univariate distributions

We saw the quick-and-dirty way to plot a bunch of distributions at once in an earlier recipe; this one focuses on a production-ready single-distribution plot using the ggplot2 package.

This package can seem daunting at first; the power of its immense flexibility and beauty comes at the cost of a bewildering variety of options. But it's well worth the effort to learn, as you'll be able to customize virtually anything you decide to plot. The documentation is very thorough, but if you need a cheat-sheet to simply the possibilities down to the common needs and uses, Zev Ross has a great one on his blog at http://zevross.com/blog/2014/08/04/beautiful-plotting-in-r-a-ggplot2-cheatsheet-3/

Load the bike share data we've been using, as well as the packages we'll use in this recipe (if they're not already loaded).

```
require(ggplot2)
require(scales)
```

For quantitative data, business users are used to seeing histograms; quants prefer density plots. Here's how to do both, separately as well as together.

```
ggplot(bike_share_daily, aes(casual)) +
    geom_density(col="blue", fill="blue", alpha=0.3) +
    xlab("Casual Use") +
    theme_bw()

ggplot(bike_share_daily, aes(casual)) +
    geom_histogram(col="blue", fill="blue", alpha=0.3) +
    xlab("Casual Use") +
    theme_bw()

ggplot(bike_share_daily, aes(casual)) +
    ylab("density and count") +
    xlab("Casual Use") +
    geom_histogram(aes(y=..density..), col="blue", fill="blue", alpha=0.3) +
    geom_density(col="blue", fill="blue", alpha=0.2) +
    theme_bw() +
    theme(axis.ticks.y = element_blank(), axis.text.y = element_blank())
```



When you have categorical data, bar or dot plots serve the same purpose:

```
ggplot(bike_share_daily, aes(weathersit)) +
    geom_bar(col="blue", fill="blue", alpha=0.3) +
    xlab("Weather Pattern") +
    scale_x_discrete(breaks=c(1, 2, 3), labels=c("Clear", "Cloudy/Rainy", 
    "Stormy")) +
    theme_bw()
ggplot(bike_share_daily, aes(x=weathersit, y=..count.. )) +
    geom_point(stat = "bin", size = 3, pch=15, col="blue") +
    xlab("Weather Pattern") +
    scale_x_discrete(breaks=c(1, 2, 3), labels=c("Clear", "Cloudy/Rainy", 
    "Stormy")) +
    theme_bw()
ggplot(bike_share_daily, aes(weathersit)) +
    geom_bar(col="blue", fill="blue", alpha=0.3) +
    xlab("") +
    scale_x_discrete(breaks=c(1, 2, 3), labels=c("Clear", "Cloudy/Rainy", 
    "Stormy")) +
    coord_flip() +
    theme_bw()
```

When asked, I’ve usually explained density plots as “more objectives histograms,” which, while not quite true, is accurate enough for business users. If you do need to explain things, see ?density for more details on the methods and useful additional references; ?stat\_density provides an overview of calculation methods used in ggplot2, while ?geom_density provides information on graphical options. That pattern carries over to many ggplot2 options—stat_ and geom_ for calculation and plotting information, respectively.

## Plotting multiple univariate distributions with faceting

We saw the quick-and-dirty way to plot multiple histograms with multi.hist a few recipes ago; here's how to use ggplot2 to create the same thing more elegantly:

```
ggplot(bike_share_daily, aes(casual, fill=season)) +
    geom_histogram(aes(y = ..density..), alpha=0.2, color="gray50") +
    geom_density(alpha=0.5, size=0.5) +
    facet_wrap(~season) +
    theme_light() +
    xlab("Daily Bike Use Count") +
    ylab("") +
    theme(legend.position="none") +
    theme(axis.ticks.y = element_blank(), axis.text.y = element_blank(),
          axis.title = element_text(size=9, face=2, color="gray30"),
          axis.title.x = element_text(vjust=-0.5))
```



The same can be done for categorical distributions, too, of course:

```
ggplot(bike_share_daily, aes(weathersit, fill=season)) +
    geom_bar(alpha=0.5) +
    xlab("") +
    ylab("Number of Days") +
    scale_x_discrete(breaks=c(1, 2, 3), labels=c("Clear", "Cloudy/Rainy", 
    "Stormy")) +
    coord_flip() +
    facet_wrap(~season, ncol=1) +
    theme_light()
```

## What about pie charts?

With apologies to Hunter S. Thompson, if you use pie charts for anything outside of GraphJam, you deserve whatever happens to you.

## Plotting bivariate and comparative distributions

Most of the time, we want to understand how one thing compares to something else. There’s no better way to do this by plotting each group’s distribution against the other’s.

We continue using the bike share data, along with ggplot2. Make sure to load the vcd package as well:  

```
require(ggplot2)
require(scales)
require(vcd)
```

For quantitative data, plotting the two distributions over each other provides a clear, direct comparison:

```
ggplot(bike_share_daily, aes(casual, fill=workingday, color=workingday)) +
    geom_density(alpha=0.4) +
    theme_minimal() +
    xlab("Daily Casual Bike Use Count") +
    ylab("") +
    scale_fill_discrete(name="Work Day?") +
    scale_color_discrete(name="Work Day?") +
    theme(axis.ticks.y = element_blank(), axis.text.y = element_blank(), 
    legend.position="top")
```

The vcd package has a nice implementation of mosaic plots for categorical data called, as you might expect, mosaic:

```
mosaic(~ weathersit + season, data=bike_share_daily, shade=T, 
  legend=F, labeling_args = list(set_varnames = c(season = "Season", 
  weathersit = "Primary Weather Pattern"), set_labels = list(weathersit = 
  c("Clear", "Cloudy/Rainy",  "Stormy"))))
  ```

The first plot is simply a plot of the distributions of each group, i.e., a set of overplotted density histograms.
If you have categorical data, you'll probably want to use mosaic plots to visualize differences between groups; if you're not already familiar with them, think of mosaics plots as contingency tables made with shapes—the larger the shape, the higher the cell count.

For comparison, here's the table that the mosaic plot represents:

```
table(bike_share_daily$weathersit, bike_share_daily$season)
```

Weather Code
Winter
Spring
Summer
Fall
1 (Clear)
111
113
136
103
2 (Cloud/Rainy)
66
68
48
65
3 (Stormy)
4
3
4
10

## Multivariate bivariate comparisons with faceting

As with univariate plots, we can facet on another variable for comparison’s sake; for example, if we take the bike share data shown above and facet by season:

```
ggplot(bike_share_daily, aes(casual, fill=workingday, color=workingday)) +
    geom_density(alpha=0.4) +
    theme_minimal() +
    xlab("Daily Casual Bike Use Count") +
    ylab("") +
    scale_fill_discrete(name="Work Day?") +
    scale_color_discrete(name="Work Day?") +
    facet_wrap(~season, ncol=2) +
    theme(axis.ticks.y = element_blank(), axis.text.y = element_blank(), 
    legend.position="top")
```

## Plotting survey data

There are a variety of ways you can plot survey data, but the likert package makes things really simple. We can use the mass dataset (Math Anxiety Scale Survey) built into likert to demonstrate:

```
require(likert)
mathiness = likert(mass[2:15])
plot(mathiness)
```

There's a known bug in likert where you have to load the reshape package manually (not reshape2) to make the grouping option work.

```
require(reshape)
gender_math = likert(items=mass[,c(4,6,15), drop=FALSE], grouping=mass$Gender)
plot(gender_math, include.histogram=TRUE)
```

Because the reshape package masks functions in reshape2 and dplyr, you should detach likert and reshape when you're done plotting.

```
detach("package:likert", unload=TRUE)
detach("package:reshape", unload=TRUE)
```

## Effect sizes: Measuring differences between groups

Perhaps the most common question an analyst will get from a business decision maker is whether or not there is a difference between groupings. There are a lot of ways to answer that, of course, but the bread-and-butter answer should always be based on effect sizes.

In this recipe, we'll go through all the major effect size metrics for differences; the following recipe will explore effect sizes for comparisons between groups.

Effect size measures that estimate a population value (such as a difference in means or a variance ratio) should be accompanied by an appropriate confidence interval whenever possible. This is usually best done by bootstrap unless you can meet distributional assumptions, although at larger sample sizes the differences will be trivial. We'll look at several ways to obtains CIs in this recipe.

We'll continue to use the bike share data, but we'll cast and filter some of it for the data for this recipe. We'll also use bootES, orddom, and asympTest to acquire confidence intervals for some of the contrasts we'll consider.

```
require(bootES)
require(orddom)
require(asympTest)
require(reshape2)
casual_workingday_use = dcast(bike_share_daily, yr~workingday, 
  value.var="casual", sum)
casual_workingday_use$sum = casual_workingday_use$Yes + casual_workingday_use$No
require(dplyr)
casual_notworkingday = filter(bike_share_daily, workingday == "No" & 
  season == "Spring" | workingday == "No" & season == "Fall")
casual_notworking_Spring =  filter(notworkingday, season == "Spring")
casual_notworking_Fall =  filter(notworkingday, season == "Fall")
```

For a difference between two proportions, we'll look at the effect of it being a working day on casual bike use in 2011 versus 2012:

```
workday_diff = prop.test(casual_workingday_use$Yes, casual_workingday_use$sum)
round(workday_diff$estimate[1] - workday_diff$estimate[2], 2)


-0.02

round(workday_diff$conf.int, 2)

-0.02 -0.01
```

When we wish to describe differences in central tendency, we'll want to know whether two means or medians are different from each other. For example, to see if there's a difference in average (or median) casual bike use between Spring and Fall non-working days, we can use t.test (or wilcox.test):

```
casual_notworkingday_mean = t.test(casual~season, data=casual_notworkingday)
abs(casual_notworkingday_mean$estimate[1] - 
  casual_notworkingday_mean$estimate[2]); casual_notworkingday_mean$conf.int

636.6031
350.6546 922.5516
```

Because the distribution isn't really normal, getting bootstrapped CIs on the difference of means is probably a better option in this case. Using the bootES package:

```
bootES(casual_notworkingday, data.col="casual", group.col="season", 
  contrast=c("Fall", "Spring"), effect.type="unstandardized")


User-specified lambdas: (Fall, Spring)
Scaled lambdas: (-1, 1)
95.00% bca Confidence Interval, 2000 replicates
Stat         CI (Low)     CI (High)    bias        SE           
636.603      332.203      908.222      3.116       149.380     
```


To look at a difference of medians (with a CI), use wilcox.test:

```
casual_notworkingday_median = wilcox.test(casual~season, 
  data=casual_notworkingday, conf.int=TRUE)
casual_notworkingday_median$estimate; casual_notworkingday_median$conf.int


684.0001
399 997
```

While not always easily explained to business users, standardized effect sizes are often very useful for analysts. When comparing means or medians, Hedge's g can be used for most cases (as it is an improvement on Cohen's d, its more famous predecessor), while a robust version of Cohen's d can be acquired by setting the effect.type option to "akp.robust.d".

```bootES(casual_notworkingday, data.col="casual", group.col="season", 
  contrast=c("Fall", "Spring"), effect.type="hedges.g")

User-specified lambdas: (Fall, Spring)
Scaled lambdas: (-1, 1)
95.00% bca Confidence Interval, 2000 replicates
Stat        CI (Low)    CI (High)   bias        SE          
0.825       0.374       1.252       0.004       0.219      


bootES(casual_notworkingday, data.col="casual", group.col="season", 
  contrast=c("Fall", "Spring"), effect.type="akp.robust.d")

User-specified lambdas: (Fall, Spring)
Scaled lambdas: (-1, 1)
95.00% bca Confidence Interval, 2000 replicates
Stat        CI (Low)    CI (High)   bias        SE          
0.866       0.433       1.406       0.039       0.250      
```


When you don't want to make any distributional assumptions, Cliff's delta and Vargha-Delaney's A (which is the same as the AUC statistic applied to a two-group contrast) are the best options. Both can be obtained from the orddom package, with CIs obtained via BCa by default:

```dmes.boot(casual_notworking_Fall$casual, casual_notworking_Spring$casual,
  theta.es="dc")$theta

       dc
0.4555138

dmes.boot(casual_notworking_Fall$casual, casual_notworking_Spring$casual,
  theta.es="dc")$theta.bci.lo; dmes.boot(casual_notworking_Fall$casual, 
  casual_notworking_Spring$casual,theta.es="dc")$theta.bci.up

0.2493734
0.6265664


dmes.boot(casual_notworking_Fall$casual, casual_notworking_Spring$casual,
  theta.es="Ac")$theta
  

       Ac
0.7277569

dmes.boot(casual_notworking_Fall$casual, casual_notworking_Spring$casual,
  theta.es="Ac")$theta.bci.lo; dmes.boot(casual_notworking_Fall$casual, 
  casual_notworking_Spring$casual,theta.es="Ac")$theta.bci.up

0.6246867
0.8132832
```

This package also has a neat feature in that it will plot the results and summary of a Cliff's delta assessment with the delta_gr function:

```
delta_gr(casual_notworking_Fall$casual, casual_notworking_Spring$casual, 
  x.name="Fall", y.name="Spring")
```



We also often want to know whether two groups differ in terms of their variances, i.e., is one group more variable than another? If both groups are normally distributed, var.test is appropriate to obtain the variance ratio:

```
var.test(casual_notworkingday$casual ~ casual_notworkingday$season)$estimate

ratio of variances
          1.105087

var.test(casual_notworkingday$casual ~ casual_notworkingday$season)$conf.int

0.6498105 1.8817627
```

More typically, our groups are not normally distributed, and when that's the case, the var.test will give misleading results because it is very sensitive to departures from normality. The asymp.test function in the asympTest package provides a suitable alternative:

```
asymp.test(casual_notworkingday$casual ~ casual_notworkingday$season,
  parameter = "dVar")$estimate

difference of variances
               58696.86

asymp.test(casual_notworkingday$casual ~ casual_notworkingday$season,
  parameter = "dVar")$conf.int

-197619.6  315013.4
```

Effect sizes provide the results that business users really want to know—not just whether two groups are different, but how different are they? These should become standard results alongside summary plots and statistics whenever contrasts are being made.

## What are the chances? Determining the probability of a difference

The rapid increase in computing power in recent years has put previously-intractable analytics tools at anyone's fingertips. Markov-chain Monte Carlo (MCMC) is one of the breakout stars of this revolution, which provides the mathematical basis for the resurgence of interest in Bayesian methods.

Bayesian methods provide the answers to the questions we actually have, as opposed the questions classical statistics actually answer (see the Preface for more thoughts on this issue YES OR NO?). This book probably would have been written primarily from that background had the R implementations of MCMC been more developed—but since they aren't, at this time we'll just explore some basic Bayesian approaches to effect size determination.

You'll need to install JAGS to your system before starting (http://mcmc-jags.sourceforge.net/), and also install the R package rjags to allow R to converse with JAGS.

BayesianFirstAid provides one-line Bayesian alternatives to common R functions for CIs ("credible intervals") and basic effect sizes. It's in development at the time of this writing and is best installed from Github:

```
require(devtools)
devtools::install_github("rasmusab/bayesian_first_aid")
```

Using the earlier example above for the differences in proportions, we get the same sort of results (i.e., the effect size and Cis), as well as a plot of the posterior distributions:

```
workday_diff_bayes = bayes.prop.test(casual_workingday_use$Yes, 
  casual_workingday_use$sum)
workday_diff_bayes


    Bayesian First Aid propotion test

data: casual_workingday_use$Yes out of casual_workingday_use$sum
number of successes:  118354, 184931
number of trials:     247252, 372765
Estimated relative frequency of success [95% credible interval]:
  Group 1: 0.48 [0.48, 0.48]
  Group 2: 0.50 [0.49, 0.50]
Estimated group difference (Group 1 - Group 2):
  -0.02 [-0.02, -0.015]
The relative frequency of success is larger for Group 1 by a probability
of <0.001 and larger for Group 2 by a probability of >0.999 .



plot(workday_diff_bayes)
```


To look at the difference between means or medians, we can use the bayes.t.test function:

```
casual_notworkingday_mean_bayes = bayes.t.test(casual~season, 
  data=casual_notworkingday)
casual_notworkingday_mean_bayes


    Bayesian estimation supersedes the t test (BEST) - two sample

data: group Spring (n = 56) and group Fall (n = 57)

  Estimates [95% credible interval]
mean of group Spring: 1892 [1684, 2107]
mean of group Fall: 1235 [1039, 1439]
difference of the means: 656 [357, 938]
sd of group Spring: 780 [637, 949]
sd of group Fall: 742 [609, 904]

The difference of the means is greater than 0 by a probability of >0.999
and less than 0 by a probability of <0.001


plot(casual_notworkingday_mean_bayes)
```

The package allows the use of summary for its output; with it we can see the CIs for each statistic as well as the standardized effect size (d) on this contrast:

```
summary(casual_notworkingday_mean_bayes)

  Measures
               mean      sd    HDIlo    HDIup %<comp %>comp
mu_x       1891.896 107.961 1684.426 2107.064  0.000  1.000
sigma_x     784.985  80.819  636.610  949.354  0.000  1.000
mu_y       1235.311 103.061 1038.706 1439.451  0.000  1.000
sigma_y     747.461  76.146  609.178  904.010  0.000  1.000
mu_diff     656.585 148.684  357.229  938.414  0.000  1.000
sigma_diff   37.523 109.254 -181.929  249.049  0.360  0.640
nu           46.196  32.717    5.557  110.800  0.000  1.000
eff_size      0.860   0.206    0.450    1.259  0.000  1.000
x_pred     1891.952 827.714  307.633 3559.847  0.012  0.988
y_pred     1231.742 781.921 -290.477 2794.578  0.056  0.944

'HDIlo' and 'HDIup' are the limits of a 95\% HDI credible interval.
'\%<comp' and '\%>comp' are the probabilities of the respective parameter being smaller or larger than 0.

  Quantiles
              q2.5%     q25%   median     q75%   q97.5%
mu_x       1680.655 1820.233 1891.884 1963.547 2103.933
sigma_x     642.575  728.117  779.837  835.775  958.585
mu_y       1035.024 1165.506 1235.177 1304.515 1437.405
sigma_y     614.269  694.245  741.870  794.897  912.781
mu_diff     365.089  556.231  656.493  756.053  947.740
sigma_diff -178.064  -33.623   37.084  108.491  253.580
nu            9.355   23.275   37.863   60.040  128.710
eff_size      0.461    0.720    0.859    0.996    1.271
x_pred      264.184 1346.469 1892.044 2432.797 3520.282
y_pred     -309.647  721.162 1225.480 1743.096 2781.779
```

Evaluations of the difference in medians and variances are planned but not yet implemented.

The single sample functions available to acquire simple CIs as of this writing include:

Statistic
Data type
Distribution
Function
Mean
Continuous
Normal, t, “normal enough”
bayes.t.test(x)
Proportion
Percent
Binomial
bayes.binom.test(x, n)
Count
Count
Poisson
bayes.poisson.test(x)
Rate
Count/T
Poisson
bayes.poisson.test(x, T)

Finally, you can review the MCMC diagnostics for any of these functions by using diagnostics:

```
diagnostics(casual_notworkingday_mean_bayes) # not shown
```

## Effect sizes: Measuring relationships between groups

The other basic analytics question aims to understand similarities between groups: how similar are two groups? Correlation is the primary tool here, although a wide variety of alternative approaches exist.

Correlation is one of the oldest and most well-known statistical tools, and for good reason: it provides a lot of detail in a single metric. While Pearson’s correlation is often the default approach by many analysts, it’s not always the best—in many cases, Spearman’s rho, Kendall’s tau-b, or polyserial/polychoric approaches are better choices. And, when you have categorical data, using association coefficients like the odds ratio, Yule’s Q, or Cohen’s kappa are appropriate tools, and are just as useful (and old) as correlation. We’ll explore how you can acquire each of these values with R in this recipe.

As before, every effect size measure should be accompanied by an appropriate confidence interval whenever possible, which is usually best done by bootstrap unless you can meet the method’s assumptions.

Although correlation coefficients are easy to acquire with R’s base installation, confidence intervals are not, so we’ll use the psych package for the non-parametric correlation part of this recipe and boot/bootES for some confidence intervals. We’ll also use a piece of the bike share dataset to illustrate the methods.

```
require(bootES)
require(psych)
bike_use_atemp = data.frame(bike_share_daily$atemp, bike_share_daily$cnt)
colnames(bike_use_atemp) = c("air_temp", "count")
```

Traditional Pearson’s correlation (r) is easy to acquire:

```
cor(bike_use_atemp$air_temp, bike_use_atemp$count)
cor.test(bike_use_atemp$air_temp, bike_use_atemp$count)$conf.int
```

0.6310657
0.5853376 0.6727918

You can get BCa bootstrap CIs for Pearson’s r by using:

```
bootES(c(bike_use_atemp$air_temp, bike_use_atemp$count), effect.type="r")

95.00% bca Confidence Interval, 2000 replicates
Stat        CI (Low)    CI (High)   bias        SE          
0.650       0.633       0.667       -0.000      0.009      
```

However, it’s pretty clear that this relationship isn’t truly linear if you plot it. Spearman’s and Kendall’s can be acquired from psych’s cor.ci function as follows (use the .emp—empirical—CI values to obtain the percentile confidence interval values, although the differences will be trivial with large sample sizes):

```cor.ci(bike_use_atemp, method="spearman", n.iter = 10000, plot=FALSE)

Coefficients and bootstrapped confidence intervals
         ar_tm count
air_temp 1.00       
count    0.62  1.00

 scale correlations and bootstrapped confidence intervals
            lower.emp lower.norm estimate upper.norm upper.emp p
ar_tm-count      0.57       0.57     0.62       0.67      0.67 0


cor.ci(bike_use_atemp, method="kendall", n.iter = 10000, plot=FALSE)

Coefficients and bootstrapped confidence intervals
         ar_tm count
air_temp 1.00       
count    0.43  1.00

 scale correlations and bootstrapped confidence intervals
            lower.emp lower.norm estimate upper.norm upper.emp p
ar_tm-count      0.39       0.39     0.43       0.47      0.47 0

```


Since the bike share data doesn’t lend itself well to exploring the association of categorical variables, we’ll explore “categorical correlation” with the Aspirin data from the abd package (?Aspirin). The functions we’ll use include the oddsratio function from the epitools package, and the Yule (Yule’s Q) function from psych:

```
data(Aspirin, package="abd")
oddsratio(table(Aspirin))$measure

         odds ratio with 95% C.I.
treatment  estimate     lower    upper
  Aspirin 1.0000000        NA       NA
  Placebo 0.9913098 0.9187395 1.069632


Yule(table(Aspirin))

-0.004352794

```

For categorical data, association coefficients provide the same type of information as correlation does for quantitative data, that is, a value scaled to [-1,1]. Perhaps the most famous association metric is the odds ratio, which is really useful but can be misleading for those not used to its mathematical properties. Yule’s Q has more desirable properties in many cases, especially in explaining results to business users. Cohen’s kappa is probably the best tool when you want to compare agreement instead of association (see below).

##nBootstrapping BCa CIs for non-parametric correlation

If you want to use the boot package to calculate BCa confidence intervals on the Spearman or Kendell correlation coefficients, here’s how to do it for each method:

```
rs_function = function(x,i){cor(x[i,1], x[i,2], method="spearman")
rs_boot = boot(bike_use_atemp, rs_function, R=10000)
boot.ci(rs_boot, type="bca")$bca[4:5]

0.5748000 0.6669792

rt_function = function(x,i){cor(x[i,1], x[i,2], method="kendall")
rt_boot = boot(bike_use_atemp, rt_function, R=10000)
boot.ci(rt_boot, type="bca")$bca[4:5]

0.3931281 0.4665541
```

## Partial correlations

If you have variables you want to control for in a correlation, we can turn again to the psych package. For example, if we wanted to evaluate the correlation between air temperature and bike use while controlling for windspeed, we could do it by creating a correlation matrix with the corr.test function:

```
bike_use_atemp_wind = data.frame(bike_share_daily$temp, bike_share_daily$cnt, 
  bike_share_daily$windspeed )
atemp_wind_count = corr.test(bike_use_atemp_wind, method="kendall")
atemp_wind_count$ci[1:3]

                               lower          r       upper
bk_shr_dly.t-bk_shr_dly.c  0.3713082  0.4321852  0.48936255
bk_shr_dly.t-bk_shr_dly.w -0.1807640 -0.1096882 -0.03747266
bk_shr_dly.c-bk_shr_dly.w -0.2173056 -0.1471101 -0.07540089
```

Once we have a matrix (see it with atemp_wind_count$\textdollar$), we can plug it into the partial.r function, specifying that the first two variables of the matrix are the correlation of interest and the third is the variable we wish to control for (the second and third inputs, respectively):

```partial.r(as.matrix(atemp_wind_count$r), c(1:2), 3)

                       bike_share_daily.atemp bike_share_daily.cnt
bike_share_daily.atemp                   1.00                 0.42
bike_share_daily.cnt                     0.42                 1.00
```

## Polychoric and polyserial correlation for ordinal data

Polychoric (ordinal-ordinal) and polyserial (numeric-ordinal) correlation allows you to perform correlation when some or all of your variables of interest are on an ordinal scale. Again, psych provides the functions we'll use. To illustrate, we'll use the math attitudes survey (mass) data seen earlier in this chapter, converted from ordered factor to numeric for the polychoric function to work:

```
data(mass, package="likert")
poly_math = data.frame(as.numeric(mass[,7]), as.numeric(mass[,14]))
colnames(poly_math) = c("worry", "enjoy")
polychoric(poly_math)$rho

           worry      enjoy
worry  1.0000000 -0.7731209
enjoy -0.7731209  1.0000000
```

To perform a polyserial correlation, you need to designate the two variables in the function. We'll add a fake variable on each student's pre-college math assessment test score to see whether it relates to their stated enjoyment:

```
math_score = c(755, 642, 626, 671, 578, 539, 769, 614, 550, 615, 749, 676, 753, 
  509, 798, 783, 508, 767, 738, 660)
polyserial(math_score, poly_math$enjoy)

0.2664201
```

\subparagraph{Cohen’s kappa for comparisons of agreement

Ratings on ordinal scales require special treatment, since they aren’t true numbers, and comparisons of agreement makes it even more necessary to choose a statistically-appropriate tool. Cohen’s kappa (cohen.kappa in the psych package) provides a way to compare ordinal ratings, e.g., a doctor’s ratings for a set of patients potentially eligible for care management as compared with the same output from a predictive model:

```
doctor = c(1,2,3,4,1,2,3,4,1,2,3,4)
model = c(1,3,1,3,1,4,4,4,4,2,3,1)
cohen.kappa(x=cbind(doctor,model))

Cohen Kappa and Weighted Kappa correlation coefficients and confidence boundaries

                 lower estimate upper
unweighted kappa -0.15     0.22  0.59
weighted kappa   -0.45     0.15  0.75
 Number of subjects = 12
```

You can also use raw categories instead of dummy variables:

```
doctor = c("yes", "no", "yes", "unsure", "yes", "no", "unsure", "no", "no", 
  "yes", "no", "yes", "yes")
model = c("yes", "yes", "unsure", "yes", "no", "no", "unsure", "no", "unsure", 
  "no", "yes", "yes", "unsure")
doctor_vs_model = data.frame(doctor, model)
cohen.kappa(doctor_vs_model)

Cohen Kappa and Weighted Kappa correlation coefficients and confidence boundaries

                 lower estimate upper
unweighted kappa -0.34   0.0631  0.47
weighted kappa   -0.57  -0.0078  0.56
 Number of subjects = 13
```

The agree output value shows the proportion of agreement for each category:

```
cohen.kappa(doctor_vs_model)$agree

        x2f
x1f              no     unsure        yes
  no     0.15384615 0.07692308 0.15384615
  unsure 0.00000000 0.07692308 0.07692308
  yes    0.15384615 0.15384615 0.15384615

  ```
  
## Bayesian correlation

We saw the Bayesian approach to difference-based effect size calculations in the previous recipe; here's the approach for correlations:

```
require(BayesianFirstAid)
bayes.cor.test(bike_use_atemp$air_temp, bike_use_atemp$count)


    Bayesian First Aid Pearson's Correlation Coefficient Test

data: bike_use_atemp$\textdollar$air_temp and bike_use_atemp$\textdollar$count (n = 731)
Estimated correlation:
  0.63
95\% credible interval:
  0.59 0.68
The correlation is more than 0 by a probability of >0.999
and less than 0 by a probability of <0.001


plot(atemp_bike_cor_bayes)
```
