# Chapter 4: Know Thy Data

- Creating summary plots
- Obtaining summary statistics
- Plotting univariate distributions
- Plotting bivariate and comparative distributions

It's often tempting to jump into analysis as soon as possible, but it is a cardinal sin to not evaluate your data with the basics first. This chapter covers the evaluations that must be done before anything else.

In this chapter, we'll primarily use the same bike share data we downloaded from the UCI Machine Learning Repository in the Loading data recipe of Chapter 2. To make it easy, the code below downloads the particular file we'll use throughout much of this chapter and writes the data to a csv file saved in your working directory. The first recipe then loads the csv, adjusts the column types appropriately, and runs through the essential first step of any analysis.

```
tf = tempfile()
download.file("http://archive.ics.uci.edu/ml/machine-learning-databases/00275
  /Bike-Sharing-Dataset.zip", tf)
bike_share_daily_raw = read.table(unz(tf, "day.csv"), header=TRUE, sep=",")
file.remove(tf)
write.table(bike_share_daily_raw, “bike_share_daily.csv”, row.names=FALSE)
rm(bike_share_daily_raw)
```

## Creating summary plots

If you were to poll data scientists on the "Three Most Important Steps in Any Analysis," Plot Thy Data should account for all three. Far too many business professionals rely on tables and simple summary statistics to make decisions, which are useful but can never provide the richness of information that graphs can. Further, decisions based on single statistic (usually the mean) are made much too often, usually with either good results reached by luck or accident, or—more often—you end up with (predictably) poor results. These days, I only rarely provide tables of summary stats to decision makers, often choosing instead to provide distributional or other EDA plots with corresponding ranges of variation (standard deviation, IQR, etc.) or uncertainty (confidence limits, credibility intervals, etc.) instead of “naked” single values like a mean or median.  

The recipes below are aimed at giving you a quick visual summary of the data, not to produce final plots—and so they're rough and use functions that can be difficult to modify. We'll see recipes for production plots later in this chapter and throughout the book, but we often just need to see the data before doing anything else.

The `ggpairs` function in the `GGally` package has a relatively slow but visually wonderful way to plot your dataset, so load that library before continuing.

```
require(GGally)
```

GGally is built over `ggplot2` (which we'll use often in this book), and its syntax works in more or less the same manner as `qplot`. You can create the summary plot of a subset of the bike share data for an at-a-glance overview:

```
ggpairs(data=bike_share_daily, columns=c(14:15, 10, 13, 3, 7), 
  title="Daily Bike Sharing Data", axisLabels="show", color="season")
```
![ggpairs](/images/Ch4_01.png)

It’s helpful to use a non-parametric correlation method if you don’t know a priori whether the quantitative variables are linearly related in their raw forms. Unfortunately, `ggpairs` doesn’t yet provide this functionality, so know that for continuous pairs of variables that don’t show a linear point cloud, the correlation coefficient in this graph will be *larger than it should be*.

`GGally` is still in active development, so currently it's pretty difficult to customize the `ggpairs` plot, and it may rarely be useful to anyone other than you. Even then, it's a great way to get a sense of the data at a glance. I use this function with almost any dataset I work with.

## Create histograms of all numeric variables in one plot

Sometimes you just want to see histograms, but coding and laying out a bunch of them in a grid is more work than it needs to be. The `psych` package's `multi.hist` function can do this in a single line. Note that if you specify variables that aren't numeric, it will throw an error.

```
require(psych)
multi.hist(bike_share_daily[,sapply(bike_share_daily, is.numeric)])
```
![multi.hist](/images/Ch4_02.png)

## A better "pairs" plot

The `psych` package also has a modification of the built-in `graphics` package's `pairs` function that's more useful out of the box. As mentioned above, until you have evidence that your paired variables are indeed linear in raw form, you should modify the default calculation option to `method="spearman"` for continuous data or `method="kendall"` for ordinal data or continuous data in which you have some known outliers:

```
pairs.panels(bike_share_daily[,sapply(bike_share_daily, is.numeric)], 
  ellipses=FALSE, pch=".", las=2, cex.axis=0.7, method="kendall")
```
![pairs.panels](/images/Ch4_03.png)

## Mosaic plots—"Scatterplots" for categorical data

Once you've loaded the `vcd` package, the `pairs` function can also create a matrix of mosaic plots, either by using the table function on a dataframe, or by referencing a table object.

```
require(vcd)
pairs(table(bike_share_daily[c(3, 9)])) # not shown
```

The dataset we've used so far in this recipe isn't a great one for mosaic plots since most of those variables are time-related, so a better view of the outcome of plotting categorical pairs can be seen with the built-in `Titanic` dataset (`?Titanic`), using the highlighting option to make the categories more clearly distinct at a glance:

```
pairs(Titanic, highlighting=2)
```
![mosaic pairs](/images/Ch4_04.png)

## Obtaining summary statistics

Once you've got plots, it's good to have the summary statistics to go along with the variables you’ve plotted. Functions to calculate summary statistics are in several packages, but I’ve found that the `psych` package probably has the simplest way to obtain them, particularly for groupings or when you want to condition on a particular value or threshold.

It’s important to remember that each type of data can reasonably use only certain kinds of summary statistics. The following table should help when, for example, a decision maker asks you to calculate the mean of an ordinal variable (such as a Likert scale)—just show them this table so you don’t have to go on record as the one who said it was a stupid idea:

[BRING BACK MD TABLE]


\* Ratio variables have a "true zero" while interval variables do not (e.g., temperature in °C).
\** Discrete variables can have means/variances based on non-normal probability distributions (e.g., binomial for proportions, Poisson for counts or rates, etc.), but this should not be confused with an arithmetic mean or variance.

Load the `psych` package and the daily bike share data if they're not already loaded from the previous recipe.

```
require(psych)
bike_share_daily=read.table("bike_share_daily.csv", sep=",", header=T)
```

For numeric variables, the describe and describeBy functions do all the work for you, although you do need to specify which columns are quantitative—or which columns you want summary stats on—or it will return (non-sensically) all of them:

```
describe(bike_share_daily[10:16]) # not shown
describeBy(bike_share_daily[10:16], bike_share_daily$holiday)
```
```
group: No
           vars   n    mean      sd median trimmed     mad min  max range skew kurtosis    se
casual        1 710  841.77  680.53  711.5  739.69  584.14   2 3410  3408 1.27     1.34 25.54
registered    2 710 3685.33 1553.70 3691.0 3672.14 1695.35  20 6946  6926 0.04    -0.71 58.31
---------------------------------------------------------------------------------------------
group: Yes
           vars  n    mean      sd median trimmed     mad min  max range skew kurtosis     se
casual        1 21 1064.71  860.05    874  965.88  658.27 117 3065  2948 0.88    -0.41 187.68
registered    2 21 2670.29 1492.86   2549 2604.47 1599.73 573 5172  4599 0.27    -1.26 325.77
```

The `table` and `prop.table` functions from the `base` package provide the simplest summary stats for categorical and ordinal variables:

```
table(bike_share_daily$holiday)
No Yes
710  21

prop.table(table(bike_share_daily$holiday))

        No        Yes
0.97127223 0.02872777
```

There are a variety of approaches and options using these functions.

For example, you can subset the quantitative stats based on factors or conditions:

```
describeBy(bike_share_daily[14:16], bike_share_daily$season == "Winter") # not shown
describeBy(bike_share_daily[14:16], bike_share_daily$temp > 0.5) # not shown
describeBy(bike_share_daily$casual, bike_share_daily$windspeed > 
  mean(bike_share_daily$windspeed))

group: FALSE
  vars   n   mean     sd median trimmed    mad min  max range skew kurtosis    se
1    1 409 914.61 673.18    773  827.61 598.97   9 3410  3401 1.11     0.96 33.29
---------------------------------------------------------------------------------------------
group: TRUE
  vars   n  mean     sd median trimmed   mad min  max range skew kurtosis    se
1    1 322 763.8 695.26  613.5  642.12 579.7   2 3283  3281  1.5     1.94 38.75
```

You can create two- , three-, or *n*-way tables by adding variables to the table call, and use `addmargins` to get marginal sums:

```
table(bike_share_daily[c(3,6:7)]) # not shown
addmargins(table(bike_share_daily[c(3,6:7)])) # not shown
prop.table(table(bike_share_daily[c(3,9)]))

        weathersit
season             1           2           3
  Winter 0.151846785 0.090287278 0.005471956
  Spring 0.154582763 0.093023256 0.004103967
  Summer 0.186046512 0.065663475 0.005471956
  Fall   0.140902873 0.088919289 0.013679891
```

Sometimes you may want the counts to be in a data frame, either for use in plotting or because tables with more than 2 or 3 dimensions can be difficult to understand at a glance. You can create any *n*-way table as a data frame using the `plyr` package's `count` function:

```
require(plyr)
count(bike_share_daily[c(3,6:7)])

    season holiday weekday freq
1  Winter      No       0   27
2  Winter      No       1   20
3  Winter      No       2   24
4  Winter      No       3   25
…
```

## Inference on summary statistics: confidence intervals

When we don’t have a full population to assess, we often need to determine how uncertain our estimates of the population parameters are—we need confidence intervals on those point estimates. There are `base` installation functions for most of the ones we typically need:

| Statistic | Data type | Distribution | Function |
| --------- | --------- | ------------ | -------- |
| Median | Any | None | `wilcox.test(x, conf.int=TRUE)$conf.int` |
| Mean| Continuous | Normal, t, “normal enough” | `t.test(x)$conf.int` |
| Proportion | Percentage | Binomial | `binom.test(x, n)$conf.int` |
| Count | Count | Poisson | `poisson.test(x)$conf.int` |
| Rate | Count/*n* | Poisson | `poisson.test(x, n)$conf.int` |

Sometimes there isn’t a `base` function for the confidence interval we need, or our data depart considerably from one of the typical distributions. For example, the confidence interval of a standard deviation is very sensitive to departures from normality, so R doesn’t have a built-in function for it. Non-parametric bootstrapping is the way to get these intervals; the `boot` package provides the most options but can be a pain to work with since its syntax is fairly different from most everything else in R. Still, it does what we need it to with a relative minimum of code for most cases. In this example we’ll use the `CO2` dataset that comes with R for simplicity’s sake:

```
require(boot)
sd_boot_function = function(x,i){sd(x[i])
sd_boot = boot(CO2$conc, sd_boot_function, R=10000)
sd(CO2$conc)
boot.ci(sd_boot, type="bca")$bca[4:5]

295.9241
258.8950 333.7344
```

The BCa method is generally the appropriate calculation unless you have strong reason to believe that another method is best in the particular context. For example, you might want to use `type="norm"`, i.e., use the normal approximation for very small samples that you expect are “normal enough,” realizing that due to sampling error you could be estimating intervals that are wildly incorrect.   

We can also bootstrap confidence intervals for any summary statistic. For example, to get the bootstrapped 75th percentile, run the following:

```
q75_function = function(x,i){quantile(x[i], probs=0.75)
q75_boot = boot(CO2$conc, q75_function, R=10000)
quantile(CO2$conc,0.75)
boot.ci(q75_boot, type="bca")$bca[4:5]

675
500 675
```

## Inference on summary statistics: tolerance intervals

While confidence limits tend to be more often used in data science, tolerance intervals are more useful in engineering or industrial contexts: there’s a profound conceptual and practical difference between, for example, a good estimate of average commuting time to work (using a 95% *confidence* interval) and how long you can expect 95% of those trips will take (using a 95% *tolerance* interval). While the former type of value tends to be used more by statisticians and data miners, engineers and industrial analysts—and sometimes business users—find the latter more useful. 

Essentially, a good way to remember the difference is that as the sample size approaches the population size, the confidence interval will approach zero, while a tolerance interval will approach the population percentiles.

The `tolerance` package contains functions for estimating tolerance intervals for distributions typically used in most engineering or industrial processes. As an example, assume you have data on commuting time for 40 randomly chosen workdays. The distribution is oddly-shaped, so we’ll use non-parametric tolerance intervals to estimate (at a 95% confidence level) the number of minutes that 75% of commutes will take:

```
require(tolerance)
commute_time = c(68, 42, 40, 69, 46, 37, 68, 68, 69, 38, 51, 36, 50, 37, 41, 
  68, 59, 65, 67, 42, 67, 62, 48, 52, 52, 44, 65, 65, 46, 67, 62, 66, 43, 58, 
  45, 65, 60, 55, 48, 46)
commute_time_npti = nptol.int(commute_time, alpha=0.05, P=0.75, side=2)
commute_time_npti

  alpha    P 2-sided.lower 2-sided.upper
1  0.05 0.75            37            68
```

You can also obtain a graphical summary with the plottout function, which shows a control chart on the left and a histogram on the right:

```
plottol(commute_time_npti, commute_time, side="two", plot.type="both")
```
![tolerance interval plot](/images/0815OS_02_05.png)


If you plotted both tolerance graphs (plot.type="both"), you'll need to return par to normal manually: 

```
par(mfrow=c(1,1))
```

The following table provides a short overview of the primary univariate tolerance interval functions; see `?tolerance` for more details. I recommend you set `alpha` (confidence level), `P` (the coverage proportion, i.e., tolerance interval), and `side` (whether to calculate upper, lower, or both bounds) manually for best results; if not chosen, `alpha` defaults to 0.05, `P` defaults to 0.99, and `side` defaults to 1 (you’ll probably want 2 in many cases). Other settings depend on the distribution used, and each distribution sometimes has several algorithms to choose from, so it’s worth exploring the documentation before diving in here.



Data type
Distribution
Function
Percent
Binomial
bintol.int(x, n, m, …)
Count
Poisson
poistol.int(x, n, m, side, …)
Continuous
None
nptol.int(x, …)
Continuous
Normal, t, “normal enough”
normtol.int(x, side, …)
Continuous
Uniform
uniftol.int(x, …)
Lifetime/survival
Exponential
exptol.int(x, type.2, …)
Score
Laplace
laptol.int(x, …)
Indicies
Gamma
gamtol.int(x, …)
Reliability, extreme values
Weibull, Gumbel
extol.int(x, dist, …)

## Plotting univariate distributions

We saw the quick-and-dirty way to plot a bunch of distributions at once in an earlier recipe; this one focuses on a production-ready single-distribution plot using the `ggplot2` package.

This package can seem daunting at first; the power of its immense flexibility and beauty comes at the cost of a bewildering variety of options. But it's well worth the effort to learn, as you'll be able to customize virtually anything you decide to plot. The documentation is very thorough, but if you need a cheat-sheet to simply the possibilities down to the common needs and uses, Zev Ross has a great one on his [blog](http://zevross.com/blog/2014/08/04/beautiful-plotting-in-r-a-ggplot2-cheatsheet-3/). 

Load the bike share data we've been using, as well as the packages we'll use in this recipe (if they're not already loaded).

```
require(ggplot2)
require(scales)
```

For quantitative data, business users are used to seeing histograms; quants prefer density plots. Here's how to do both, separately...  

```
ggplot(bike_share_daily, aes(casual)) +
    geom_density(col="blue", fill="blue", alpha=0.3) +
    xlab("Casual Use") +
    theme_bw()

ggplot(bike_share_daily, aes(casual)) +
    geom_histogram(col="blue", fill="blue", alpha=0.3) +
    xlab("Casual Use") +
    theme_bw()
```

![density and histogram](/images/0815OS_02_06a.png)

ggplot(bike_share_daily, aes(casual)) +
    ylab("density and count") +
    xlab("Casual Use") +
    geom_histogram(aes(y=..density..), col="blue", fill="blue", alpha=0.3) +
    geom_density(col="blue", fill="blue", alpha=0.2) +
    theme_bw() +
    theme(axis.ticks.y = element_blank(), axis.text.y = element_blank())
```

...as well as together:

![density and histogram](/images/0815OS_02_06b.png)

When asked, I’ve usually explained density plots as “more objectives histograms,” which, while not quite true, is accurate enough for business users. If you do need to explain things, see `?density` for more details on the methods and useful additional references; `?stat_density` provides an overview of calculation methods used in `ggplot2`, while `?geom_density` provides information on graphical options. 

That pattern carries over to many `ggplot2` options—find details of the method by replacing the `x` with the stat or geom in `?stat_x` and `?geom_x` for calculation and plotting information, respectively.

When you have categorical data, bar or dot plots serve the same purpose:  

```
ggplot(bike_share_daily, aes(weathersit)) +
    geom_bar(col="blue", fill="blue", alpha=0.3) +
    xlab("Weather Pattern") +
    scale_x_discrete(breaks=c(1, 2, 3), labels=c("Clear", "Cloudy/Rainy", 
    "Stormy")) +
    theme_bw()
ggplot(bike_share_daily, aes(x=weathersit, y=..count.. )) +
    geom_point(stat = "bin", size = 3, pch=15, col="blue") +
    xlab("Weather Pattern") +
    scale_x_discrete(breaks=c(1, 2, 3), labels=c("Clear", "Cloudy/Rainy", 
    "Stormy")) +
    theme_bw()
```

![bar and dot](/images/0815OS_02_07a.png)

You can also use `coord_flip()` to move from a normal bar chart to a horizontal one:
```
ggplot(bike_share_daily, aes(weathersit)) +
    geom_bar(col="blue", fill="blue", alpha=0.3) +
    xlab("") +
    scale_x_discrete(breaks=c(1, 2, 3), labels=c("Clear", "Cloudy/Rainy", 
    "Stormy")) +
    coord_flip() +
    theme_bw()
```

![horizontal bar](/images/0815OS_02_07b.png)


## Plotting multiple univariate distributions with faceting

We saw the quick-and-dirty way to plot multiple histograms with multi.hist a few recipes ago; here's how to use ggplot2 to create the same thing more elegantly:

```
ggplot(bike_share_daily, aes(casual, fill=season)) +
    geom_histogram(aes(y = ..density..), alpha=0.2, color="gray50") +
    geom_density(alpha=0.5, size=0.5) +
    facet_wrap(~season) +
    theme_light() +
    xlab("Daily Bike Use Count") +
    ylab("") +
    theme(legend.position="none") +
    theme(axis.ticks.y = element_blank(), axis.text.y = element_blank(),
          axis.title = element_text(size=9, face=2, color="gray30"),
          axis.title.x = element_text(vjust=-0.5))
```

![ggplot multi.hist](/images/0815OS_02_08.png)

The same can be done for categorical distributions, too, of course:

```
ggplot(bike_share_daily, aes(weathersit, fill=season)) +
    geom_bar(alpha=0.5) +
    xlab("") +
    ylab("Number of Days") +
    scale_x_discrete(breaks=c(1, 2, 3), labels=c("Clear", "Cloudy/Rainy", 
    "Stormy")) +
    coord_flip() +
    facet_wrap(~season, ncol=1) +
    theme_light()
```

![ggplot multi.bar](/images/0815OS_02_09.png)

## What about pie charts?

With apologies to Hunter S. Thompson, if you use pie charts for anything outside of GraphJam, you deserve whatever happens to you.

## Plotting bivariate and comparative distributions

Most of the time, we want to understand how one thing compares to something else. There’s no better way to do this by plotting each group’s distribution against the other’s.

We continue using the bike share data, along with `ggplot2`. Make sure to load the `vcd` package as well:  

```
require(ggplot2)
require(scales)
require(vcd)
```

For quantitative data, plotting the two distributions over each other provides a clear, direct comparison:

```
ggplot(bike_share_daily, aes(casual, fill=workingday, color=workingday)) +
    geom_density(alpha=0.4) +
    theme_minimal() +
    xlab("Daily Casual Bike Use Count") +
    ylab("") +
    scale_fill_discrete(name="Work Day?") +
    scale_color_discrete(name="Work Day?") +
    theme(axis.ticks.y = element_blank(), axis.text.y = element_blank(), 
    legend.position="top")
```

![double density plot](/images/0815OS_02_10.png)


The `vcd` package has a nice implementation of mosaic plots for categorical data called, as you might expect, `mosaic`:

```
mosaic(~ weathersit + season, data=bike_share_daily, shade=T, 
  legend=F, labeling_args = list(set_varnames = c(season = "Season", 
  weathersit = "Primary Weather Pattern"), set_labels = list(weathersit = 
  c("Clear", "Cloudy/Rainy",  "Stormy"))))
```

![mosaic](/images/0815OS_02_11.png)

The first plot is simply a plot of the distributions of each group, i.e., a set of overplotted density histograms.
If you have categorical data, you'll probably want to use mosaic plots to visualize differences between groups; if you're not already familiar with them, think of mosaics plots as contingency tables made with shapes—the larger the shape, the higher the cell count.

For comparison, here's the table that the mosaic plot represents:

```
table(bike_share_daily$weathersit, bike_share_daily$season)
```

Weather Code
Winter
Spring
Summer
Fall
1 (Clear)
111
113
136
103
2 (Cloud/Rainy)
66
68
48
65
3 (Stormy)
4
3
4
10

## Multiple bivariate comparisons with faceting

As with univariate plots, we can facet on another variable for comparison’s sake; for example, if we take the bike share data shown above and facet by season:

```
ggplot(bike_share_daily, aes(casual, fill=workingday, color=workingday)) +
    geom_density(alpha=0.4) +
    theme_minimal() +
    xlab("Daily Casual Bike Use Count") +
    ylab("") +
    scale_fill_discrete(name="Work Day?") +
    scale_color_discrete(name="Work Day?") +
    facet_wrap(~season, ncol=2) +
    theme(axis.ticks.y = element_blank(), axis.text.y = element_blank(), 
    legend.position="top")
```

## Plotting survey data

There are a variety of ways you can plot survey data, but the `likert` package makes things really simple. We can use the `mass` dataset (Math Anxiety Scale Survey) built into `likert` to demonstrate:

```
require(likert)
mathiness = likert(mass[2:15])
plot(mathiness)
```

![basic likert plot](/images/0815OS_02_13.png)

There's a known bug in `likert` where you have to load the `reshape` package manually (not reshape2) to make the `grouping` option work.

```
require(reshape)
gender_math = likert(items=mass[,c(4,6,15), drop=FALSE], grouping=mass$Gender)
plot(gender_math, include.histogram=TRUE)
```

![grouped likert plot](/images/0815OS_02_14.png)

Because the `reshape` package masks functions in `reshape2` and `dplyr`, you should detach `likert` and `reshape` when you're done plotting.

```
detach("package:likert", unload=TRUE)
detach("package:reshape", unload=TRUE)
```
