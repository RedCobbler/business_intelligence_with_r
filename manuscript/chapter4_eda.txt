# Chapter 4: Know Thy Data---Exploratory Data Analysis

- Creating summary plots  
- Plotting univariate distributions  
- Plotting bivariate and comparative distributions  
- Plotting survey data  
- Obtaining summary and conditional statistics  
- Inference on summary statistics  
- Dealing with missing data  

Exploration and evaluation is the heart of analytics; this chapter covers a wide variety of ways to understand, display, and summarize your data. 

The `ggplot2` is perhaps the most useful (and logical) graphics package for R, and we'll use it extensively in this book. The `scales` package is often loaded with it, to provide support for things like date and time axes. 

```
require(ggplot2)
require(scales)
```

`ggplot2` can seem daunting at first; the power of its immense flexibility and beauty comes at the cost of a bewildering variety of options. But it's well worth the effort to learn, as you'll be able to customize virtually anything you decide to plot. 

I> #### Need a `ggplot2` cheat sheet?
I>
I> RStudio has a great A3/11x17 sized overview of all the [primary ggplot functions and options](http://www.rstudio.com/wp-content/uploads/2015/12/ggplot2-cheatsheet-2.0.pdf) on their [cheatsheet page](https://www.rstudio.com/resources/cheatsheets/).  
I>
I> The [`ggplot2` documentation](http://docs.ggplot2.org/current/) is very thorough, but if you need a cheat-sheet to simplify the possibilities down to some common tasks, there's a great introduction on [Computer World](http://www.computerworld.com/article/2935394/business-intelligence/my-ggplot2-cheat-sheet-search-by-task.html). For an overview of common `ggplot2` tasks with worked examples, [Cookbook R](http://www.cookbook-r.com/Graphs/) is quite useful, and Zev Ross has a more thorough but also well done overview on his [blog](http://zevross.com/blog/2014/08/04/beautiful-plotting-in-r-a-ggplot2-cheatsheet-3/).

We'll primarily use the bike share data we downloaded from the UCI Machine Learning Repository as one of the examples in Chapter 2. To make it easy, the code below downloads and imports the data we'll use throughout much of this chapter (as well as the rest of the book) and adjusts the column types and levels.

```
download.file("http://archive.ics.uci.edu/ml/machine-learning-databases/
  00275/Bike-Sharing-Dataset.zip", "Bike-Sharing-Dataset.zip")
  
bike_share_daily = read.table(unz("Bike-Sharing-Dataset.zip", "day.csv"),
  colClasses=c("character", "Date", "factor", "factor", "factor", "factor",
    "factor", "factor", "factor", "numeric", "numeric", "numeric", "numeric",
    "integer", "integer", "integer"), sep=",", header=TRUE)

levels(bike_share_daily$season) = c("Winter", "Spring", "Summer", "Fall")

levels(bike_share_daily$workingday) = c("No", "Yes")

levels(bike_share_daily$holiday) = c("No", "Yes")

bike_share_daily$mnth = ordered(bike_share_daily$mnth, 1:12)

levels(bike_share_daily$mnth) = c(month.abb)

levels(bike_share_daily$yr) = c(2011, 2012)
```

## Creating summary plots

If you were to poll data scientists on the "Three Most Important Steps in Any Analysis," ***Plot Thy Data*** should account for all three. Far too many business professionals rely on tables and simple summary statistics to make decisions, which are useful but can never provide the richness of information that graphs can. Further, decisions based on single statistic (usually the mean) are made much too often, usually with either good results reached by luck or accident, or—more often—you end up with (predictably) poor results. 

The recipes below are aimed at giving you a quick visual summary of the data, not to produce final plots—and so they're rough and use functions that can be difficult to modify. We'll see recipes for production plots later in this chapter and throughout the book, but we often just need to see the data before doing anything else.

### Everything at once: ggpairs

The `ggpairs` function in the `GGally` package has a relatively slow but visually wonderful way to plot your dataset, so load that library before continuing.

```
require(GGally)
```

GGally is built over `ggplot2` (which we'll use often in this book), and its syntax works in more or less the same manner as `qplot`. You can create the summary plot of a subset of the bike share data for an at-a-glance overview:

```
ggpairs(data=bike_share_daily, columns=c(14:15, 10, 13, 3, 7), 
  title="Daily Bike Sharing Data", axisLabels="show", 
  mapping=aes(color=season, alpha=0.3))
```

![](images/Ch4_01.png)

It’s helpful to use a non-parametric correlation method if you don’t know *a priori* whether the quantitative variables are linearly related in their raw forms. Unfortunately, `ggpairs` doesn’t yet provide this functionality, so know that for continuous pairs of variables that don’t show a linear point cloud, the correlation coefficients printed in this graph will be larger than they actually are.

`GGally` is still in active development, so currently it's pretty difficult to customize the `ggpairs` plot, and it may rarely be useful to anyone other than you. Even then, it's a great way to get a sense of the data at a glance. 


### Create histograms of all numeric variables in one plot

Sometimes you just want to see histograms, but coding and laying out a bunch of them in a grid is more work than it needs to be. The `psych` package's `multi.hist` function can do this in a single line. Note that if you specify variables that aren't numeric, it will throw an error.

```
require(psych)

multi.hist(bike_share_daily[,sapply(bike_share_daily, is.numeric)])
```

![](images/Ch4_02.png)


### A better "pairs" plot

The `psych` package also has a modification of the built-in `graphics` package's `pairs` function that's more useful out of the box. As mentioned above, until you have evidence that your paired variables are indeed linear in raw form, you should modify the default calculation option to `method="spearman"` for continuous data or `method="kendall"` for ordinal data or continuous data in which you have some known outliers:

```
pairs.panels(bike_share_daily[,sapply(bike_share_daily, is.numeric)], 
  ellipses=FALSE, pch=".", las=2, cex.axis=0.7, method="kendall")
```

![](images/Ch4_03.png)

### Mosaic plots: "Scatterplots" for categorical data

Once you've loaded the `vcd` package, the `pairs` function can also create a matrix of mosaic plots, either by using the table function on a dataframe, or by referencing a table object.

The dataset we've used so far in this recipe isn't a great one for mosaic plots since most of those variables are time-related, so a better view of the outcome of plotting categorical pairs can be seen with the built-in `Titanic` dataset (`?Titanic`), using the highlighting option to make the categories more clearly distinct at a glance:

```
require(vcd)

pairs(Titanic, highlighting=2)
```

![](images/Ch4_04.png)


## Plotting univariate distributions

We saw the quick-and-dirty way to plot a bunch of distributions at once above; now we focus on production-ready plots using the `ggplot2` package.


### Histograms and density plots

For quantitative data, business users are used to seeing histograms; quants prefer density plots. Here's how to do both, separately...  

```
ggplot(bike_share_daily, aes(casual)) +
  geom_density(col="blue", fill="blue", alpha=0.3) +
  xlab("Casual Use") +
  theme_bw()

ggplot(bike_share_daily, aes(casual)) +
  geom_histogram(col="blue", fill="blue", alpha=0.3) +
  xlab("Casual Use") +
  theme_bw()
```

![](images/0815OS_02_06a.png)

...as well as together, with the histogram command set to plot on the density scale instead of by counts:

```
ggplot(bike_share_daily, aes(casual)) +
  ylab("density and count") +
  xlab("Casual Use") +
  geom_histogram(aes(y=..density..), col="blue", fill="blue", alpha=0.3) +
  geom_density(col="blue", fill="blue", alpha=0.2) +
  theme_bw() +
  theme(axis.ticks.y = element_blank(), axis.text.y = element_blank())
```

![](images/0815OS_02_06b.png)

When asked, I’ve usually explained density plots as “more objective histograms,” which, while not quite true, is accurate enough for business users. See `?density` for more details on the methods and useful additional references; `?stat_density` provides an overview of calculation methods used in `ggplot2`, while `?geom_density` provides information on graphical options. 

I> That pattern carries over to many `ggplot2` options—find details of the method by adding the particular stat or geom in `?stat_` and `?geom_` for calculation and plotting information, respectively.

### Bar and dot plots

When you have categorical data, bar or dot plots serve the same purpose:  

```
ggplot(bike_share_daily, aes(weathersit)) +
  geom_bar(col="blue", fill="blue", alpha=0.3) +
  xlab("Weather Pattern") +
  scale_x_discrete(breaks=c(1, 2, 3), labels=c("Clear",
    "Cloudy/Rainy", "Stormy")) +
  theme_bw()

ggplot(bike_share_daily, aes(x=weathersit, y=..count..)) +
  geom_bar(stat="count", width=0.01) +
  geom_point(stat = "count", size=4, pch=21, fill="darkblue") +
  xlab("Weather Pattern") +
  scale_x_discrete(breaks=c(1, 2, 3), labels=c("Clear", 
    "Cloudy/Rainy", "Stormy")) +
  coord_flip() +
  theme_bw()
```

![](images/bar_and_dot.png)


### Plotting multiple univariate distributions with faceting

We saw the quick-and-dirty way to plot multiple histograms with `multi.hist` a few recipes ago; here's how to use ggplot2 to create the same thing more elegantly:

```
ggplot(bike_share_daily, aes(casual, fill=season)) +
  geom_histogram(aes(y = ..density..), alpha=0.2, color="gray50") +
  geom_density(alpha=0.5, size=0.5) +
  facet_wrap(~season) +
  theme_light() +
  xlab("Daily Bike Use Count") +
  ylab("") +
  theme(legend.position="none") +
  theme(axis.ticks.y = element_blank(), axis.text.y = element_blank(),
    axis.title = element_text(size=9, face=2, color="gray30"),
    axis.title.x = element_text(vjust=-0.5))
```

![](images/0815OS_02_08.png)

The same can be done for categorical distributions, too, of course:

```
ggplot(bike_share_daily, aes(weathersit, fill=season)) +
  geom_bar(alpha=0.5) +
  xlab("") +
  ylab("Number of Days") +
  scale_x_discrete(breaks=c(1, 2, 3), labels=c("Clear", 
    "Cloudy/Rainy", "Stormy")) +
  coord_flip() +
  facet_wrap(~season, ncol=1) +
  theme_light()
```

![](images/0815OS_02_09.png)

A> #### What about pie charts?
A> 
A> With apologies to Hunter S. Thompson, if you use pie charts for anything outside of GraphJam, you deserve whatever happens to you.


## Plotting bivariate and comparative distributions

Most of the time, we want to understand how one thing compares to something else. There’s no better way to do this other than by plotting each group’s distribution against the other’s.

We continue using the bike share data, along with `ggExtra`, `vcd, and `beanplot`. 

```
require(ggExtra)
require(vcd)
require(beanplot)
```

### Double density plots

For quantitative data, plotting the two distributions over each other provides a clear, direct comparison:

```
ggplot(bike_share_daily, aes(casual, fill=workingday, color=workingday)) +
  geom_density(alpha=0.4) +
  theme_minimal() +
  xlab("Daily Casual Bike Use Count") +
  ylab("") +
  scale_fill_discrete(name="Work Day?") +
  scale_color_discrete(name="Work Day?") +
  theme(axis.ticks.y = element_blank(), axis.text.y = element_blank(), 
    legend.position="top")
```

![](images/bike_double_density.png)

### Boxplots

Boxplots can be useful for comparing more than a couple of distributions, and can do so using more than one factor:

```
ggplot(bike_share_daily, aes(mnth, casual, fill=workingday)) +
  xlab("Month") +
  ylab("Daily Casual Bike Use Count") +
  geom_boxplot() +
  theme_minimal() +
  scale_fill_discrete(name="Work Day?") +
  scale_color_discrete(name="Work Day?") +
  theme(legend.position="bottom")
```

![](images/bike_boxplot.png)

### Beanplots

Violin plots have been considered a more useful alternative to boxplots, as you can see the shape of the distribution, not just its major quantiles. Unfortunately, violin plots can sometimes look like a series of genetalia, which pretty much precludes its use in business settings. 

Fortunately, bean plots allow you to create "half" violin plots, or more accurately, comparitive density histograms. The `side` option allows you to only plot the density on one side of a line; using `side="first"` has the height of each curve going to the left side of the plot. By playing with the `what` options, you can determine what pieces of the bean plot are shown. One of those pieces is a reference line, which defaults to the mean; `overallline="median"` allows you to use that statistic instead. 

```
beanplot(casual ~ mnth, data = bike_share_daily, side="first", 
  overallline="median", what=c(1,1,1,0), col=c("gray70", 
  "transparent", "transparent", "blue"), xlab = "Month", 
  ylab = "Daily Casual Bike Use Count")
```

![](images/beanplot1.png)  


If you prefer a more traditional look, with the density curves pointing up, you can use `horizontal=TRUE`, change `side` to `"second"`, and flip your axis titles:

```
beanplot(casual ~ mnth, data = bike_share_daily, side="second", 
  overallline="median", what=c(1,1,1,0), col=c("gray70", 
  "transparent", "transparent", "blue"), ylab = "Month", 
  xlab = "Daily Casual Bike Use Count", horizontal=TRUE)
```

![](images/beanplot2.png)  

Like the boxplot example above, you can also add a comparison for groups within a category, and each group's density is plotted base-to-base. So while you don't have the exact symmetry of violin plots, the result can provide much the same visual impression, so it's probably best to stick with the use of one side of the "bean."


### Scatterplots and marginal distributions

The `ggExtra` package provides a simple way to plot scatterplots with marginal distributions with the `ggMarginal` function. It defaults to density curves, but the histogram option presents a clearer picture. First, build the scatterplot and assign it to an object.

```
bike_air_temp = ggplot(bike_share_daily, aes(x=atemp, y=casual)) +
  xlab("Daily Mean Normalized Air Temperature") +
  ylab("Number of Total Casual Bike Uses") +
  geom_point(col="gray50") +
  theme_bw()
```

Then, use `ggMarginal` to create the final product:

```
bike_air_temp_mh = ggMarginal(bike_air_temp, type="histogram")

bike_air_temp_mh
```

![](images/scatter_marginal_hist.png)

Sometimes density plots can be useful, or perhaps you want to keep it simple and show a boxplot. Just change the `type` option:

```
bike_air_temp_md = ggMarginal(bike_air_temp, type="density")

bike_air_temp_md
```

![](images/scatter_marginal_density.png)  

```
bike_air_temp_mb = ggMarginal(bike_air_temp, type="boxplot")

bike_air_temp_mb
```

![](images/scatter_marginal_boxplot.png)  


### Mosaic plots

The `vcd` package has a nice implementation of mosaic plots for exploring categorical data distributions called, as you might expect, `mosaic`:

```
mosaic(~ weathersit + season, data=bike_share_daily, shade=T, 
  legend=F, labeling_args = list(set_varnames = c(season = "Season", 
  weathersit = "Primary Weather Pattern"), set_labels = list(weathersit = 
  c("Clear", "Cloudy/Rainy",  "Stormy"))))
```

![](images/0815OS_02_11.png)

A> #### Um. What's a mosaic plot?
A> 
A> If you're not already familiar with them, think of mosaic plots as contingency tables made with shapes---the larger the shape, the higher the cell count.
A> 
A> For comparison, here's the table that the mosaic plot represents:  
A> 
A> | | Winter | Spring | Summer | Fall |
A> |:-|------:|------:|------:|----:|
A> | Clear | 111 | 113 | 136 | 103 |
A> | Cloudy/Rainy | 66 | 68 | 48 | 65 |
A> | Stormy | 4 | 3 | 4 | 10 |
A> 
A> Sure, a table can be more compact, but the mosaic gives you the pattern at a glance; either is an option depending on what you need to get across.


### Multiple bivariate comparisons with faceting

As with univariate plots, we can facet on another variable for comparison’s sake; for example, if we take the bike share data shown above and facet by season:

```
ggplot(bike_share_daily, aes(casual, fill=workingday, color=workingday)) +
  geom_density(alpha=0.4) +
  theme_minimal() +
  xlab("Daily Casual Bike Use Count") +
  ylab("") +
  scale_fill_discrete(name="Work Day?") +
  scale_color_discrete(name="Work Day?") +
  facet_wrap(~season, ncol=2) +
  theme(axis.ticks.y = element_blank(), axis.text.y = element_blank(), 
    legend.position="top")
```

![](images/bike_facet_densities.png)


## Pareto charts

Pareto charts are a mainstay of BI work, though the effort to create them can often outweigh the time a decision-maker spends with the result. The `qcc` library makes their creation simple. 

The bike share data doesn't lend itself to pareto chart use, so we'll use data from unplanned hospital readmissions in Hong Kong (Table 2 in [Wong et al. 2011](http://bmchealthservres.biomedcentral.com/articles/10.1186/1472-6963-11-149)).

```
library(qcc)

Readmits = c(148, 685, 16, 679, 192, 8, 1601, 37, 269, 48)
Dx = c('Septicemia', 'Cancer', 'Diabetes', 'Heart disease', 
       'Stroke', 'Aortic aneurysm', 'Pneumonia', 
       'Chronic liver disease', 'Nephritis/nephrosis',
       'Injury/poisoning') 

pareto.chart(xtabs(Readmits ~ Dx), main='Pareto Chart 
  for Unplanned Readmissions')

Pareto chart analysis for xtabs(Readmits ~ Dx)
                        Frequency Cum.Freq. Percentage Cum.Percent.
  Pneumonia                  1601      1601 43.4699973     43.47000
  Cancer                      685      2286 18.5989682     62.06897
  Heart disease               679      2965 18.4360576     80.50502
  Nephritis/nephrosis         269      3234  7.3038284     87.80885
  Stroke                      192      3426  5.2131415     93.02199
  Septicemia                  148      3574  4.0184632     97.04046
  Injury/poisoning             48      3622  1.3032854     98.34374
  Chronic liver disease        37      3659  1.0046158     99.34836
  Diabetes                     16      3675  0.4344285     99.78279
  Aortic aneurysm               8      3683  0.2172142    100.00000
```

![](images/pareto_chart.png)


## Plotting survey data

There are a variety of ways you can plot survey data, but the `likert` package makes things really simple. We can use the `mass` dataset (Math Anxiety Scale Survey) built into `likert` to demonstrate:

```
require(likert)

data(mass)

# Create a likert object
mathiness = likert(mass[2:15])

# Plot the likert object
plot(mathiness)
```

![](images/0815OS_02_13.png)

```
# Create likert object with a grouping factor
gender_math = likert(items=mass[,c(4,6,15), drop=FALSE], grouping=mass$Gender)

# Grouped plot
plot(gender_math, include.histogram=TRUE)
```

![](images/0815OS_02_14.png)


## Obtaining summary and conditional statistics

It's no accident that all the plotting work came before the summary stats in this book, although traditionally it's been the other way around. In the old days, when graphing was hard work, summary stats were used as a workaround to tell the story more simply. Now, the emphasis in both business and academia is finding the graph that tells the story, and summary stats are provided more as points of reference than the focal point. As the old saying goes, "the mean alone means nothing." 

Still, they do provide useful points of reference to help interpret graphical results. Functions to calculate summary statistics are in the `base` installation and a wide variety of packages, but I’ve found that the `psych` package probably has the most useful way to obtain them simply, particularly for groupings or when you want to condition on a particular value or threshold.

```
require(psych)
```

For numeric variables, the `describe` and `describeBy` functions do all the work for you, although you do need to specify which columns are quantitative—or which columns you want summary stats on---or it will return (non-sensically) all of them.

Basic `describe`:

```
describe(bike_share_daily[10:16])
```

![](images/describe1.png)

Using `describeBy`:

```
describeBy(bike_share_daily[10:16], bike_share_daily$holiday)
```

![](images/describe2.png)

The `table` and `prop.table` functions from the `base` package provide the simplest summary stats for categorical and ordinal variables:

```
table(bike_share_daily$holiday)

No Yes
710  21

prop.table(table(bike_share_daily$holiday))

        No        Yes
0.97127223 0.02872777
```

There are a variety of approaches and options using these functions to acquire summary statistics. For example, you can subset the quantitative stats based on factors or conditions. 

Summary stats for bike use during the winter season:

```
describeBy(bike_share_daily[14:16], bike_share_daily$season == "Winter")
```

![](images/describe3.png)

Summary stats for weather on days where there were less than/greater than 1000 casual uses:

```
describeBy(bike_share_daily[10:13], bike_share_daily$casual <= 1000)
```

![](images/describe4.png)

Summary stats for bike use on days in which the windspeed was greater than/less than average:

```
describeBy(bike_share_daily$casual, bike_share_daily$windspeed > 
  mean(bike_share_daily$windspeed))
```

![](images/describe5.png)


I> Although `describeBy` gives you more versatility, you can also use `sum` for a quick-and-dirty conditional count, e.g., 
I> 
I> ```
I> sum(bike_share_daily$cnt > 2500, na.rm=T)  
I> 
I> 587
I> 
I> sum(bike_share_daily$workingday == 'Yes')
I> 
I> 500
I> ```


You can create two- , three-, or *n*-way tables by adding variables to the table call, and use `addmargins` to get marginal sums. 

A three-way table:

```
table(bike_share_daily[c(3,6:7)]) 

, , weekday = 0

        holiday
season   No Yes
  Winter 27   0
  Spring 26   0
  Summer 26   0
  Fall   26   0

, , weekday = 1

        holiday
season   No Yes
  Winter 20   6
  Spring 24   3
  Summer 23   3
  Fall   23   3
…
```

A three-way table with marginals:

```
addmargins(table(bike_share_daily[c(3,6:7)]))

, , weekday = 0

        holiday
season    No Yes Sum
  Winter  27   0  27
  Spring  26   0  26
  Summer  26   0  26
  Fall    26   0  26
  Sum    105   0 105

, , weekday = 1

        holiday
season    No Yes Sum
  Winter  20   6  26
  Spring  24   3  27
  Summer  23   3  26
  Fall    23   3  26
  Sum     90  15 105
…
```

A table of proportions:

```
prop.table(table(bike_share_daily[c(3,9)]))

        weathersit
season             1           2           3
  Winter 0.151846785 0.090287278 0.005471956
  Spring 0.154582763 0.093023256 0.004103967
  Summer 0.186046512 0.065663475 0.005471956
  Fall   0.140902873 0.088919289 0.013679891
```


Sometimes you may want the counts to be in a data frame, either for use in plotting or because tables with more than 2 or 3 dimensions can be difficult to understand at a glance. You can quickly create any *n*-way table as a data frame using `dplyr`:

```
require(dplyr)

summarize(group_by(bike_share_daily, season, holiday, weekday), count=n())

Source: local data frame [37 x 4]
Groups: season, holiday

   season holiday weekday count
1  Winter      No       0    27
2  Winter      No       1    20
3  Winter      No       2    24
4  Winter      No       3    25
…
```


## Finding the mode or local maxima/minima

The `which.max` (or `which.min`) function can show you the set of values for a chosen maximum (or minimum) observation: 

```
# What were the other variables on the day of minimum casual use?
bike_share_daily[which.min(bike_share_daily[,14]),]
```

![](images/whichmax.png)

This trick can help you find the mode of a distribution (actually, the point of maximum density):

```
# Calculate density of casual bike use
casual_dens = data.frame(casual = density(bike_share_daily$casual)$x, 
  density_value = density(bike_share_daily$casual)$y)

# Mode / maximum density
casual_dens[which.max(casual_dens[,2]),]

     casual density_value
80 238.3353  0.0007430897
```

We can then plot that value on our ggplot from earlier in this chapter by adding it as a `geom_vline`:

```
ggplot(bike_share_daily, aes(casual)) +
  ylab("density and count") +
  xlab("Casual Use") +
  geom_histogram(aes(y=..density..), col="blue", fill="blue", alpha=0.3) +
  geom_density(col="blue", fill="blue", alpha=0.2) +
  theme_bw() +
  theme(axis.ticks.y = element_blank(), axis.text.y = element_blank()) +
  geom_vline(xintercept = dens[which.max(dens[,2]),1], color = "yellow")
```

![](images/whichmax_ggplot.png)


## Inference on summary statistics

### Confidence intervals

When we don’t have a full population to assess, we often need to determine how uncertain our estimates of the population parameters are—we need confidence intervals on those point estimates. There are `base` installation functions for most (but not all) of the ones we typically need:

{width="90%"}
| Statistic | Data type | Distribution | Function |
| --------- | --------- | ------------ | -------- |
| Median | Any | Any | `asbio::ci.median(x)` |
| Mean| Continuous | Normal, t, "normal enough" | `t.test(x)$conf.int` |
| Proportion | Percentage | Binomial | `binom.test(x, n)$conf.int` |
| Count | Count | Poisson | `poisson.test(x)$conf.int` |
| Rate | Count/*n* | Poisson | `poisson.test(x, n)$conf.int` |

```
# 95% CI for median daily casual bike use
asbio::ci.median(bike_share_daily$casual) 

95% Confidence interval for population median 
Estimate     2.5%    97.5% 
  713      668      760 

# 95% CI for mean daily casual bike use
t.test(bike_share_daily$casual)$conf.int

798.3192 898.0337

# 95% CI for proportion of casual bike use of all rentals
binom.test(sum(bike_share_daily$casual), 
  sum(bike_share_daily$cnt))$conf.int

0.1878795 0.1887244

# Subset data to winter only
bike_share_winter = filter(bike_share_daily, season == "Winter")

# 95% CI for count of winter casual bike use
poisson.test(sum(bike_share_winter$casual))$conf.int

60140.37 61106.52

# 95% CI for count of winter casual bike use per 1000 rentals
poisson.test(sum(bike_share_winter$casual), 
  sum(bike_share_winter$cnt)/1000)$conf.int

127.5923 129.6421
```


Sometimes there isn’t a `base` function for the confidence interval we need, or our data depart considerably from one of the typical distributions. For example, the confidence interval of a standard deviation is very sensitive to departures from normality, so R doesn’t have a built-in function for it. 

Non-parametric bootstrapping is the way to get these intervals; the `boot` package provides the most options but can be a pain to work with since its syntax is fairly different from most everything else in R. Still, it does what we need it to with a relative minimum of code for most cases. In this example we’ll use the `PlantGrowth` dataset that comes with R for simplicity’s sake:

```
require(boot)

# Create the boot function
sd_boot_function = function(x,i){sd(x[i])}

# Run the bootstrapping
sd_boot = boot(PlantGrowth$weight, sd_boot_function, R=10000)

# Bootstrapped sd
sd(PlantGrowth$weight)

0.7011918

# 95% CI for bootstrapped sd 
boot.ci(sd_boot, type="bca")$bca[4:5]

0.5741555 0.8772850
```

The BCa method is generally the appropriate calculation unless you have strong reason to believe that another method is best in the particular context. For example, you might want to use `type="norm"`, i.e., use the normal approximation for very small samples that you expect are “normal enough,” realizing that due to sampling error you could be estimating intervals that are wildly incorrect.   

We can also bootstrap confidence intervals for any summary statistic. For example, to get the bootstrapped 75th percentile, run the following:

```
q75_function = function(x,i){quantile(x[i], probs=0.75)}

q75_boot = boot(PlantGrowth$weight, q75_function, R=10000)

quantile(PlantGrowth$weight,0.75)

5.53 

boot.ci(q75_boot, type="bca")$bca[4:5]

5.24 5.99
```

You can get quick (gg)plots of means/medians and confidence intervals, standard deviations (`mult`, defaults to 2), or quantiles with the `stat_summary` functions:  

{width="100%"}
| Statistic | Interval Type | Function |
| --------- | ------------- | -------- |
| Mean | Bootstrapped CI | `(fun.data = mean_cl_boot, fun.args = list(conf.int=0.95), ...)` |
| Mean | Normal CI | `(fun.data = mean_cl_normal, fun.args = list(conf.int=0.95), ...)` |
| Mean | Standard deviation | `(fun.data = mean_sdl, fun.args = list(mult=2), ...)` |
| Median | Quantile | `(fun.data = median_hilow, fun.args = list(conf.int=0.5), ...)` |


The following code demonstrates all four options (and provides an example of plotting `ggplot2` objects together using the `grid.arrange` function from the `gridExtra` package):

```
require(gridExtra)

p1 = ggplot(PlantGrowth, aes(group, weight)) +
  ggtitle("Bootstrapped") +
  stat_summary(fun.data = mean_cl_boot, fun.args = list(conf.int = 0.95)) 

p2 = ggplot(PlantGrowth, aes(group, weight)) +
  ggtitle("Normal") +
  stat_summary(fun.data = mean_cl_normal, fun.args = list(conf.int = 0.95)) 

p3 = ggplot(PlantGrowth, aes(group, weight)) +
  ggtitle("2 SDs") +
  stat_summary(fun.data = mean_sdl, fun.args = list(mult = 2)) 

p4 = ggplot(PlantGrowth, aes(group, weight)) +
  ggtitle("Median+IQR") +
  stat_summary(fun.data = median_hilow, fun.args = list(conf.int = 0.5)) 

grid.arrange(p1, p2, p3, p4, nrow = 2)
```

![](images/plotcis.png)


### Tolerance intervals

While confidence or Bayesian credible intervals tend to be more often used in data science, tolerance intervals are more useful in engineering or industrial contexts: there’s a profound conceptual and practical difference between, for example, a good estimate of average commuting time to work (using a 95% *confidence* interval) and how long you can expect 95% of those trips will take (using a 95% *tolerance* interval). While the former type of value tends to be used more by statisticians and data miners, engineers and industrial analysts—and sometimes business users—find the latter more useful. 

Essentially, a good way to remember the difference is that as the sample size approaches the population size, the confidence interval will approach zero, while a tolerance interval will approach the population percentiles.

The `tolerance` package contains functions for estimating tolerance intervals for distributions typically used in most engineering or industrial processes. As an example, assume you have data on commuting time for 40 randomly chosen workdays. The distribution is oddly-shaped, so we’ll use non-parametric tolerance intervals to estimate (at a 95% confidence level) the number of minutes that 75% of commutes will take:

```
require(tolerance)

commute_time = c(68, 42, 40, 69, 46, 37, 68, 68, 69, 38, 51, 36, 50, 37, 41, 
  68, 59, 65, 67, 42, 67, 62, 48, 52, 52, 44, 65, 65, 46, 67, 62, 66, 43, 58, 
  45, 65, 60, 55, 48, 46)

commute_time_npti = nptol.int(commute_time, alpha=0.05, P=0.75, side=2)

commute_time_npti

  alpha    P 2-sided.lower 2-sided.upper
1  0.05 0.75            37            68
```

You can also obtain a graphical summary with the `plottol` function, which shows a control chart on the left and a histogram on the right:

```
plottol(commute_time_npti, commute_time, side="two", plot.type="both")
```

![](images/0815OS_02_05.png)

If you plotted both tolerance graphs (`plot.type="both"`), you'll need to return par to normal manually: 

```
par(mfrow=c(1,1))
```

The following table provides a short overview of the primary univariate tolerance interval functions; see `?tolerance` for more details. I recommend you set `alpha` (confidence level), `P` (the coverage proportion, i.e., tolerance interval), and `side` (whether to calculate upper, lower, or both bounds) manually for best results; if not chosen, `alpha` defaults to 0.05, `P` defaults to 0.99, and `side` defaults to 1 (you’ll probably want 2 in many cases). Other settings depend on the distribution used, and each distribution sometimes has several algorithms to choose from, so it’s worth exploring the documentation before diving in.

{width="90%"}
| Data type | Distribution | Function |
| --------- | ------------ | -------- |
| Percent | Binomial | `bintol.int(x, n, m, …)` |
| Count or Rate | Poisson | `poistol.int(x, n, m, side, …)` |
| Nonparametric | None | `nptol.int(x, …)` |
| Continuous | Normal, t, "normal enough" | `normtol.int(x, side, …)`|
| Continuous | Uniform | `uniftol.int(x, …)` |
| Lifetime/survival | Exponential | `exptol.int(x, type.2, …)` |
| Score | Laplace | `laptol.int(x, …)` |
| Indicies | Gamma | `gamtol.int(x, …)` |
| Reliability, extreme values | Weibull, Gumbel | `extol.int(x, dist, …)` |


## Dealing with missing data

### Visualizing missing data

The `VIM` package has some great tools for visualizing missing data patterns. As the bike share data doesn't have any missing values, we'll use a dataset derived from the [Tropical Atmosphere Ocean](http://www.pmel.noaa.gov/tao/) project for this recipe.

```
require(VIM)

data(tao)

# Rename the Sea.Surface.Temp column to make label fit on plot
colnames(tao)[4] = "Sea.Temp"

# Look at the data, esp. NAs
summary(tao)
```

![](images/tao_summary.png)

The `matrixplot` function will display the entire dataset as a set of rectangles to represent each cell. Similar values in the data will have similar grayscale values, and NAs will show up clearly in red:

```
matrixplot(tao)
```

![](images/tao_cells.png)

I> The error message you may see in the console about `gamma` was submitted as an issue to the creators of `VIM` in mid 2016. It doesn't affect the plotting in this package (as the message says, it `has no effect`), so you can just ignore it. 

To complement the detailed picture given by `matrixplot`, you can get a numeric and visual summary of missing values with the `aggr` function:

```
tao_aggr = aggr(tao)

tao_aggr

 Missings in variables:
         Variable Count
 Sea.Surface.Temp     3
         Air.Temp    81
         Humidity    93
```

![](images/tao_aggr.png)

You can compare missingness across fields with histograms, such as the distribution of missing humidity values by corresponding air temperature:

```
histMiss(tao[5:6])
```

![](images/tao_hist1.png)

You can flip the variables to see the opposite distribution:

```
histMiss(tao[c(6,5)])
```

![](images/tao_hist2.png)

You can show the same data in a scatterplot for more details:

```
marginplot(tao[5:6])
```

![](images/tao_marginplot.png)

This can be expanded to pairwise visual comparisons with the `marginmatrix` function:

```
marginmatrix(tao[4:6])
```

![](images/tao_marginmatrix.png)

I> `VIM` also has a separate GUI for interactive/on-the-fly exploration called `VIMGUI`. Even if you prefer coding, it contains a [vignette](http://cran.r-project.org/web/packages/VIMGUI/vignettes/VIM-Imputation.pdf) from which these examples are drawn, so it's worth installing to learn more about how it works and the many options you can control in the imputation process. 


### Imputation for missing values

`VIM` also performs imputation on missing values using a variety of methods. Generally, replacing values with a constant is a bad idea, as it will bias the distribution (see below for an example). Using a method that accounts for more variables or adds some noise or randomness is often a better approach. But like all things in stats, the best approach will depend on the entire analytics context, from the variables' data types to the business question itself. There is no best way, so proceed with caution whenever you need to impute data. 

The *k*-nearest neighbors approach can provide reasonable imputed values for many situations. Using the same data as above:

```
# Perform k Nearest Neighbors imputation
# Result is new dataframe with imputed values
tao_knn = kNN(tao)
```

This function appends a set of indicator variables to the data that show TRUE/FALSE for whether a particular observation was imputed. This becomes useful in the subsequent plotting of the results, e.g., for the `Air.Temp` and `Humidity` variables:

```
marginplot(tao_knn[c(5:6, 13:14)], delimiter="_imp")
```

![](images/tao_marginplot_imputed.png)

Light and dark orange show results imputed for the variable on that axis; black points show observations for which both were imputed. Blue continues to show the distribution of the non-imputed values. 

Viewing a matrix works here as well:

```
marginmatrix(tao_knn[c(4:6, 12:14)], delimiter="_imp")
```

![](images/tao_marginmatrix_imputed.png)

There are other methods in `VIM`, including a model-based approach:

```
# Perform standard Iterative Robust Model-based Imputation
tao_irmi = irmi(tao)

# Perform robust Iterative Robust Model-based Imputation
tao_irmi_robust = irmi(tao, robust=TRUE)
```

By subsetting the results of different methods, you can plot their densities to see how each approach might influence subsequent analysis. In addition to the original data and the three methods above, we'll add a mean imputation to show why a constant can be a bad choice for imputation.

```
# Create a mean-imputed air temp variable
tao$tao_airtemp_mean = ifelse(is.na(tao$Air.Temp), mean(tao$Air.Temp, 
  na.rm=TRUE), tao$Air.Temp)

# Make a data frame of each air temp result
tao_compare_airtemp = data.frame(tao=tao[,5], tao_knn=tao_knn[,5], 
  tao_irmi=tao_irmi[,5], tao_irmi_robust=tao_irmi_robust[,5], mean=tao[,9])

# Melt the various air temp results into a long data frame
require(reshape2)

tao_compare_melt = melt(tao_compare_airtemp, value.name="Air.Temp")

# Plot density histograms of each option and 
# add black dotted line to emphasize the original data
ggplot(tao_compare_melt, aes(Air.Temp, color=variable)) +
  geom_density(lwd=1.25) + 
  geom_density(data=subset(tao_compare_melt, variable=="tao"), 
    aes(Air.Temp), lty=3, lwd=1.5, color="black") +
  theme_minimal()
```

![](images/tao_compare_melt.png)

I> #### More imputation in R: other packages
I> 
I> Since imputation is a complex subject, there are of course a lot of different R packages to do it; explore the *Imputation* section of the CRAN Task View [Official Statistics & Survey Methodology](http://cran.r-project.org/web/views/OfficialStatistics.html) for a short overview of those packages and their methods.

| |
| | 
| | 
