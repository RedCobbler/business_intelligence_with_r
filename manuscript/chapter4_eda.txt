# Chapter 4: Know Thy Data

- Creating summary plots  
- Plotting univariate distributions  
- Plotting bivariate and comparative distributions  
- Plotting survey data  
- Obtaining summary and conditional statistics  
- Inference on summary statistics  
- Dealing with missing data  

Exploration and evaluation is the heart of analytics; this chapter covers a wide variety of ways to understand, display, and summarize your data. 

We'll primarily use the same bike share data we downloaded from the UCI Machine Learning Repository in Chapter 2. To make it easy, the code below downloads the particular file we'll use throughout much of this chapter and adjusts the column types and levels.

```
download.file("http://archive.ics.uci.edu/ml/machine-learning-databases/
    00275/Bike-Sharing-Dataset.zip", "Bike-Sharing-Dataset.zip")
bike_share_daily = read.table(unz("Bike-Sharing-Dataset.zip", "day.csv"),
    colClasses=c("character", "Date", "factor", "factor", "factor", "factor",
        "factor", "factor", "factor", "numeric", "numeric", "numeric", "numeric",
        "integer", "integer", "integer"), sep=",", header=TRUE)
levels(bike_share_daily$season) = c("Winter", "Spring", "Summer", "Fall")
levels(bike_share_daily$workingday) = c("No", "Yes")
levels(bike_share_daily$holiday) = c("No", "Yes")
bike_share_daily$mnth = ordered(bike_share_daily$mnth, 1:12)
levels(bike_share_daily$mnth) = c(month.abb)
levels(bike_share_daily$yr) = c(2011, 2012)
```

## Creating summary plots

If you were to poll data scientists on the "Three Most Important Steps in Any Analysis," ***Plot Thy Data*** should account for all three. Far too many business professionals rely on tables and simple summary statistics to make decisions, which are useful but can never provide the richness of information that graphs can. Further, decisions based on single statistic (usually the mean) are made much too often, usually with either good results reached by luck or accident, or—more often—you end up with (predictably) poor results. 

The recipes below are aimed at giving you a quick visual summary of the data, not to produce final plots—and so they're rough and use functions that can be difficult to modify. We'll see recipes for production plots later in this chapter and throughout the book, but we often just need to see the data before doing anything else.

### Everything at once: `ggpairs`

The `ggpairs` function in the `GGally` package has a relatively slow but visually wonderful way to plot your dataset, so load that library before continuing.

```
require(GGally)
```

GGally is built over `ggplot2` (which we'll use often in this book), and its syntax works in more or less the same manner as `qplot`. You can create the summary plot of a subset of the bike share data for an at-a-glance overview:

```
ggpairs(data=bike_share_daily, columns=c(14:15, 10, 13, 3, 7), 
  title="Daily Bike Sharing Data", axisLabels="show", color="season")
```

![ggpairs](images/Ch4_01.png)

It’s helpful to use a non-parametric correlation method if you don’t know *a priori* whether the quantitative variables are linearly related in their raw forms. Unfortunately, `ggpairs` doesn’t yet provide this functionality, so know that for continuous pairs of variables that don’t show a linear point cloud, the correlation coefficients printed in this graph will be larger than they actually are.

`GGally` is still in active development, so currently it's pretty difficult to customize the `ggpairs` plot, and it may rarely be useful to anyone other than you. Even then, it's a great way to get a sense of the data at a glance. 


### Create histograms of all numeric variables in one plot

Sometimes you just want to see histograms, but coding and laying out a bunch of them in a grid is more work than it needs to be. The `psych` package's `multi.hist` function can do this in a single line. Note that if you specify variables that aren't numeric, it will throw an error.

```
require(psych)
multi.hist(bike_share_daily[,sapply(bike_share_daily, is.numeric)])
```

![multi.hist](images/Ch4_02.png)


### A better "pairs" plot

The `psych` package also has a modification of the built-in `graphics` package's `pairs` function that's more useful out of the box. As mentioned above, until you have evidence that your paired variables are indeed linear in raw form, you should modify the default calculation option to `method="spearman"` for continuous data or `method="kendall"` for ordinal data or continuous data in which you have some known outliers:

```
pairs.panels(bike_share_daily[,sapply(bike_share_daily, is.numeric)], 
  ellipses=FALSE, pch=".", las=2, cex.axis=0.7, method="kendall")
```

![pairs.panels](images/Ch4_03.png)

### Mosaic plots: "Scatterplots" for categorical data

Once you've loaded the `vcd` package, the `pairs` function can also create a matrix of mosaic plots, either by using the table function on a dataframe, or by referencing a table object.

The dataset we've used so far in this recipe isn't a great one for mosaic plots since most of those variables are time-related, so a better view of the outcome of plotting categorical pairs can be seen with the built-in `Titanic` dataset (`?Titanic`), using the highlighting option to make the categories more clearly distinct at a glance:

```
require(vcd)
pairs(Titanic, highlighting=2)
```

![mosaic pairs](images/Ch4_04.png)


## Plotting univariate distributions

We saw the quick-and-dirty way to plot a bunch of distributions at once above; now we focus on production-ready plots using the `ggplot2` package.

```
require(ggplot2)
require(scales)
```

This package can seem daunting at first; the power of its immense flexibility and beauty comes at the cost of a bewildering variety of options. But it's well worth the effort to learn, as you'll be able to customize virtually anything you decide to plot. 

I> #### Need a `ggplot` cheat sheet?
I>
I> The `ggplot2` documentation is very thorough, but if you need a cheat-sheet to simply the possibilities down to the common needs and uses, there's a great introduction on [Computer World](http://www.computerworld.com/article/2935394/business-intelligence/my-ggplot2-cheat-sheet-search-by-task.html), and Zev Ross has a more thorough but also well done overview on his [blog](http://zevross.com/blog/2014/08/04/beautiful-plotting-in-r-a-ggplot2-cheatsheet-3/).
I>
I> You can scrape Sharon Machlis' wonderful `ggplot` cheat sheet from the above link into a local pipe-delimited text file, to format and display as works best for you.  
I> 
I> ~~~~~~~~  
I> require(XML)  
I> machlis_cw_ggplot_page = readLines("http://www.computerworld.com/article/  
I>    2935394/business-intelligence/my-ggplot2-cheat-sheet-search-by-task.html")   
I> gg_cheat_sheet = readHTMLTable(machlis_cw_ggplot_page, header=T, which=1,  
I>    stringsAsFactors=F)   
I> write.table(gg_cheat_sheet, "gg_cheat_sheet.txt", sep="|", row.names=F)   
I> ~~~~~~~~  


### Histograms and density plots

For quantitative data, business users are used to seeing histograms; quants prefer density plots. Here's how to do both, separately...  

```
ggplot(bike_share_daily, aes(casual)) +
    geom_density(col="blue", fill="blue", alpha=0.3) +
    xlab("Casual Use") +
    theme_bw()

ggplot(bike_share_daily, aes(casual)) +
    geom_histogram(col="blue", fill="blue", alpha=0.3) +
    xlab("Casual Use") +
    theme_bw()
```

![density and histogram](images/0815OS_02_06a.png)

...as well as together:

```
ggplot(bike_share_daily, aes(casual)) +
    ylab("density and count") +
    xlab("Casual Use") +
    geom_histogram(aes(y=..density..), col="blue", fill="blue", alpha=0.3) +
    geom_density(col="blue", fill="blue", alpha=0.2) +
    theme_bw() +
    theme(axis.ticks.y = element_blank(), axis.text.y = element_blank())
```

![density and histogram](images/0815OS_02_06b.png)

When asked, I’ve usually explained density plots as “more objectives histograms,” which, while not quite true, is accurate enough for business users. See `?density` for more details on the methods and useful additional references; `?stat_density` provides an overview of calculation methods used in `ggplot2`, while `?geom_density` provides information on graphical options. 

I> That pattern carries over to many `ggplot2` options—find details of the method by replacing the `x` with the stat or geom in `?stat_x` and `?geom_x` for calculation and plotting information, respectively.

### Bar and dot plots

When you have categorical data, bar or dot plots serve the same purpose:  

```
ggplot(bike_share_daily, aes(weathersit)) +
    geom_bar(col="blue", fill="blue", alpha=0.3) +
    xlab("Weather Pattern") +
    scale_x_discrete(breaks=c(1, 2, 3), labels=c("Clear", "Cloudy/Rainy", 
    "Stormy")) +
    theme_bw()
    
ggplot(bike_share_daily, aes(x=weathersit, y=..count.. )) +
    geom_point(stat = "bin", size = 3, pch=15, col="blue") +
    xlab("Weather Pattern") +
    scale_x_discrete(breaks=c(1, 2, 3), labels=c("Clear", "Cloudy/Rainy", 
    "Stormy")) +
    theme_bw()
```

![bar and dot](images/0815OS_02_07a.png)

You can also use `coord_flip()` to move from a normal bar chart to a horizontal one:
```
ggplot(bike_share_daily, aes(weathersit)) +
    geom_bar(col="blue", fill="blue", alpha=0.3) +
    xlab("") +
    scale_x_discrete(breaks=c(1, 2, 3), labels=c("Clear", "Cloudy/Rainy", 
    "Stormy")) +
    coord_flip() +
    theme_bw()
```

![horizontal bar](images/0815OS_02_07b.png)

### Plotting multiple univariate distributions with faceting

We saw the quick-and-dirty way to plot multiple histograms with multi.hist a few recipes ago; here's how to use ggplot2 to create the same thing more elegantly:

```
ggplot(bike_share_daily, aes(casual, fill=season)) +
    geom_histogram(aes(y = ..density..), alpha=0.2, color="gray50") +
    geom_density(alpha=0.5, size=0.5) +
    facet_wrap(~season) +
    theme_light() +
    xlab("Daily Bike Use Count") +
    ylab("") +
    theme(legend.position="none") +
    theme(axis.ticks.y = element_blank(), axis.text.y = element_blank(),
          axis.title = element_text(size=9, face=2, color="gray30"),
          axis.title.x = element_text(vjust=-0.5))
```

![ggplot multi.hist](images/0815OS_02_08.png)

The same can be done for categorical distributions, too, of course:

```
ggplot(bike_share_daily, aes(weathersit, fill=season)) +
    geom_bar(alpha=0.5) +
    xlab("") +
    ylab("Number of Days") +
    scale_x_discrete(breaks=c(1, 2, 3), labels=c("Clear", "Cloudy/Rainy", 
    "Stormy")) +
    coord_flip() +
    facet_wrap(~season, ncol=1) +
    theme_light()
```

![ggplot multi.bar](images/0815OS_02_09.png)

A> #### What about pie charts?
A> 
A> With apologies to Hunter S. Thompson, if you use pie charts for anything outside of GraphJam, you deserve whatever happens to you.


## Plotting bivariate and comparative distributions

Most of the time, we want to understand how one thing compares to something else. There’s no better way to do this by plotting each group’s distribution against the other’s.

We continue using the bike share data, along with `ggplot2`, `vcd`, and `beanplot`. 

```
require(ggplot2)
require(ggExtra)
require(scales)
require(vcd)
require(beanplot)
```

### Double density plots

For quantitative data, plotting the two distributions over each other provides a clear, direct comparison:

```
ggplot(bike_share_daily, aes(casual, fill=workingday, color=workingday)) +
    geom_density(alpha=0.4) +
    theme_minimal() +
    xlab("Daily Casual Bike Use Count") +
    ylab("") +
    scale_fill_discrete(name="Work Day?") +
    scale_color_discrete(name="Work Day?") +
    theme(axis.ticks.y = element_blank(), axis.text.y = element_blank(), 
    legend.position="top")
```

![double density plot](images/bike_double_density.png)

### Boxplots

Boxplots can be useful for comparing more than a couple of distributions, and can do so using more than one factor:

```
ggplot(bike_share_daily, aes(mnth, casual, fill=workingday)) +
    xlab("Month") +
    ylab("Daily Casual Bike Use Count") +
    geom_boxplot() +
    theme_minimal() +
    scale_fill_discrete(name="Work Day?") +
    scale_color_discrete(name="Work Day?") +
    theme(legend.position="bottom")
```

![boxplot](images/bike_boxplot.png)

### Beanplots

Violin plots have been considered a more useful alternative to boxplots, as you can see the shape of the distribution, not just its major quantiles. Unfortunately, violin plots can sometimes look like a series of genetalia, which pretty much precludes its use in business settings. 

Fortunately, bean plots allow you to create "half" violin plots, or more accurately, comparitive density histograms. The `side` option allows you to only plot the density on one side of a line; using `side="first"` has the height of each curve going to the left side of the plot. By playing with the `what` options, you can determine what pieces of the bean plot are shown. One of those piece is a reference line, which defaults to the mean; `overallline="median") allows you to use that statistic instead. 

```
beanplot(casual ~ mnth, data = bike_share_daily, side="first", 
    overallline="median", what=c(1,1,1,0), col=c("gray70", 
    "transparent", "transparent", "blue"), xlab = "Month", 
    ylab = "Daily Casual Bike Use Count")
```

If you prefer a more traditional look, with the density curves pointing up, you can use `horizontal=TRUE`, change `side` to `"second"`, and flip your axis titles:

![beanplot](images/beanplot1.png)  

```
beanplot(casual ~ mnth, data = bike_share_daily, side="second", 
    overallline="median", what=c(1,1,1,0), col=c("gray70", 
    "transparent", "transparent", "blue"), ylab = "Month", 
    xlab = "Daily Casual Bike Use Count", horizontal=TRUE)
```

![bean density](images/beanplot2.png)  

Like the boxplot example above, you can also add a comparison for groups within a category, and each group's density is plotted base-to-base. So while you don't have the exact symmetry of violin plots, the result can provide much the same visual impression, so it's probably best to stick with the use of one side of the "bean."


### Scatterplots and marginal distributions

The `ggExtra` package provides a simple way to plot scatterplots with marginal distributions with the `ggMarginal` function. It defaults to density curves, but the histogram option presents a clearer picture. First, build the scatterplot and assign it to an object.

```
bike_air_temp = ggplot(bike_share_daily, aes(x=atemp, y=casual)) +
    xlab("Daily Mean Normalized Air Temperature") +
    ylab("Number of Total Casual Bike Uses") +
    geom_point(col="gray50") +
    theme_bw()
```

Then, use `ggMarginal` to create the final product:

```
bike_air_temp_mh = ggMarginal(bike_air_temp, type="histogram")
```

![marginal histograms](images/scatter_marginal_hist.png)

Sometimes density plots can be useful, or perhaps you want to keep it simple and show a boxplot. Just change the `type` option:

![marginal density](images/scatter_marginal_density.png)  

![marginal boxplot](images/scatter_marginal_boxplot.png)  


### Mosaic plots

The `vcd` package has a nice implementation of mosaic plots for categorical data called, as you might expect, `mosaic`:

```
mosaic(~ weathersit + season, data=bike_share_daily, shade=T, 
  legend=F, labeling_args = list(set_varnames = c(season = "Season", 
  weathersit = "Primary Weather Pattern"), set_labels = list(weathersit = 
  c("Clear", "Cloudy/Rainy",  "Stormy"))))
```

![mosaic](images/0815OS_02_11.png)

A> #### Um. What's a mosaic plot?
A> 
A> If you're not already familiar with them, think of mosaic plots as contingency tables made with shapes—the larger the shape, the higher the cell count.
A> 
A> For comparison, here's the table that the mosaic plot represents:  
A> 
A> | | Winter | Spring | Summer | Fall |
A> |:-|------:|------:|------:|----:|
A> | Clear | 111 | 113 | 136 | 103 |
A> | Cloudy/Rainy | 66 | 68 | 48 | 65 |
A> | Stormy | 4 | 3 | 4 | 10 |
A> 
A> Sure, a table can be more compact, but the mosaic gives you the pattern at a glance; each is an option depending on what you need to get across.


### Multiple bivariate comparisons with faceting

As with univariate plots, we can facet on another variable for comparison’s sake; for example, if we take the bike share data shown above and facet by season:

```
ggplot(bike_share_daily, aes(casual, fill=workingday, color=workingday)) +
    geom_density(alpha=0.4) +
    theme_minimal() +
    xlab("Daily Casual Bike Use Count") +
    ylab("") +
    scale_fill_discrete(name="Work Day?") +
    scale_color_discrete(name="Work Day?") +
    facet_wrap(~season, ncol=2) +
    theme(axis.ticks.y = element_blank(), axis.text.y = element_blank(), 
    legend.position="top")
```

![faceted bikeshare densities](images/bike_facet_densities.png)


## Plotting survey data

There are a variety of ways you can plot survey data, but the `likert` package makes things really simple. We can use the `mass` dataset (Math Anxiety Scale Survey) built into `likert` to demonstrate:

```
require(likert)
mathiness = likert(mass[2:15])
plot(mathiness)
```

![basic likert plot](images/0815OS_02_13.png)

E> There's a known bug in `likert` where you have to load the `reshape` package manually (not reshape2) to make the `grouping` option work.

```
require(reshape)
gender_math = likert(items=mass[,c(4,6,15), drop=FALSE], grouping=mass$Gender)
plot(gender_math, include.histogram=TRUE)
```

![grouped likert plot](images/0815OS_02_14.png)

Because the `reshape` package masks functions in `reshape2` and `dplyr`, you should detach `likert` and `reshape` when you're done plotting.

```
detach("package:likert", unload=TRUE)
detach("package:reshape", unload=TRUE)
```


## Obtaining summary and conditional statistics

Once you've got plots, it's good to have the summary statistics to go along with the variables you’ve plotted. Functions to calculate summary statistics are in the `base` installation and a wide variety of packages, but I’ve found that the `psych` package probably has the most useful way to obtain them simply, particularly for groupings or when you want to condition on a particular value or threshold.

```
require(psych)
```

For numeric variables, the `describe` and `describeBy` functions do all the work for you, although you do need to specify which columns are quantitative—or which columns you want summary stats on—or it will return (non-sensically) all of them.

Basic `describe`:

```
describe(bike_share_daily[10:16])
```

![describe1](images/describe1.png)

Using `describeBy`:

```
describeBy(bike_share_daily[10:16], bike_share_daily$holiday)
```

![describe2](images/describe2.png)

The `table` and `prop.table` functions from the `base` package provide the simplest summary stats for categorical and ordinal variables:

```
table(bike_share_daily$holiday)
No Yes
710  21

prop.table(table(bike_share_daily$holiday))

        No        Yes
0.97127223 0.02872777
```

There are a variety of approaches and options using these functions. For example, you can subset the quantitative stats based on factors or conditions. 

Summary stats for bike use during the winter season:

```
describeBy(bike_share_daily[14:16], bike_share_daily$season == "Winter")
```

![describe1](images/describe3.png)

Summary stats for weather on days where there were less than/greater than 1000 casual uses:
```
describeBy(bike_share_daily[10:13], bike_share_daily$casual <= 1000)
```

![describe1](images/describe4.png)

Summary stats for bike use on days in which the windspeed was greater than/less than average:

```
describeBy(bike_share_daily$casual, bike_share_daily$windspeed > 
  mean(bike_share_daily$windspeed))
```

![describe1](images/describe5.png)


I> Although `describeBy` gives you more versatility, you can use `sum` for a
I> quick-and-dirty conditional count, e.g., 
I> `sum(bike_share_daily$cnt > , na.rm=T` or
I> `sum(bike_share_daily$workingday == 'Yes')`. 


You can create two- , three-, or *n*-way tables by adding variables to the table call, and use `addmargins` to get marginal sums. 

A three-way table:

```
table(bike_share_daily[c(3,6:7)]) 
, , weekday = 0

        holiday
season   No Yes
  Winter 27   0
  Spring 26   0
  Summer 26   0
  Fall   26   0

, , weekday = 1

        holiday
season   No Yes
  Winter 20   6
  Spring 24   3
  Summer 23   3
  Fall   23   3
…
```

A three-way table with marginals:

```
addmargins(table(bike_share_daily[c(3,6:7)]))
, , weekday = 0

        holiday
season    No Yes Sum
  Winter  27   0  27
  Spring  26   0  26
  Summer  26   0  26
  Fall    26   0  26
  Sum    105   0 105

, , weekday = 1

        holiday
season    No Yes Sum
  Winter  20   6  26
  Spring  24   3  27
  Summer  23   3  26
  Fall    23   3  26
  Sum     90  15 105
…
```

A table of proportions:

```
prop.table(table(bike_share_daily[c(3,9)]))

        weathersit
season             1           2           3
  Winter 0.151846785 0.090287278 0.005471956
  Spring 0.154582763 0.093023256 0.004103967
  Summer 0.186046512 0.065663475 0.005471956
  Fall   0.140902873 0.088919289 0.013679891
```


Sometimes you may want the counts to be in a data frame, either for use in plotting or because tables with more than 2 or 3 dimensions can be difficult to understand at a glance. You can quickly create any *n*-way table as a data frame using `dplyr`:

```
require(dplyr)
summarise(group_by(bike_share_daily, season, holiday, weekday), count=n())
Source: local data frame [37 x 4]
Groups: season, holiday

   season holiday weekday count
1  Winter      No       0    27
2  Winter      No       1    20
3  Winter      No       2    24
4  Winter      No       3    25
…
```


A> #### Finding local maxima/minima
A>
A> The `which.max` (or `which.min`) function can show you the set of values for a chosen maximum (or minimum) observation: 
A> 
A> ```
A> bike_share_daily[which.min(bike_share_daily[,14]),]
A> ```
A> 
A> ![](images/whichmax.png)



## Inference on summary statistics

### Confidence intervals

When we don’t have a full population to assess, we often need to determine how uncertain our estimates of the population parameters are—we need confidence intervals on those point estimates. There are `base` installation functions for most of the ones we typically need:

{width="90%"}
| Statistic | Data type | Distribution | Function |
| --------- | --------- | ------------ | -------- |
| Median | Any | Any | `asbio::ci.median(bike_share_daily$cnt)` |
| Mean| Continuous | Normal, t, “normal enough” | `t.test(x)$conf.int` |
| Proportion | Percentage | Binomial | `binom.test(x, n)$conf.int` |
| Count | Count | Poisson | `poisson.test(x)$conf.int` |
| Rate | Count/*n* | Poisson | `poisson.test(x, n)$conf.int` |

Sometimes there isn’t a `base` function for the confidence interval we need, or our data depart considerably from one of the typical distributions. For example, the confidence interval of a standard deviation is very sensitive to departures from normality, so R doesn’t have a built-in function for it. Non-parametric bootstrapping is the way to get these intervals; the `boot` package provides the most options but can be a pain to work with since its syntax is fairly different from most everything else in R. Still, it does what we need it to with a relative minimum of code for most cases. In this example we’ll use the `PlantGrowth` dataset that comes with R for simplicity’s sake:

```
require(boot)
sd_boot_function = function(x,i){sd(x[i])}
sd_boot = boot(PlantGrowth$weight, sd_boot_function, R=10000)
sd(PlantGrowth$weight)
boot.ci(sd_boot, type="bca")$bca[4:5]

0.7011918
0.5741555 0.8772850
```

The BCa method is generally the appropriate calculation unless you have strong reason to believe that another method is best in the particular context. For example, you might want to use `type="norm"`, i.e., use the normal approximation for very small samples that you expect are “normal enough,” realizing that due to sampling error you could be estimating intervals that are wildly incorrect.   

We can also bootstrap confidence intervals for any summary statistic. For example, to get the bootstrapped 75th percentile, run the following:

```
q75_function = function(x,i){quantile(x[i], probs=0.75)}
q75_boot = boot(PlantGrowth$weight, q75_function, R=10000)
quantile(PlantGrowth$weight,0.75)
boot.ci(q75_boot, type="bca")$bca[4:5]

5.53 
5.24 5.99
```

You can get quick (gg)plots of means/medians and confidence intervals, standard deviations (defaults to 2), or quantiles with the `stat_summary` and one of the following options:  

| Statistic | Interval Type | Function |
| --------- | ------------- | -------- |
| Mean | Bootstrapped CI | `mean_cl_boot(x, conf.int=0.95, ...)` |
| Mean | Normal CI | `mean_cl_mean_sdl(x, conf.int=0.95, ...)` |
| Mean | Standard deviation | `mean_sdl(x, mult=2, ...)` |
| Median | Quantile | median_hilow(x, conf.int=0.95, ...)


The following code demonstrates all three options:

```
require(ggplot2)
require(gridExtra)

p1 = ggplot(PlantGrowth, aes(group, weight)) +
    ggtitle("Bootstrapped") +
    stat_summary(fun.data = mean_cl_boot, conf.int = 0.95) 

p2 = ggplot(PlantGrowth, aes(group, weight)) +
    ggtitle("Normal") +
    stat_summary(fun.data = mean_cl_normal, conf.int = 0.95) 

p3 = ggplot(PlantGrowth, aes(group, weight)) +
    ggtitle("2 SDs") +
    stat_summary(fun.data = mean_sdl, mult=2) 

p4 = ggplot(PlantGrowth, aes(group, weight)) +
    ggtitle("Median+IQR") +
    stat_summary(fun.data = median_hilow, conf.int = 0.5) 

grid.arrange(p1, p2, p3, p4, nrow=2)
```

![conf interval plots](images/plotcis.png)




### Tolerance intervals

While confidence or Bayesian credible intervals tend to be more often used in data science, tolerance intervals are more useful in engineering or industrial contexts: there’s a profound conceptual and practical difference between, for example, a good estimate of average commuting time to work (using a 95% *confidence* interval) and how long you can expect 95% of those trips will take (using a 95% *tolerance* interval). While the former type of value tends to be used more by statisticians and data miners, engineers and industrial analysts—and sometimes business users—find the latter more useful. 

Essentially, a good way to remember the difference is that as the sample size approaches the population size, the confidence interval will approach zero, while a tolerance interval will approach the population percentiles.

The `tolerance` package contains functions for estimating tolerance intervals for distributions typically used in most engineering or industrial processes. As an example, assume you have data on commuting time for 40 randomly chosen workdays. The distribution is oddly-shaped, so we’ll use non-parametric tolerance intervals to estimate (at a 95% confidence level) the number of minutes that 75% of commutes will take:

```
require(tolerance)
commute_time = c(68, 42, 40, 69, 46, 37, 68, 68, 69, 38, 51, 36, 50, 37, 41, 
  68, 59, 65, 67, 42, 67, 62, 48, 52, 52, 44, 65, 65, 46, 67, 62, 66, 43, 58, 
  45, 65, 60, 55, 48, 46)
commute_time_npti = nptol.int(commute_time, alpha=0.05, P=0.75, side=2)
commute_time_npti

  alpha    P 2-sided.lower 2-sided.upper
1  0.05 0.75            37            68
```

You can also obtain a graphical summary with the plottout function, which shows a control chart on the left and a histogram on the right:

```
plottol(commute_time_npti, commute_time, side="two", plot.type="both")
```

![tolerance interval plot](images/0815OS_02_05.png)


If you plotted both tolerance graphs (plot.type="both"), you'll need to return par to normal manually: 

```
par(mfrow=c(1,1))
```

The following table provides a short overview of the primary univariate tolerance interval functions; see `?tolerance` for more details. I recommend you set `alpha` (confidence level), `P` (the coverage proportion, i.e., tolerance interval), and `side` (whether to calculate upper, lower, or both bounds) manually for best results; if not chosen, `alpha` defaults to 0.05, `P` defaults to 0.99, and `side` defaults to 1 (you’ll probably want 2 in many cases). Other settings depend on the distribution used, and each distribution sometimes has several algorithms to choose from, so it’s worth exploring the documentation before diving in.

{width="90%"}
| Data type | Distribution | Function |
| --------- | ------------ | -------- |
| Percent | Binomial | `bintol.int(x, n, m, …)` |
| Count or Rate | Poisson | `poistol.int(x, n, m, side, …)` |
| Nonparametric | None | `nptol.int(x, …)` |
| Continuous | Normal, t, “normal enough” | `normtol.int(x, side, …)`|
| Continuous | Uniform | `uniftol.int(x, …)` |
| Lifetime/survival | Exponential | `exptol.int(x, type.2, …)` |
| Score | Laplace | `laptol.int(x, …)` |
| Indicies | Gamma | `gamtol.int(x, …)` |
| Reliability, extreme values | Weibull, Gumbel | `extol.int(x, dist, …)` |


## Dealing with missing data

### Visualizing missing data

The `VIM` package has some great tools for visualizing missing data patterns. As the bike share data doesn't have any missing values, we'll use a dataset derived from the [Tropical Atmosphere Ocean](http://www.pmel.noaa.gov/tao/) project for this recipe.

```
require(VIM)
data(tao)

# Rename the Sea.Surface.Temp column to make label fit on plot
colnames(tao)[4] = "Sea.Temp"

# Look at the data, esp. NAs
summary(tao)
```

![tao summary](images/tao_summary.png)

The `matrixplot` function will display the entire dataset as a set of rectangles to represent each cell. Similar values in the data will have similar grayscale values, and NAs will show up clearly in red:

```
matrixplot(tao)
```

![matrixplot image](images/tao_cells.png)

To complement the detailed picture given by `matrixplot`, you can get a numeric and visual summary of missing values with the `aggr` function:

```
tao_aggr = aggr(tao)
tao_aggr

 Missings in variables:
         Variable Count
 Sea.Surface.Temp     3
         Air.Temp    81
         Humidity    93
```

![aggr image](images/tao_aggr.png)

You can compare missingness across fields with histograms, such as the distribution of missing humidity values by corresponding air temperature:

```
histMiss(tao[5:6])
```

![histMiss image](images/tao_hist1.png)

You can flip the variables to see the opposite distribution:

```
histMiss(tao[c(6,5)])
```

![histMiss reversed image](images/tao_hist2.png)

You can show the same data in a scatterplot for more details:

```
marginplot(tao[5:6])
```

![marginplot image](images/tao_marginplot.png)

This can be expanded to pairwise visual comparisons with the `marginmatrix` function:

```
marginmatrix(tao[4:6])
```

![marginmatrix image](images/tao_marginmatrix.png)

I> `VIM` also has a separate GUI for interactive/on-the-fly exploration called `VIMGUI`. Even if you prefer coding, it contains a [vignette](http://cran.r-project.org/web/packages/VIMGUI/vignettes/VIM-Imputation.pdf) from which these examples are drawn, so it's worth installing to learn more about how it works and the many options you can control in the imputation process. 


### Imputation for missing values

`VIM` also performs imputation on missing values using a variety of methods. Generally, replacing values with a constant is a bad idea, as it will bias the distribution (see below for an example). Using a method that accounts for more variables or adds some noise or randomness is often a better approach. But like all things in stats, the best approach will depend on the entire analytics context, from the variables' data types to the business question itself. There is no best way, so proceed with caution whenever you need to impute data. 

The *k*-nearest neighbors approach can provide reasonable imputed values for many situations. Using the same data as above:

```
# Perform k Nearest Neighbors imputation
# Result is new dataframe with imputed values
tao_knn = kNN(tao)
```

This function appends a set of indicator variables to the data that show TRUE/FALSE for whether a particular observation was imputed. This becomes useful in the subsequent plotting of the results, e.g., for the `Air.Temp` and `Humidity` variables:

```
marginplot(tao_knn[c(5:6, 13:14)], delimiter="_imp")
```

![marginplot imputed](images/tao_marginplot_imputed.png)

Light and dark orange show results imputed for the variable on that axis; black points show observations for which both were imputed. Blue continues to show the distribution of the non-imputed values. 

Viewing a matrix works here as well:

```
marginmatrix(tao_knn[c(4:6, 12:14)], delimiter="_imp")
```

![marginmatrix imputed](images/tao_marginmatrix_imputed.png)

There are other methods in `VIM`, including a model-based approach:

```
# Perform standard Iterative Robust Model-based Imputation
tao_irmi = irmi(tao)

# Perform robust Iterative Robust Model-based Imputation
tao_irmi_robust = irmi(tao, robust=TRUE)
```

By subsetting the results of different methods, you can plot their densities to see how each approach might influence subsequent analysis. In addition to the original data and the three methods above, we'll add a mean imputation to show why a constant can be a bad choice for imputation.

```
# Create a mean-imputed air temp variable
tao$tao_airtemp_mean = ifelse(is.na(tao$Air.Temp), mean(tao$Air.Temp, 
    na.rm=TRUE), tao$Air.Temp)

# Make a data frame of each air temp result
tao_compare_airtemp = data.frame(tao=tao[,5], tao_knn=tao_knn[,5], 
    tao_irmi=tao_irmi[,5], tao_irmi_robust=tao_irmi_robust[,5], mean=tao[,9])

# Melt the various air temp results into a long data frame
require(reshape2)
tao_compare_melt = melt(tao_compare_airtemp, value.name="Air.Temp")

# Plot density histograms of each option and 
# add black dotted line to emphasize the original data
ggplot(tao_compare_melt, aes(Air.Temp, color=variable)) +
    geom_density(lwd=1.25) + 
    geom_density(data=subset(tao_compare_melt, variable=="tao"), 
        aes(Air.Temp), lty=3, lwd=1.5, color="black") +
    theme_minimal()
```

![imputed results](images/tao_compare_melt.png)

I> #### More imputation in R: other packages
I> 
I> Since imputation is a complex subject, there are of course a lot of different R packages to do it; explore the *Imputation* section of the CRAN Task View [Official Statistics & Survey Methodology](http://cran.r-project.org/web/views/OfficialStatistics.html) for a short overview of those packages and their methods.
