# Chapter 9: Discovering Patterns in Text

## Introduction

In this chapter, you will learn BLAH AND BLAH AND BLAH.

## Working with Strings

The string functions in R will get the job done, but when compared to other modern programming languages and natural language processing packages they are neither particularly elegant nor exceptionally powerful. The real value of using R comes into play when you export data about your texts into a quantitative form that can be handled using the analytical methods most easily found in R.

In this chapter we will make heavy use of the `stringr` package, which provides more consistent, readable naming conventions and a bit of extra functionality on top of the base R string functions. For a more comprehensive introduction to working with strings in both base R and `stringr`, we recommend [Handling_and_Processing_Strings_in_R](http://gastonsanchez.com/Handling_and_Processing_Strings_in_R.pdf), a free ebook by Gaston Sanchez.


### Handling Backslashes and Other Special Characters

Before we get to the serious work of finding patterns in text, let's have a quick refresher on escape sequences in R strings. 

R uses the backslash (`\`) to indicate the beginning of a special character sequence. This is used to enter non-printing ASCII characters such as a newline (`\n`) or tab (`\t`). If you want your string to contain the actual backslash character, you need to add a second backslash so that R knows you're serious.

```
# How to use escape characters

# In this example, R is trying to interpret each backslash sequence as 
# a special character - but complains when it doesn't recognize \d
my_string = 'C:\dos\run'
Error: '\d' is an unrecognized escape in character string starting "'C:\d"

# This won't give an error message, but the '\r' will be interpreted as
# a carriage return.
my_other_string = "dos\run"

# Printing the string from the command line won't necessarily reveal the problem,
# but try opening test.txt in a text editor and see if the result is what you expect.
my_other_string
[1] "dos\run"
write(my_other_string, 'test.txt')

# To represent a backslash, you need to input two backslashes.
better_string = 'C:\\dos\\run'

# The extra backslash will still be there when you look at the string from 
# the command line, but it will output correctly when writing to files.
better_string
[1] "C:\\dos\\run"
write(better_string, 'test2.txt')

# Last but not least, remember you can use a backslash to escape quotes 
# and other problem characters:
string_on_finger = 'I won\'t forget' 
```


### Defining Patterns with Regular Expressions

Since the dawn of time (circa 1968), humankind has used a special syntax to define patterns that occur in character sequences. If you have ever wanted to find words that have been accidentally doubled in a text, or extract a certain part of a directory path, regular expressions are just the trick.

If you are familiar with regular expressions from another programming environment, great---just watch your backslashes and you'll be fine in R. If not, we'll cover a couple of simple examples here, but we are not going to teach you how to unlock the full power of regular expressions in this book. We recommend spending some quality time with:

- A good reference sheet. There are many, and they all get the job done so you should just pick one that looks nice in the print or screen format you use most. Googling something like "regex cheat sheet" or "regex quick reference" will lead you to them.  
- An interactive tool that will test your regular expressions against a piece of text. We like the one at [rubular.com](rubular.com) a lot, because it fits both a testing widget and a quick reference onto a laptop screen. There are many others.  
- The game of [Regex Golf](https://regex.alf.nu/)


### Finding Numbers in Text

In this chapter we will work with a set of public-domain cookbooks that we downloaded as plain text files from Project Gutenberg. One of our first tasks will be to attempt to identify the publication year of each cookbook. To do this, we'll look for pieces of text that might represent a year. We know that the possible publication years for these books are all between 1390 (the most commonly given date for *The Forme of Cury*) and 1923 (the latest date at which published works are automatically considered public domain in the United States). We can also go ahead and ignore years that are written in words. 

For us, then, a "year" is something that starts with the digit 1, and contains 3 more digits. 

As a regular expression, our definition of a "year" looks like this:

`1\d{3}`

That's the digit 1, followed by the special character sequence `\d`, which means "any digit". The number in braces indicates that the previous character (any digit, in this case) should be repeated three times.

Regular expressions are entered as strings in R, which means we need to add an additional backslash to get everything escaped just right:

`regex_year = "1\\d3"`

***[[PUT IN A TABLE OF REGEX SPECIAL CHARACTERS / METACHARACTERS - 1 column for "vanilla" regex, another column with double backslashes for input into R.]]***

| TABLE | GOES | HERE |
| ----- | ---- | ---- |
|       |      |      |

If we had modern cookbooks in the mix, we could specify that the year must begin with either 1 or 2:  

`regex_year = "[1|2]\\d3"`

Everything inside the square brackets defines a character class; the pipe character acts as the OR in "must begin with 1 or 2."

***[[PUT IN A TABLE OF CHARACTER CLASS METACHARACTERS]]***

| TABLE | GOES | HERE |
| ----- | ---- | ---- |
|       |      |      |

Finally, if we want to match a literal backslash character (splitting Windows directory paths?) we need to use a double backslash in the regular expression, which means four backslashes in R:

`regex_windows_drive = "[A-Z]:\\\\"`



## Handling English As She Is Spoke

[But why, you and another book seller, you does not to imprint some  
    good wooks?  
  There is a reason for that, it is that you cannot to sell its. The  
    actual-liking of the public is depraved they does not read who  
    for to amuse one's self ant but to instruct one's.  
  But the letter's men who cultivate the arts and the sciences they  
    can't to pass without the books.  
  A little learneds are happies enough for to may to satisfy their   
    fancies on the literature.](http://publicdomainreview.org/collections/english-as-she-is-spoke-1884/)

### Stemming

*Stemming* is the process of converting inflected forms of a word to a common root form, for example, reducing "winning" to "win". This makes it easier to compare uses of the same verb in different tenses, or the same noun in both singular and plural forms.

English is messy, so we don't recommend writing your own stemming algorithm. The SnowballC package implements the popular Porter algorithm as well as methods for 15 other languages.

```
> require("SnowballC")

# The Porter stemmer strips off most common suffixes
wordStem( c("nounify", "verbing", "adjectified", "adverbially", "cromulent"))
[1] "nounifi"   "verb"      "adjectifi" "adverbi" "cromul"

# But it's not always obvious when it will leave them alone
wordStem( c("recorder", "winner", "baker", "nounifier"))
[1] "record"  "winner"  "baker" "nounifi"

# And sometimes it takes things a little too far
wordStem( c("is") )
[1] "i"

# It can also be thrown off by punctuation and capitalization
wordStem( c("winning", "Winning", "WINNING", "winning;"))
[1] "win" "Win"  "WINNING" "winning;"
```

The Porter stemmer has some quirks, but for most applications it provides a good balance between fast computability, reducing the total number of word forms in a text, and preserving information. If you want to try a different algorithm, the `RWeka` package uses the Lovins stemmer, which relies more heavily on a lookup dictionary than the Porter algorithm does.

```
require("RWeka")

LovinsStemmer(  c("this", "is", "a", "perfectly", "cromulent", "tokenized", "sentence"))
[1] "th"      "is"      "a"       "perfect" "croml"   "token"   "sent"  
```


### Finding English Word Relationships

WordNet &reg; is a lexical database of English---similar to a thesaurus, but with richer semantic structure. In addition to synonyms and antonyms it provides categorical relationships (e.g., an elm is a kind of tree, a leaf is a part of a tree).

Many programming languages have packages that interface with WordNet data. If you are going to do serious work with it, we recommend switching to something like Java or Python's Natural Language Toolkit. The `wordnet` package in R provides basic functionality, with unfortunately basic documentation, and the overall hassle-to-reward ratio is unsatisfying.

The `wordnet` package relies on an external instance of the WordNet database, which must be installed separately. Package managers for Linux or OS X may install it for you, or you can download it directly from the [WordNet website](http://wordnet.princeton.edu). 

When the `wordnet` package is loaded into R, it will search common installation locations for the dictionary. If it can't find it, it will complain. If this happens, you can point it to the right place using `setDict()`:

````
library("wordnet")
# here is where my package manager put my wordnet dictionary on OSx:
setDict("/usr/local/Cellar/wordnet/3.1/dict")
```

In this example we'll use WordNet to look at the categories of ingredients used in each cookbook. 

***Example: Count herbs and spices used in each cookbook (by year?)***

The Wordnik API provides similar functionality to WordNet. If you're only doing a handful of lookups it may be easier to just use the API than to install your own copy of WordNet.


## Evaluating Emotions in Text





## Creating a Document Corpus with `tm`



```
# Turn a directory full of plain text files into a tm corpus
# that is ready for bag-of-words analysis

library("tm")
library("SnowballC")

dir = "/Users/maria/Projects/RDSCB/datasets/cookbooks"
cookbooks = Corpus(DirSource(dir, encoding="UTF-8"), readerControl=list(language="en"))

# Create a version of the corpus for bag-of-words analysis
# Make everything lowercase, remove punctuation & numbers
cb = tm_map(cookbooks, content_transformer(tolower))

# Remove common words - default English stopwords 
# and some Project Gutenberg license terms
custom_stopwords = c("gutenberg", "project", "gutenbergtm", "ebook", "electronic")
all_stopwords = c(stopwords("english"), custom_stopwords)
cb = tm_map(cb, removeWords, all_stopwords)

# Don't remove punctuation until after filtering out stopwords
# so that entries like "don't" and "you'll" are treated correctly
cb = tm_map(cb, removePunctuation)
cb = tm_map(cb, removeNumbers)

# Stem all documents in the corpus using the default stemmer
cb = tm_map(cb, stemDocument)
```


The content of the text is stored as a vector, with each element representing one line of text from the original file.




### Working with Metadata

***METADATA IS GREAT. HERE ARE SOME COMMON METADATA FORMATS.***


You can create any meta tag you like:

```
meta(cookbooks[[2]], "sparkle princess message") = "I like ponies!"
```

Each document in a corpus can have its own unique set of meta tags. Assigning a meta tag to one document will not affect the rest of the corpus.

The documents in the cookbooks directory come from *[Project Gutenberg](http://www.gutenberg.org/), which includes a header and footer on each file. In most (but not all) of the documents the header contains useful metadata such as the book's title, author, etc.


## Creating Term and Document Matrices

The workhorse data format for most quantitative text analysis is ***SOMETHING IMPORTANT***

The `tm` package provides two built-in functions, `TermDocumentMatrix` and `DocumentTermMatrix`. `TermDocumentMatrix` will create ***SOMETHING IMPORTANT***

```
# Creating a term-document matrix
# from our existing "cookbooks" corpus (see Section XXXXX)

# SnowballC contains the stemming function & is loaded separately from tm
library("SnowballC")

# Specifications in the controls list will be processed in the order they are listed.
# It's important to remove the stopwords before stripping punctuation,
# so that words like "don't" and "you'll" will be taken out.

# We picked some magic numbers for the bounds and wordLengths arguments.
# The corpus has 115 documents, so throwing out anything that appears in more than
# 100 seemed like a good way to get rid of Project Gutenberg license terms, etc.
tdm_controls = list(
    bounds = list(c(2,100)),
    wordLengths = c(3, 30),
    stopwords=TRUE,
    removePunctuation=TRUE,
    removeNumbers = TRUE,
    stemming=TRUE
)

tdm = TermDocumentMatrix(cb, control=tdm_controls)

tdm = removeSparseTerms(tdm, 0.95)

findFreqTerms(tdm, 7000)
 [1] "add"        "bake"       "beat"       "boil"       "bread"      "brown"     
 [7] "butter"     "cake"       "can"        "chop"       "cold"       "cook"      
[13] "cover"      "cream"      "cup"        "cut"        "dish"       "dri"       
[19] "egg"        "fat"        "fine"       "fire"       "fish"       "flour"     
[25] "food"       "four"       "fri"        "fruit"      "good"       "half"      
[31] "hot"        "hour"       "juic"       "larg"       "lemon"      "let"       
[37] "littl"      "made"       "make"       "may"        "meat"       "milk"      
[43] "minut"      "mix"        "must"       "one"        "onion"      "ounc"      
[49] "oven"       "pan"        "pepper"     "piec"       "pint"       "place"     
[55] "potato"     "pound"      "pour"       "project"    "put"        "quart"     
[61] "remov"      "salt"       "sauc"       "season"     "serv"       "set"       
[67] "slice"      "small"      "soup"       "stir"       "sugar"      "tablespoon"
[73] "take"       "teaspoon"   "thick"      "three"      "till"       "time"      
[79] "togeth"     "two"        "use"        "water"      "well"       "white"     
[85] "will"       "work"  

findAssocs(tdm, "cinnamon", 0.75)
       cinnamon
raisin     0.79
roll       0.79
add        0.78
mix        0.78
sift       0.78
beat       0.77
layer      0.76
seed       0.76

```

* Explain what "sparsity" means in the context of `removeSparseTerms`.


## Identifying frequent and/or important terms



## Topic Modeling



