# Chapter 5: Effect Sizes

- Overview 
- Effect sizes: Measuring differences between groups
- Effect sizes: Measuring relationships between groups


## Overview

The use of effect sizes has been gaining in popularity in research for the last decade or so, but unfortunately have made little headway into business. But effect sizes provide the results that business users *really* want to know—not just whether two groups are different or the same, but ***how*** different or similar are they? 

The following tables summarize major effect size statistics (ESSs) by type of effect size (differences in tendency, differences in variation, and relationships), and some packages and functions I've found useful to calculate them. After the tables are recipes and examples of these ESSs. There are a variety of options inside R, of course, but the functions in this chapter should provide most of the essential values you'll need. 

--------

***Differences in tendency*** 

{width="wide"}
| Type | Statistic | Package | Function | 
| ---- | --------- | ------- | -------- | 
| | | | | |  
| Difference in means | \|D\| | base | `t.test(...)$estimate` | 
| | | | `t.test(...)$conf.int` |  
| | | | | |  
| | \|D\| | bootES | `bootES` with `effect.type="unstandardized"` |  
| | | | | |  
| | \|D\| | BayesianFirstAid | `bayes.t.test` |   
| | | | | |  
| Difference in medians |  | simpleboot | `two.boot(..., median)$t0` |   
| | | | | `boot.ci` of `two.boot` object |  
| | | | | |  
| |  | BayesianFirstAid | *not yet implemented* | 
| | | | | |  
| Difference in quantiles |  | simpleboot | `two.boot(..., quantile, probs=0.75)$t0` | 
| | | | | `boot.ci` of `two.boot` object |  
| | | | | |  
| Standardized mean difference | Cohen's *d* | bootES | `bootES` with `effect.type="cohens.d"` | 
| | | | | |  
| | robust Cohen's *d* | bootES | `bootES` with `effect.type="akp.robust.d"` |
| | | | | | 
| | Hedge's *g* | bootES | `bootES` with `effect.type="hedges.g"` | 
| | | | | |  
| Difference between proportions |  | base | `prop.test(...)$estimate`, `prop.test(...)$conf.int` |
| | | | | |  
| | | BayesianFirstAid | `bayes.prop.test` |
| | | | | |  
| Difference between counts or rates | rate ratio  | base | `poisson.test(...)$estimate`, `poisson.test(...)$conf.int` |
| | | | | |  
| | rate ratio | BayesianFirstAid | `bayes.prop.test`  |
| | | | | |  
| Standardized group differences | Cliff's &Delta; | orrdom | `dmes.boot` with `theta.es="dc"` | 
| | | | | |  
| | Vargha-Delaney's *A* | orrdom | `dmes.boot` with `theta.es="Ac"` | 
| | | | | |  


--------

***Differences in variability*** 

{width="wide"}
| Type | Package | Function | 
| ---- | ------- | -------- | 
| | | | |   
| Variance ratio | base | `var.test(...)$estimate`, `var(...).test$conf.int` | 
| | | | |  
| | BayesianFirstAid | *not yet implemented* | 
| | | | |
| Difference between variances | asympTest | `asymp.test(...)$estimate`, `asymp(...).test$conf.int` with `parameter="dVar"` | 
| | | | |
| | | | |

--------

***Relationships and Associations*** 

{width="wide"}
| Type | Statistic | Package | Function | 
| ---- | --------- | ------- | -------- | 
| | | | | |  
| Correlation | Pearson's *r* | base | `cor`, `cor.test(...)$conf.int` |
| | | | | |  
| | | BayesianFirstAid | `bayes.cor.test` | 
| | | | | |  
| | | bootES | `bootES` with `effect.type="r"` |
| | | | | |  
| | Spearman's &rho; | pysch | `cor.ci` with `method="spearman"` |
| | | | | |  
| | | boot | *function in recipe below*|
| | | | | |  
| | Kendall's &tau;-b | pysch | `cor.ci` with `method="kendall"` |
| | | | | |  
| | | boot | *function in recipe below*|
| | | | | |  
| Partial correlation | | psych | `corr.test`, `partial.r` | 
| | | | | |  
| Polychoric correlation (ordinal/ordinal) | | psych | `polychoric` |
| | | | | |  
| Polyserial correlation (numeric/ordinal) | | psych | `polyserial` | 
| | | | | |  
| Odds ratio | OR | psych | `oddsratio` |
| | | | | |  
| Standardized odds ratio | Yule's Q | psych | `Yule` |
| | | | | |  
| Comparisons of agreement | Cohen's &kappa; | psych | `cohen.kappa` | 
| | | | | |  
| Regression coefficient | &beta; | base | `lm`, `confint` |
| | | | | |  

--------

## Effect sizes: Measuring *differences* between groups

Perhaps the most common question an analyst will get from a business decision maker is whether or not there is a difference between groupings. There are a lot of ways to answer that, of course, but the bread-and-butter answer should always be based on effect sizes.

In this section, we'll go through all the major effect size metrics for differences; the following section will explore effect sizes for comparisons of agreement between between groups.

Effect size measures that estimate a population value should be accompanied by an appropriate confidence or credible interval whenever possible. This is usually best done by bootstrap unless you can meet distributional assumptions, although at larger sample sizes the differences will be trivial. We'll look at several ways to obtains CIs in this section.

We'll continue to use the bike share data, but we'll reshape and filter some of it for this section. We'll use `bootES`, `orddom`, 'BayesianFirstAid`, and `asympTest` to acquire confidence intervals for some of the contrasts we'll consider.

```
# Load libraries
require(simpleboot)
require(bootES)
require(orddom)
require(asympTest)
require(BayesianFirstAid)
require(reshape2)
require(dplyr)

# Reshape the data
casual_workingday_use = dcast(bike_share_daily, yr~workingday, 
  value.var="casual", sum)
casual_workingday_use$sum = casual_workingday_use$Yes + casual_workingday_use$No

# Filter the data into subsets
casual_notworkingday = filter(bike_share_daily, workingday == "No" & 
  season == "Spring" | workingday == "No" & season == "Fall")
casual_notworking_Spring = filter(casual_notworkingday, season == "Spring")
casual_notworking_Fall = filter(casual_notworkingday, season == "Fall")
```

### Basic differences

For a difference between two proportions, we'll look at the effect of it being a working day on casual bike use in 2011 versus 2012:

```
workday_diff = prop.test(casual_workingday_use$Yes, casual_workingday_use$sum)
round(workday_diff$estimate[1] - workday_diff$estimate[2], 2)

-0.02

round(workday_diff$conf.int, 2)

-0.02 -0.01
```

When we wish to describe differences in central tendency, we'll want to know whether two means or medians are different from each other. For example, to see if there's a difference in average (or median) casual bike use between Spring and Fall non-working days, we can use `t.test` (or `two.test`):

```
casual_notworkingday_mean = t.test(casual~season, data=casual_notworkingday)
abs(casual_notworkingday_mean$estimate[1] - 
  casual_notworkingday_mean$estimate[2]); casual_notworkingday_mean$conf.int

636.6031
350.6546 922.5516
```

Because the distribution isn't really normal, getting bootstrapped CIs on the difference of means is probably a better option in this case. Using the `bootES` package:

```
bootES(casual_notworkingday, data.col="casual", group.col="season", 
  contrast=c("Fall", "Spring"), effect.type="unstandardized")

User-specified lambdas: (Fall, Spring)
Scaled lambdas: (-1, 1)
95.00% bca Confidence Interval, 2000 replicates
Stat         CI (Low)     CI (High)    bias        SE           
636.603      332.203      908.222      3.116       149.380     
```

To look at a difference between medians (with a CI), use `two.test` from the `simpleboot` package. Unfortunately, `simpleboot` doesn't use formula notation so we'll have to reuse some intermediate data frames created above:

```
diff_medians = two.boot(casual_notworking_Spring$casual, casual_notworking_Fall$casual, median, R=2000)
diff_medians_ci = boot.ci(diff_medians, conf=0.95, type='bca')
diff_medians$t0; diff_medians_ci

834

BOOTSTRAP CONFIDENCE INTERVAL CALCULATIONS
Based on 2000 bootstrap replicates

CALL : 
boot.ci(boot.out = diff_medians, conf = 0.95, type = "bca")

Intervals : 
Level       BCa          
95%   ( 480.0, 1277.8 )  
Calculations and Intervals on Original Scale
```

`simpleboot` allows you to compare any two univariate statistics. For example, you could compare the 75% percentile of the two groups:

```
diff_75 = two.boot(casual_notworking_Spring$casual, casual_notworking_Fall$casual, quantile, probs=0.75, R=2000)
diff_75_ci = boot.ci(diff_medians, conf=0.95, type='bca')
diff_75$t0; diff_75_ci

   75% 
731.25 

BOOTSTRAP CONFIDENCE INTERVAL CALCULATIONS
Based on 2000 bootstrap replicates

CALL : 
boot.ci(boot.out = diff_medians, conf = 0.95, type = "bca")

Intervals : 
Level       BCa          
95%   ( 480.0, 1277.8 )  
Calculations and Intervals on Original Scale
```

Sometimes you may want the median difference instead of the difference between medians. Use `wilcox.test` for that:

```
median_diff = wilcox.test(casual~season, 
  data=casual_notworkingday, conf.int=TRUE)
median_diff$estimate; median_diff$conf.int

684.0001
399 997
```

We also often want to know whether two groups differ in terms of their variances, i.e., is one group more variable than another? If both groups are normally distributed, `var.test` is appropriate to obtain the variance ratio:

```
var.test(casual_notworkingday$casual ~ casual_notworkingday$season)$estimate

ratio of variances
          1.105087

var.test(casual_notworkingday$casual ~ casual_notworkingday$season)$conf.int

0.6498105 1.8817627
```

More typically, our groups are not normally distributed, and when that's the case, the `var.test` will give misleading results because it is very sensitive to departures from normality. The `asymp.test` function in the `asympTest` package provides a suitable alternative:

```
asymp.test(casual_notworkingday$casual ~ casual_notworkingday$season,
  parameter = "dVar")$estimate

difference of variances
               58696.86

asymp.test(casual_notworkingday$casual ~ casual_notworkingday$season,
  parameter = "dVar")$conf.int

-197619.6  315013.4
```


### Standardized differences

While not always easily explained to business users, standardized effect sizes are often very useful for analysts. When comparing means, Hedge's *g* can be used for most cases (as it is an improvement on Cohen's *d*, its more famous predecessor), while a robust version of Cohen's *d* can be acquired by setting the `effect.type` option to `"akp.robust.d"`.

```
bootES(casual_notworkingday, data.col="casual", group.col="season", 
  contrast=c("Fall", "Spring"), effect.type="hedges.g")

User-specified lambdas: (Fall, Spring)
Scaled lambdas: (-1, 1)
95.00% bca Confidence Interval, 2000 replicates
Stat        CI (Low)    CI (High)   bias        SE          
0.825       0.374       1.252       0.004       0.219      

bootES(casual_notworkingday, data.col="casual", group.col="season", 
  contrast=c("Fall", "Spring"), effect.type="akp.robust.d")

User-specified lambdas: (Fall, Spring)
Scaled lambdas: (-1, 1)
95.00% bca Confidence Interval, 2000 replicates
Stat        CI (Low)    CI (High)   bias        SE          
0.866       0.433       1.406       0.039       0.250      
```

When you don't want to make any distributional assumptions, Cliff's &Delta; and Vargha-Delaney's *A* (which is the same as the AUC statistic applied to a two-group contrast) are the best options, and, in fact, are simply linear transformations of each other. Both can be obtained from the `orddom` package, with CIs obtained via BCa by default:

```
dmes.boot(casual_notworking_Fall$casual, casual_notworking_Spring$casual,
  theta.es="dc")$theta

       dc
0.4555138

dmes.boot(casual_notworking_Fall$casual, casual_notworking_Spring$casual,
  theta.es="dc")$theta.bci.lo
dmes.boot(casual_notworking_Fall$casual, casual_notworking_Spring$casual,
  theta.es="dc")$theta.bci.up

0.2493734
0.6265664

dmes.boot(casual_notworking_Fall$casual, casual_notworking_Spring$casual,
  theta.es="Ac")$theta

       Ac
0.7277569

dmes.boot(casual_notworking_Fall$casual, casual_notworking_Spring$casual,
  theta.es="Ac")$theta.bci.lo
dmes.boot(casual_notworking_Fall$casual, casual_notworking_Spring$casual,
  theta.es="Ac")$theta.bci.up

0.6246867
0.8132832
```

This package also has a neat feature in that it will plot the results and summary of a Cliff's &Delta; assessment with the `delta_gr` function:

```
delta_gr(casual_notworking_Fall$casual, casual_notworking_Spring$casual, 
  x.name="Fall", y.name="Spring")
```

![delta gr ES plot](images/0801OS_02_15.png)


### Determining the probability of a difference

The rapid increase in computing power in recent years has put previously-intractable analytics tools at anyone's fingertips. Markov-chain Monte Carlo (MCMC) is one of the breakout stars of this revolution, which provides the mathematical basis for the resurgence of interest in Bayesian methods.

You'll need to install JAGS to your system before starting (http://mcmc-jags.sourceforge.net/), and also install the R package `rjags` to allow R to converse with JAGS.

`BayesianFirstAid` provides one-line Bayesian alternatives to common R functions for CIs ("credible intervals") and basic effect sizes. It's in development at the time of this writing and needs to be installed from Github:

```
require(devtools)
devtools::install_github("rasmusab/bayesian_first_aid")
require(BayesianFirstAid)
```

Using the earlier example above for the differences in proportions, we get the same sort of results (i.e., the effect size and CIs), as well as a plot of the posterior distributions:

```
workday_diff_bayes = bayes.prop.test(casual_workingday_use$Yes, 
  casual_workingday_use$sum)
workday_diff_bayes

    Bayesian First Aid propotion test

data: casual_workingday_use$Yes out of casual_workingday_use$sum
number of successes:  118354, 184931
number of trials:     247252, 372765
Estimated relative frequency of success [95% credible interval]:
  Group 1: 0.48 [0.48, 0.48]
  Group 2: 0.50 [0.49, 0.50]
Estimated group difference (Group 1 - Group 2):
  -0.02 [-0.02, -0.015]
The relative frequency of success is larger for Group 1 by a probability
of <0.001 and larger for Group 2 by a probability of >0.999 .

plot(workday_diff_bayes)
```

![Bayes prop ES](images/0815OS_02_16.png)

To look at the difference between means of two groups, we can use the `bayes.t.test` function:

```
casual_notworkingday_mean_bayes = bayes.t.test(casual~season, 
  data=casual_notworkingday)
casual_notworkingday_mean_bayes

    Bayesian estimation supersedes the t test (BEST) - two sample

data: group Spring (n = 56) and group Fall (n = 57)

  Estimates [95% credible interval]
mean of group Spring: 1892 [1684, 2107]
mean of group Fall: 1235 [1039, 1439]
difference of the means: 656 [357, 938]
sd of group Spring: 780 [637, 949]
sd of group Fall: 742 [609, 904]

The difference of the means is greater than 0 by a probability of >0.999
and less than 0 by a probability of <0.001

plot(casual_notworkingday_mean_bayes)
```

![Bayes mean ES](images/0815OS_02_17.png)

The package allows the use of `summary` for its output; with it we can see the CIs for each statistic as well as the standardized effect size (`eff_size`, *d*) on this contrast:

```
summary(casual_notworkingday_mean_bayes)

  Measures
               mean      sd    HDIlo    HDIup %<comp %>comp
mu_x       1891.896 107.961 1684.426 2107.064  0.000  1.000
sigma_x     784.985  80.819  636.610  949.354  0.000  1.000
mu_y       1235.311 103.061 1038.706 1439.451  0.000  1.000
sigma_y     747.461  76.146  609.178  904.010  0.000  1.000
mu_diff     656.585 148.684  357.229  938.414  0.000  1.000
sigma_diff   37.523 109.254 -181.929  249.049  0.360  0.640
nu           46.196  32.717    5.557  110.800  0.000  1.000
eff_size      0.860   0.206    0.450    1.259  0.000  1.000
x_pred     1891.952 827.714  307.633 3559.847  0.012  0.988
y_pred     1231.742 781.921 -290.477 2794.578  0.056  0.944

'HDIlo' and 'HDIup' are the limits of a 95% HDI credible interval.
'%<comp' and '%>comp' are the probabilities of the respective parameter 
  being smaller or larger than 0.

  Quantiles
              q2.5%     q25%   median     q75%   q97.5%
mu_x       1680.655 1820.233 1891.884 1963.547 2103.933
sigma_x     642.575  728.117  779.837  835.775  958.585
mu_y       1035.024 1165.506 1235.177 1304.515 1437.405
sigma_y     614.269  694.245  741.870  794.897  912.781
mu_diff     365.089  556.231  656.493  756.053  947.740
sigma_diff -178.064  -33.623   37.084  108.491  253.580
nu            9.355   23.275   37.863   60.040  128.710
eff_size      0.461    0.720    0.859    0.996    1.271
x_pred      264.184 1346.469 1892.044 2432.797 3520.282
y_pred     -309.647  721.162 1225.480 1743.096 2781.779
```

Evaluations of the difference in median and variance contrasts (among others) are planned but not yet implemented. Follow it on [GitHub](https://github.com/rasmusab/bayesian_first_aid) to check for updates. 


Finally, you can review the MCMC diagnostics for any of these functions by using diagnostics:

```
diagnostics(casual_notworkingday_mean_bayes)

Iterations = 601:10600
Thinning interval = 1 
Number of chains = 3 
Sample size per chain = 10000 

  Diagnostic measures
               mean      sd mcmc_se n_eff  Rhat
mu_x       1891.532 109.652   0.815 18162 1.000
sigma_x     786.383  81.211   0.681 14276 1.000
mu_y       1237.637 103.310   0.781 17532 1.000
sigma_y     747.915  76.431   0.618 15346 1.001
mu_diff     653.895 150.729   1.113 18408 1.001
sigma_diff   38.468 110.067   0.868 16083 1.000
nu           46.731  32.070   0.389  6879 1.002
eff_size      0.855   0.208   0.002 16886 1.001
x_pred     1884.072 827.166   4.776 29999 1.000
y_pred     1240.818 784.492   4.552 29712 1.000

mcmc_se: the estimated standard error of the MCMC approximation of the mean.
n_eff: a crude measure of effective MCMC sample size.
Rhat: the potential scale reduction factor (at convergence, Rhat=1).

  Model parameters and generated quantities
mu_x: the mean of group Spring 
sigma_x: the scale of group Spring , a consistent
  estimate of SD when nu is large.
mu_y: the mean of group Fall 
sigma_y: the scale of group Fall 
mu_diff: the difference in means (mu_x - mu_y)
sigma_diff: the difference in scale (sigma_x - sigma_y)
nu: the degrees-of-freedom for the t distribution
  fitted to casual by season 
eff_size: the effect size calculated as 
  (mu_x - mu_y) / sqrt((sigma_x^2 + sigma_y^2) / 2)
x_pred: predicted distribution for a new datapoint
  generated as group Spring 
y_pred: predicted distribution for a new datapoint
  generated as group Fall 
```

![bayes diagnostics](images/bayes_diff_diagnostics.png)


## Effect sizes: Measuring *relationships* between groups

The other basic analytics question aims to understand similarities between groups: how similar are two groups? Correlation is the primary (standardized) tool here, although a wide variety of other approaches exist; of particular use is the use of regression coeffients to answer the answer the question: to what extent are two groups related?


### Associations between numeric variables (correlation)

Correlation is one of the oldest and most well-known statistical tools, and for good reason: it provides a lot of detail in a single metric. While Pearson’s correlation (*r*) is often the default approach by many analysts, it’s not always the best—in many cases, Spearman’s rho (&rho;), Kendall’s tau-b (&tau;-b), or polyserial/polychoric approaches are better choices. And, when you have categorical data, using association coefficients like the odds ratio, Yule’s Q, or Cohen’s kappa (&kappa;) are appropriate tools, and are just as useful (and old) as correlation. We’ll explore how you can acquire each of these values with R in this section.

As before, every effect size measure should be accompanied by an appropriate credible or confidence interval whenever possible, which is usually best done by bootstrap unless you can meet the method’s assumptions.

Although correlation coefficients are easy to acquire with R’s `base` installation, anything other than confidence/credible intervals are not, so we’ll use the `psych` package for the non-parametric correlation part of this recipe and `boot`/`boot.ci`/`bootES` and `BayesianFirstAid` for those intervals. We’ll also use a piece of the bike share dataset to illustrate the methods.

```
require(psych)
require(bootES)

# Use count and air temp variables from bike share data
bike_use_atemp = data.frame(air_temp = bike_share_daily$atemp, 
  count = bike_share_daily$cnt)
```

Traditional Pearson’s correlation (*r*) is easy to acquire:

```
cor(bike_use_atemp$air_temp, bike_use_atemp$count)
cor.test(bike_use_atemp$air_temp, bike_use_atemp$count)$conf.int

0.6310657
0.5853376 0.6727918
```

You can get BCa bootstrap CIs for Pearson’s *r* by using:

```
bootES(c(bike_use_atemp$air_temp, bike_use_atemp$count), effect.type="r")

95.00% bca Confidence Interval, 2000 replicates
Stat        CI (Low)    CI (High)   bias        SE          
0.650       0.633       0.667       -0.000      0.009      
```

However, it’s pretty clear that this relationship isn’t truly linear if you plot it. Spearman’s and Kendall’s can be acquired with the `cor.ci` function as follows (use the *x*`.emp`—empirical—CI values to obtain the percentile confidence interval values, although the differences will be trivial with large sample sizes):

```
cor.ci(bike_use_atemp, method="spearman", n.iter = 10000, plot=FALSE)

Coefficients and bootstrapped confidence intervals
         ar_tm count
air_temp 1.00       
count    0.62  1.00

 scale correlations and bootstrapped confidence intervals
            lower.emp lower.norm estimate upper.norm upper.emp p
ar_tm-count      0.57       0.57     0.62       0.67      0.67 0

cor.ci(bike_use_atemp, method="kendall", n.iter = 10000, plot=FALSE)

Coefficients and bootstrapped confidence intervals
         ar_tm count
air_temp 1.00       
count    0.43  1.00

 scale correlations and bootstrapped confidence intervals
            lower.emp lower.norm estimate upper.norm upper.emp p
ar_tm-count      0.39       0.39     0.43       0.47      0.47 0

```

### Bootstrapping BCa CIs for non-parametric correlation

If you want to use the `boot` package to calculate BCa confidence intervals on the Spearman or Kendell correlation coefficients, here’s how to acquire them for each method:

```
rs_function = function(x,i){cor(x[i,1], x[i,2], method="spearman")
rs_boot = boot(bike_use_atemp, rs_function, R=10000)
boot.ci(rs_boot, type="bca")$bca[4:5]

0.5748000 0.6669792

rt_function = function(x,i){cor(x[i,1], x[i,2], method="kendall")
rt_boot = boot(bike_use_atemp, rt_function, R=10000)
boot.ci(rt_boot, type="bca")$bca[4:5]

0.3931281 0.4665541
```


### Determining the probability of a correlation

We saw the Bayesian approach to difference-based effect size calculations earlier in this chapter; here's the approach for correlation:

```
require(BayesianFirstAid)
bayes.cor.test(bike_use_atemp$air_temp, bike_use_atemp$count)


    Bayesian First Aid Pearson's Correlation Coefficient Test

data: bike_use_atemp$\textdollar$air_temp and bike_use_atemp$count (n = 731)
Estimated correlation:
  0.63
95% credible interval:
  0.59 0.68
The correlation is more than 0 by a probability of >0.999
and less than 0 by a probability of <0.001

plot(atemp_bike_cor_bayes)
```

![Bayes cor](images/0815OS_02_18.png)


### Partial correlations

If you have variables you want to control for in a correlation, we can turn again to the `psych` package. For example, if we wanted to evaluate the correlation between air temperature and bike use while controlling for windspeed, we could do it by creating a correlation matrix with the corr.test function:

```
bike_use_atemp_wind = data.frame(bike_share_daily$temp, bike_share_daily$cnt, 
  bike_share_daily$windspeed )
atemp_wind_count = corr.test(bike_use_atemp_wind, method="kendall")
atemp_wind_count$ci[1:3]

                               lower          r       upper
bk_shr_dly.t-bk_shr_dly.c  0.3713082  0.4321852  0.48936255
bk_shr_dly.t-bk_shr_dly.w -0.1807640 -0.1096882 -0.03747266
bk_shr_dly.c-bk_shr_dly.w -0.2173056 -0.1471101 -0.07540089
```

Once we have a matrix, we can plug it into the `partial.r` function, specifying that the first two variables of the matrix are the correlation of interest and the third is the variable we wish to control for (the second and third inputs, respectively):

```
partial.r(as.matrix(atemp_wind_count$r), c(1:2), 3)

                       bike_share_daily.atemp bike_share_daily.cnt
bike_share_daily.atemp                   1.00                 0.42
bike_share_daily.cnt                     0.42                 1.00
```

### Polychoric and polyserial correlation for ordinal data

Polychoric (ordinal-ordinal) and polyserial (numeric-ordinal) correlation allows you to perform correlation when some or all of your variables of interest are on an ordinal scale. Again, `psych` provides the functions we'll use. To illustrate, we'll use the math attitudes survey (`mass`) data seen earlier in this chapter, converted from ordered factor to numeric for the polychoric function to work:

```
data(mass, package="likert")
poly_math = data.frame(as.numeric(mass[,7]), as.numeric(mass[,14]))
colnames(poly_math) = c("worry", "enjoy")
polychoric(poly_math)$rho

           worry      enjoy
worry  1.0000000 -0.7731209
enjoy -0.7731209  1.0000000
```

To perform a polyserial correlation, we'll add a fake variable representing each student's pre-college math assessment test score to see whether it relates to their stated enjoyment:

```
math_score = c(755, 642, 626, 671, 578, 539, 769, 614, 550, 615, 749, 676, 753, 
  509, 798, 783, 508, 767, 738, 660)
polyserial(math_score, poly_math$enjoy)

0.2664201
```

### Associations between categorical variables

Perhaps the most famous association metric is the odds ratio, which is really useful but can be misleading for those not used to its mathematical properties. Yule’s Q has more desirable properties in many cases, especially in explaining results to business users: it provides the same type of information as correlation does for quantitative data, that is, a value scaled to [-1,1]. Cohen’s &kappa; is probably the best tool when you want to compare agreement instead of association.

Since the bike share data doesn’t lend itself well to exploring the association of categorical variables, we’ll explore “categorical correlation” with the `Aspirin` data from the `abd` package. The functions we’ll use include the `oddsratio` function from the `epitools` package, and the `Yule` (Yule’s Q) function from `psych`:

```
require(epitools)
require(psych)
data(Aspirin, package="abd")

# Obtain the odds ratio and CI
oddsratio(table(Aspirin))$measure

         odds ratio with 95% C.I.
treatment  estimate     lower    upper
  Aspirin 1.0000000        NA       NA
  Placebo 0.9913098 0.9187395 1.069632

# Obtain Yule's Q
Yule(table(Aspirin))

-0.004352794
```


### Cohen’s kappa for comparisons of agreement

Ratings on ordinal scales require special treatment, since they aren’t true numbers, and comparisons of agreement make it even more necessary to choose a statistically-appropriate tool. Cohen’s &kappa; (`cohen.kappa` in the `psych` package) provides a way to compare ordinal ratings, e.g., a doctor’s ratings for a set of patients potentially eligible for care management as compared with the same output from a predictive model:

```
# Doctor ratings
doctor = c("yes", "no", "yes", "unsure", "yes", "no", "unsure", "no", "no", 
  "yes", "no", "yes", "yes")
# Model ratings
model = c("yes", "yes", "unsure", "yes", "no", "no", "unsure", "no", "unsure", 
  "no", "yes", "yes", "unsure")

# Obtain Cohen's kappa
cohen.kappa(x=cbind(doctor,model))

Cohen Kappa and Weighted Kappa correlation coefficients and confidence boundaries

                 lower estimate upper
unweighted kappa -0.15     0.22  0.59
weighted kappa   -0.45     0.15  0.75
 Number of subjects = 12
```

The `$agree` output value shows the proportion of agreement for each category:

```
cohen.kappa(doctor_vs_model)$agree

        x2f
x1f              no     unsure        yes
  no     0.15384615 0.07692308 0.15384615
  unsure 0.00000000 0.07692308 0.07692308
  yes    0.15384615 0.15384615 0.15384615

  ```
  

### Regression coefficient

Although technically the use of regression implies causality, there are times when you just wish to know the impact of one variable on its relationship with another. When the "predictor" is categorical, the regression coefficient is simply the difference in means, seen at the start of this chapter (`lm(casual~season, data=casual_notworkingday)`). When both variables are numeric, the coefficient is the change in the y-variable for each unit of the x-variable, so regardless of which variable you choose to the be descriptor, you can see the effect size.

We'll use the `tao` dataset seen in the previous chapter to explore the effect of air temperature on sea surface temperature using the `lm` function. 

```
data(tao, package="VIM")

# run the linear model
effect_air_on_sea = lm(Sea.Surface.Temp ~ Air.Temp, data=tao)

# review model coefficients
effect_air_on_sea

Call:
lm(formula = Sea.Surface.Temp ~ Air.Temp, data = tao)

Coefficients:
(Intercept)     Air.Temp  
     -3.867        1.176  

# get 95% confidence interval
confint(effect_air_on_sea)
                2.5 %    97.5 %
(Intercept) -4.320256 -3.412817
Air.Temp     1.157677  1.193817
```

We can see that for every 1 degree increase in air temperature, we get an increase of about 1.18 degrees in sea surface temperature (95% CI [1.16, 1.19]):   


### *R^2^*: Proportion of variance explained 
 
The *R^2^* effect size is simply the correlation coeffient squared, of course, but oddly enough there does not seem to be a built-in function to readily obtain the CIs for it. We can use the `boot` package to create one, thanks to [a function by Rob Kabacoff](http://www.statmethods.net/advstats/bootstrapping.html):
 
```
library(boot)
rsq = function(formula, data, indices) {
  d = data[indices,] # allows boot to select sample
  fit = lm(formula, data=d)
  return(summary(fit)$r.square)
  }
 
# bootstrap R2 with 10k replications
air_temp_R2 = boot(data=tao, statistic=rsq, R=10000,
  formula=Sea.Surface.Temp ~ Air.Temp)

# view bootstrapped R2 results
air_temp_R2

ORDINARY NONPARAMETRIC BOOTSTRAP

Call:
boot(data = tao, statistic = rsq, R = 10000, formula = Sea.Surface.Temp ~ 
    Air.Temp)


Bootstrap Statistics :
     original       bias    std. error
t1* 0.9615353 4.891461e-05 0.003223984

# get 95% confidence interval
boot.ci(air_temp_R2, type="bca")

BOOTSTRAP CONFIDENCE INTERVAL CALCULATIONS
Based on 10000 bootstrap replicates

CALL : 
boot.ci(boot.out = air_temp_R2, type = "bca")

Intervals : 
Level       BCa          
95%   ( 0.9541,  0.9669 )  
Calculations and Intervals on Original Scale

# plot results
plot(air_temp_R2)
```

![R2 bootstrapped CI](images/r2bootstrapped.png)
