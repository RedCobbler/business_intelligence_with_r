# Chapter 1: An entire project in a few lines of code

Learning R is an infamously difficult thing to do. However, if you need to accomplish a particular analytics goal, a solution in R can be found relatively easily if you focus on that single task. Over time, completing a variety of single tasks leads you to a practical working knowledge of R in general. So don't think about "learning R"—think instead about "learning how to solve the problem at hand with R," and things get a lot easier.  

Essentially, the technical part of the typical BI workflow is to:

1. Set up and develop project space  
2. Acquire data  
3. Wrangle data  
4. Perform analytics  
5. Develop report/data product  
6. Document the process  

This chapter shows how simple it can be do some advanced analytics in R, working through each of these six steps.

## The analytics problem

The data we'll use in this chapter contains 2+ million records of minute-scale power consumption in a single household for about 47 months, collected by Electricité de France and archived at the UCI Machine Learning Repository. The data set's page can be accessed [here](https://archive.ics.uci.edu/ml/datasets/Individual+household+electric+power+consumption) for more details, but for convenience, the attributes of this dataset are as follows:

1. **`date`**: date in format dd/mm/yyyy
2. **`time`**: time in format hh:mm:ss
3. **`global_active_power`**: household global minute-averaged active power (in kilowatts)
4. **`global_reactive_power`**: household global minute-averaged reactive power (in kilowatts)
5. **`voltage`**: minute-averaged voltage (in volts)
6. **`global_intensity`**: household global minute-averaged current intensity (in amperes)
7. **`sub_metering_1`**: energy sub-metering No. 1 (in watt-hours of active energy). It corresponds to the kitchen, containing mainly a dishwasher, an oven, and a microwave (hot plates are not electric but gas powered).
8. **`sub_metering_2`**: energy sub-metering No. 2 (in watt-hours of active energy). It corresponds to the laundry room, containing a washing-machine, a tumble-drier, a refrigerator, and a light.
9. **`sub_metering_3`**: energy sub-metering No. 3 (in watt-hours of active energy). It corresponds to an electric water-heater and an air-conditioner.

Our purpose is to describe past monthly power usage and forecast potential usage for six months following the last full month of data in the dataset.


## Set up

First, we need to set up the working environment—this is generally easiest to do using the file system (see *Appendix 1*), but for sake of explanation and thoroughness, we'll do the entire process in this chapter within R. To ensure clarity and reproducibility, we need to set up a single directory for each project, with subfolders that contain and separate code, data, results, and so on. 

Open **RStudio**. 

R starts in the default directory, symbolized at the top of the console window as `~/`. 

Create the directory space with `dir.create`, the R equivalent to `mkdir`:

```
# Creates directory/ies, use recursive=TRUE in the first
# command to create the entire path at once
dir.create("~/BIWR/Chapter1/Code", recursive=TRUE)
dir.create("~/BIWR/Chapter1/Data")
dir.create("~/BIWR/Chapter1/Results")
```

Normally, at this point we'd use the script window to write out the script, often using R Markdown to create the report and documentation as we go. For now, you either cut and paste each line into the console as we go, open a new R Script window and put the code into that, or just load [this chapter's code from Github](https://github.com/Rmadillo/business_intelligence_with_r/blob/master/manuscript/code/BIWR_Chapter_1_Code.R). At the end of this chapter, we'll go through the process using an R Markdown file to document the project and create a final data product. 

Remember, if you are cutting and pasting code from the book, you will need to manually edit the URLs, which are broken up here because of page width constraints. 

Move into your new directory and load the R packages we'll use in this project:

```
# Set the working directory
setwd("~/BIWR/Chapter1")

# This package will allow us to interpolate between missing time series values
require(zoo)

# These packages provide functions for easy data wrangling
require(dplyr)
require(reshape2)

# This package provides automated forecasting of time series data
require(forecast)

# This package allows us to create publication-quality plots
require(ggplot2)

# This package allows creation of javascript widgets for use in webpages
require(htmlwidgets)

# This packages uses htmlwidgets to make time series widgets
require(dygraphs)
```


## Acquire data


Download the data from a zipped flat file (semi-colon delimted .txt format) on the UCI Machine Learning Repository:

```
# Download the zip file into the data folder (note the line break here!)
download.file("http://archive.ics.uci.edu/ml/machine-learning-databases/00235/
  household_power_consumption.zip", destfile = 
  "Data/household_power_consumption.zip")

# Unzip the data from the zip file into the Data folder
unzip("Data/household_power_consumption.zip", exdir="Data")

# Read the data into R 
# NAs are represented by blanks and ? in this data, so need to change
power = read.table("Data/household_power_consumption.txt", sep=";", header=T, 
  na.strings=c("?",""), stringsAsFactors=FALSE)
```


## Wrangling data

We'll look at the structure of the data to see what's been read in:

```
# str gives you the structure of the dataset
str(power)
```

![](images/power_str_1.png)

The `Date` and `Time` variables were read in as characters, so we'll convert them to date and time classes, respectively, as well as create a new `DateTime` column:

```
# Convert character date to an ISO date 
power$Date = as.Date(power$Date, format="%d/%m/%Y")

# Create a DateTime object
power$DateTime = as.POSIXct(paste(power$Date, power$Time))

# Obtain the Month and Year for each data point
power$Month = format(power$Date,"%Y-%m")

# Add the first to each Y-m combo and convert back to ISO Date
power$Month = as.Date(paste0(power$Month, "-01"))

# Verify the changes
str(power)
```

![](images/power_str_2.png)

```
# Get an overview of the variables
summary(power)
```

![](images/summary_power.png)

We can see from the summary that there are about 26k missing values (`NAs`) in our primary variable, `Global_active_power`—about 1.25% of the ~2 million records. We can quickly get a table and graph of missing data over time by setting a counting marker for missing values with `ifelse`, then using the `dplyr` package to group and summarize the data, and finally pull a ready-made R function from the web to create a calendar graph that shows the daily distribution of the missing values.  

```
# Use ifelse to count each minute that is NA
power$Missing = ifelse(is.na(power$Global_active_power), 1, 0)

# Use dplyr's group_by function to group the data by Date
power_group_day = group_by(power, Date)

# Use dplyr's summarize function to summarize by our NA indicator 
# (where 1 = 1 minute with NA)
power_day_missing = summarize(power_group_day, Count_Missing = sum(Missing))

# Download the 'calendarHeat' function from revolutionanalytics.com
source("http://blog.revolutionanalytics.com/downloads/calendarHeat.R")

# Plot the calendar graph to view the missing data pattern
calendarHeat(power_day_missing$Date, power_day_missing$Count_Missing, 
  varname="Missing Data", color="w2b")
```

![](images/biwr_01_power_missing.png)

You can view the actual values of missing data on each day by exploring the `power_day_missing` data frame. 

The 4-5 day spans of missing data near the end of the time series may be a little concerning since we want to perform automated forecasting. However, since we're aggregating to months and forecasting into months with very few missing values, we should be ok. But to make automatic forecasting work, we need to fill in those missing values. If we convert each missing value to 0, we'll definitely underestimate usage for those times. 

A reasonable approach here is to carry the last value forward, which we can do with the `na.locf` function in the `zoo` package. While other approaches are possible (and perhaps better, e.g., using some sort of mean or median value instead of the last value, or even a seasonal Kalman filter), we'll proceed with this option to keep the example simple. 

```
# Use zoo to perform interpolation for missing time series values
power$Global_active_power_locf = na.locf(power$Global_active_power)

# Compare the original and interpolated distributions by
# reshaping the two variables into long form for ggplot
power_long = melt(power, id.vars= "DateTime", measure.vars=
  c("Global_active_power", "Global_active_power_locf"))

# Create density plot
ggplot(power_long, aes(value, fill=variable, color=variable)) +
  geom_density(alpha=0.75) +
  facet_wrap(~variable)

# Save density plot to Results folder
# Note that ggave uses "dpi" to set image resolution
ggsave("Results/density_plot.png", width=6, height=4, dpi=600, units="in")
```

![](images/biwr_01_power_histos.png)

The overall shape hasn't changed, though we can see a small spike at about 1 kW in the last-observation-carried-forward data. This should be fine for our purposes, as the overall pattern is essentially the same. 

Now that we have a complete time series, we can determine total monthly use (kWh). While we're at it, we can calculate maximum demand for a given month (kW) over the period of record as an example of how to calculate multiple summaries at once. kWh measures *use*, i.e., how much energy is used, while kW measures *demand*. We're interested in usage, because that's how power companies charge us.

```
# Use dplyr to group by month
power_group = group_by(power, Month)

# Use dplyr to get monthly max demand and total use results
power_monthly = summarize(power_group, 
  Max_Demand_kW = max(Global_active_power_locf), 
  Total_Use_kWh = sum(Global_active_power_locf)/60)

# Remove partial months from data frame
power_monthly = power_monthly[2:47,]

# Convert Month to Date
power_monthly$Month = as.Date(paste0(power_monthly$Month, "-01"))

# Look at structure of the result
str(power_monthly)
```

![](images/power_monthly_str.png)

## Analytics

### Explore the data

Plotting your data is the single most important part of analytics, so this book spends a lot of space on graphical representation. Here, as we're focused on power use over time, we'll plot the monthly summary we've calculated above. 

```
# Create plot of total use by month
ggplot(power_monthly, aes(Month, Total_Use_kWh)) +
  geom_line(col="blue", lwd=1) 

# Save plot as hi-res png to Results subfolder
ggsave("Results/total_use_plot.png", width=6, height=4, dpi=600, units="in")
```

![](images/total_use_plot.png)

We can see clear patterns in the data—higher in the winter and lower in the summer. 


### Run a forecasting model

Now we want to forecast total use for the next six months. We'll create the model with the automated `forecast` function from the `forecast` package, then plot the results and view the model itself. 

```
# Create a time series object of monthly total use
total_use_ts = ts(power_monthly$Total_Use_kWh, start=c(2007,1), frequency=12)

# Automatically obtain the forecast for the next 6 months
# using the forecast package's forecast function
# See ?forecast for more details
total_use_fc = forecast(total_use_ts, h=6)

# View the forecast model results
summary(total_use_fc)
```

![](images/forecast_model_output.png)

```
# Export a copy of the model results into a text file in the Results folder
sink("Results/Forecast_Model.txt")
summary(total_use_fc)
sink()

# View the forecast plot
plot(total_use_fc)

# Save the base graphics plot to the Results folder
# Note that resolution is called 'res' here
png("Results/total_use_forecast.png", width=6, height=4, res=600, units="in")
plot(total_use_fc)
dev.off() 
```

![](images/total_use_forecast.png)


## Reporting

With the forecast summary and plot we have the essential pieces for addressing the problem. Now we need to create a report for decision-making. In this case, we'll again keep it simple and just produce an interactive browser app of the monthly trends and forecast results. 

### Create an interactive HTML plot

```
# Create a data frame with the original data
# and placeholders for the forecast details
use_df = data.frame(Total_Use = power_monthly$Total_Use_kWh, 
  Forecast = NA, Upper_80 = NA, 
  Lower_80 = NA, Upper_95 = NA, Lower_95 = NA)

# Create a data frame for the forecast details
# with a placeholder column for the original data
use_fc = data.frame(Total_Use = NA, Forecast = total_use_fc$mean, 
  Upper_80 = total_use_fc$upper[,1], Lower_80 = total_use_fc$lower[,1], 
  Upper_95 = total_use_fc$upper[,2], Lower_95 = total_use_fc$lower[,2])

# Union the two data frames into one using rbind
use_ts_fc = rbind(use_df, use_fc)

# Create a time series of the data and forecast results
total_use_forecast = ts(use_ts_fc, start=c(2007, 1), freq=12)

# Create the widget
energy_use_prediction_widget = dygraph(total_use_forecast, 
  main = "Predicted Monthly Electricity Use (kWh)",
  ylab = "Total kWh", width=900, height=500) %>% 
  dySeries(c("Total_Use"), label = "Actual kWh Usage") %>%
  dyEvent(x = "2008-08-01", "Went on vacation", labelLoc = "top") %>%
  dyRangeSelector(dateWindow = c("2008-09-15", "2011-06-01")) %>%
  dyLegend(width = 800)

# Display the widget in the Viewer window
# Hit the Zoom button for a pop-out 
energy_use_prediction_widget

# Save the widget as a stand-alone HTML file to Results folder
saveWidget(energy_use_prediction_widget, "energy_use_prediction_widget.html")
```

Opening the `energy_use_prediction_widget.html` from the Reports folder shows us the result. This stand-alone file can be sent to the decision-maker(s) who can explore the exact trend and forecast values with a mouse hover, while still seeing the overall pattern.

![](images/energy_widget_screenshot.png)


## Documenting the project

R markdown is a brilliant way to create documentation and reports in one shot. For example, had this forecasting project actually required a report, I'd structure it all inside an `.Rmd` like I've shown in *Appendix 2*. To run that file manually, just load it into RStudio and click the **Knit HTML** button in the *Source* window. A preview window will pop up. In some systems, the HTML file is concurrently saved to the working directory, while in others you may need to **Open in browser** and then **Save as** from there. 

![](images/biwr_ch1_knitr_ex.png)

|  |  

To run an `.Rmd` file programatically, the `rmarkdown` package allows you to run it from the console, a command line, or a "make" file (see *Appendix 1*) via the `render` function:

```
render("Code/Hauteville_House_Power_Forecast_20101127.Rmd", "html_document")
```

W> Note: as I prefer to have the `.Rmd` file self-contained, I sometimes include the directory creation steps in the code. This will ***fail*** if you've already created the directories---comment those sections out if you're (re)running it several times locally for testing.  

I often have a script window (`.R`) and an R Markdown (.`Rmd`) window open at the same time when working on projects. Once I have the code the way I want it, I transfer that code to the `.Rmd` file if I intend to include it in any data/reporting products. When I have steps or results and want to jot down my rationale for the methods I'm using, interpretations and ideas for future use, and so on, I do that in an `.Rmd` file as well. At the end, I aim for an `.Rmd` file that will allow another user to reproduce the entire analysis and have an understanding of why I chose certain methods or came to the conclusions I report. For longer analyses, this may require a "make" file type of setup (see *Appendix 1*). 

Even better is when it's a small project, like this one---I can contain the code, documentation, and report all in one file. Since decision makers want the answer first, I include all of the code except for results inside a single code chunk near the top. The results I place in separate code chunks, placed where appropriate.  

## Summary

The entire analytics workflow for this project---from directory set-up through creating a forecast model and developing the final report---took only about 50 lines of code, excluding comments and spacing. We've seen in this chapter how much can be accomplished in just a few lines of R code; there are very few languages in which you really can do this much so succinctly.   

The rest of this book provides code snippets, examples, and full recipes for using R throughout the main portions of the analytics workflow: data acquisition through exploration and reporting. 
