# Chapter 1: An entire project in a few lines of code

Learning R is an infamously difficult thing to do. However, if you need to accomplish a particular analytics goal, the solution in R can often be pretty simple. Most business intelligence analysts just need to accomplish a set of particular tasks, many of which can be done relatively easily in R. 

Essentially, the technial part of the typical BI workflow is

1. Set up and develop project space  
2. Acquire data  
3. Wrangle data  
4. Perform analytics  
5. Develop report/data product  
6. Document the process  

This chapter shows how simple it can be do some advanced analytics in R, working through each of these six steps.

## The Analytics Problem

The data we'll use in this chapter contains 2+ million records of minute-scale power consumption in a single household for about 47 months, collected by Electricité de France and archived at the UCI Machine Learning Repository. The data set's page can be accessed [here](https://archive.ics.uci.edu/ml/datasets/Individual+household+electric+power+consumption) for more details, but for convience, the attributes of this dataset are as follows:

1. **`date`**: date in format dd/mm/yyyy
2. **`time`**: time in format hh:mm:ss
3. **`global_active_power`**: household global minute-averaged active power (in kilowatts)
4. **`global_reactive_power`**: household global minute-averaged reactive power (in kilowatts)
5. **`voltage`**: minute-averaged voltage (in volts)
6. **`global_intensity`**: household global minute-averaged current intensity (in amperes)
7. **`sub_metering_1`**: energy sub-metering No. 1 (in watt-hours of active energy). It corresponds to the kitchen, containing mainly a dishwasher, an oven and a microwave (hot plates are not electric but gas powered).
8. **`sub_metering_2`**: energy sub-metering No. 2 (in watt-hours of active energy). It corresponds to the laundry room, containing a washing-machine, a tumble-drier, a refrigerator and a light.
9. **`sub_metering_3`**: energy sub-metering No. 3 (in watt-hours of active energy). It corresponds to an electric water-heater and an air-conditioner.

Our purpose is to describe past monthly power usage and forecast potential usage for six months following the last full month of data in the dataset.


## Set up

First, we need to set up the working environment—this is generally easiest to do using the file system (see Appendix 1), but for sake of explanation and thoroughness, we'll do the entire process in this chapter within R. To ensure clarity and reproducibility, we need to set up a single directory for each project, with subfolders that contain and separate code, data, results, and so on. 

Open RStudio. 

R starts in the default directory, symbolized at the top of the console window as `~/`. 

Create the directory space with `dir.create`, the R equivalent to `mkdir`:

```
# Creates directory/ies, use recursive=TRUE in the first
# command to create the entire path at once
dir.create("~/BIWR/Chapter1/Code", recursive=T)
dir.create("~/BIWR/Chapter1/Data")
dir.create("~/BIWR/Chapter1/Results")
```

Normally, at this point we'd use the script window to write out the script, often using R Markdown to create the report and documentation as we go. For now, you either cut and paste each line into the console as we go, open a new R Script window and put the code as-is into that, or just load the chapter's code from github. At the end of this chapter, we'll go through the process using an R Markdown file to document the project and create a final data product. 

Move into your new directory and load the R packages we'll use in this project:

```
# Set the working directory
setwd("~/BIWR/Chapter1")

# This package will allow us to interpolate between missing time series values
require(zoo)

# These packages provides functions for easy data wrangling
require(dplyr)
require(reshape2)

# This package provides automated forecasting of time series data
require(forecast)

# This package allows us to create publication-quality plots
require(ggplot2)

# This package allows creation of javascript widgets for use in webpages
require(htmlwidgets)

# This packages uses htmlwidgets to make time series widgets
require(dygraphs)
```


## Acquire Data


Download the data from a zipped flat file (semi-colon delimted .txt format) on the UCI Machine Learning Repository:

```
# Download the zip file into the data folder
download.file("http://archive.ics.uci.edu/ml/machine-learning-databases/00235/household_power_consumption.zip", destfile = "Data/household_power_consumption.zip")

# Unzip the data from the zip file into the Data folder
unzip("Data/household_power_consumption.zip", exdir="Data")

# Read the data into R 
# NAs are represented by blanks and ? in this data, so need to change
power = read.table("Data/household_power_consumption.txt", sep=";", header=T, 
  na.strings=c("?",""), stringsAsFactors=FALSE)
```


## Wrangling Data

We'll look at the structure of the data to see what's been read in:

```
# str gives you the structure of the dataset
str(power)
```

![str 1](/images/power_str_1.png)

The `Date` and `Time` variables were read in as characters, so we'll convert them to date and time classes, respectively, as well as create a new `DateTime` column:

```
# Convert data to Date object
power$Date = as.Date(power$Date, format="%d/%m/%Y")

# Create a DateTime object
power$DateTime = as.POSIXct(paste(power$Date, power$Time))

# Obtain the Month and Year for each data point
power$Month = format(power$Date,"%Y-%m")

# Add the first to each Y-m combo and convert back to Date
power$Month = as.Date(paste0(power_monthly$Month, "-01"))

# Verify the changes
str(power)
```

![str 2](/images/power_str_2.png)

```
# Get an overview of the variables
summary(power)
```

![summary](/images/summary_power.png)

We can see from the summary that there are about 26k missing values (`NAs`) in our primary variable, `Global_active_power`—about 1.25% of the ~2 million records. We can quickly get a table and graph of missing data over time by using setting a counting marker for missing values with `ifelse`, then using the `dplyr` package to group and summarize the data, and finally pull a ready-made R function from the web to create a calendar graph that shows the daily distribution of the missing values.  

```
# Use 'ifelse' to count each minute that is NA
power$Missing = ifelse(is.na(power$Global_active_power), 1, 0)

# Use 'dplyr' to group the data by Date
power_group_day = group_by(power, Date)

# Use 'dplyr' to summarize by our NA indicator 
# (where 1 = 1 minute with NA)
power_day_missing = summarise(power_group_day, Count_Missing = sum(Missing))

# Download the 'calendarHeat' function from revolutionanalytics.com
source("http://blog.revolutionanalytics.com/downloads/calendarHeat.R")

# Plot the calendar graph to view the missing data pattern
calendarHeat(power_day_missing$Date, power_day_missing$Count_Missing, 
  varname="Missing Data", color="w2b")
```

![missing](/images/biwr_01_power_missing.png)

You can view the the actual values of missing data on each day by exploring the `power_day_missing` data frame. 

The "recent" 4-5 day spans of missing data may be a little concerning since we want to perform automated forecasting. However, since we're aggregating to months and forecasting into months with very few missing values, we should be ok. But to make automatic forecasting work, we need to fill in those missing values. If we convert each missing value to 0, we'll definitely underestimate usage for those times. A reasonable first pass is to carry the last value forward, which we can do with the `na.locf` function in the `zoo` package. While other approaches are possible (and perhaps better, e.g., using some sort of mean or median value instead of the last value, or even a seasonal Kalman filter), we'll proceed with this option to keep the example simple. 

```
# Use 'zoo' to perform interpolation for missing time series values
power$Global_active_power_locf = na.locf(power$Global_active_power)

# Compare the original and interpolated distributions
# Reshape the two variables into 'long' form for ggplot
power_long = melt(power, id.vars= "DateTime", measure.vars=
  c("Global_active_power", "Global_active_power_locf"))

# Create density plot
density_plot = ggplot(power_long, aes(value, fill=variable, color=variable)) +
  geom_density(alpha=0.75) +
  facet_wrap(~variable)

# Display plot
density_plot

# Save density plot to Results folder
ggsave("Results/density_plot.png", width=6, height=4, dpi=600, units="in")
```

![power histograms](/images/biwr_01_power_histos.png)

The overall shape hasn't changed, though we can see a small spike at about 1 kW. This should be fine for forecasting. 

Now that we have a complete time series, we can determine total monthly use (kWh). While we're at it, we can calculate maximum demand for a given month (kW) over the period of record as an example of how to calculate multiple summaries at once. kWh measures use, i.e., how much energy is used, while kW measures demand. We're interested in usage, because that's what power companies use to charge us.

```
# Use 'dplyr' to group by month
power_group = group_by(power, Month)

# Use 'dplyr' to get monthly max demand and total use results
power_monthly = summarise(power_group, 
    Max_Demand_kW = max(Global_active_power_locf), 
    Total_Use_kWh = sum(Global_active_power_locf)/60)

# Remove partial months from data frame
power_monthly = power_monthly[2:47,]

# Convert Month to Date
power_monthly$Month = as.Date(paste0(power_monthly$Month, "-01"))

# Look at structure of the result
str(power_monthly)
```

![power monthly str](/images/power_monthly_str.png)

## Analytics

### Explore data

```
# Create plot of total use by month
total_use_plot = ggplot(power_monthly, aes(Month, Total_Use_kWh)) +
    geom_line(col="blue", lwd=1) 

# Display plot
total_use_plot

# Save plot as hi-res png to Results subfolder
ggsave("Results/total_use_plot.png", width=6, height=4, dpi=600, units="in")
```

![total use plot](/images/total_use_plot.png)


### Forecast Total Usage

```
# Create a time series object of Total Usage
total_use_ts = ts(power_monthly$Total_Use_kWh, start=c(2007,1), frequency=12)

# Automatically obtain the forecast for the next 6 months
# using the 'forecast' package's forecast function
# See ?forecast for more details
total_use_fc = forecast(total_use_ts, h=6)

# View the forecast model results
summary(total_use_fc)
```

![fprecast model output](/images/forecast_model_output.png)

```
# "Sink" a copy of the model results into a text file in the Results folder
sink("Results/Forecast_Model.txt")
summary(total_use_fc)
sink()

# View the forecast plot
plot(total_use_fc)

# Save the plot to the Results folder
# Note that resolution is called 'res' here
png("Results/total_use_forecast.png", width=6, height=4, res=600, units="in")
plot(total_use_fc)
dev.off() 
```

![total_use_forecast](/images/total_use_forecast.png)


## Reporting

With the forecast summary and plot we have the essential pieces for addressing the problem. Now we need to create a "report" for decision-making; in this case, we'll again keep it simple and just produce an interactive browser app of the monthly trends and forecast results. 

### Create an Interactive .html Plot

```
# Create a data frame with the original data
# and space for the forecast details
use_df = data.frame(Total_Use = power_monthly$Total_Use_kWh, 
                    Forecast = NA, Upper_80 = NA, 
                    Lower_80 = NA, Upper_95 = NA, Lower_95 = NA)

# Create a data frame for the forecast details
# with a column for the original data
use_fc = data.frame(Total_Use = NA, Forecast = total_use_fc$mean, Upper_80 = 
    total_use_fc$upper[,1], Lower_80 = total_use_fc$lower[,1], 
    Upper_95 = total_use_fc$upper[,2], Lower_95 = total_use_fc$lower[,2])

# "Union" the two data frames into one
use_ts_fc = rbind(use_df, use_fc)

# Create a time series of the data and forecast results
total_use_forecast = ts(use_ts_fc, start=c(2007, 1), freq=12)

# Create the widget
energy_use_prediction_widget = dygraph(total_use_forecast, 
    main = "Predicted Monthly Electricty Use (kWh)",
    ylab = "Total kWh", width=900, height=500) %>% 
    dySeries(c("Total_Use"), label = "Actual kWh Usage") %>%
    dyEvent(date = "2008-08-01", "Went on vacation", labelLoc = "top") %>%
    dyRangeSelector(dateWindow = c("2008-09-15", "2011-06-01")) %>%
    dyLegend(width = 800)

# Display the widget in the Viewer window
# Hit the Zoom button for a pop-out 
energy_use_prediction_widget

# Save the widget as a stand-alone html file to Results folder
saveWidget(energy_use_prediction_widget, "energy_use_prediction_widget.html")
```

Opening the `energy_use_prediction_widget.html` from the Reports folder shows us the result. This stand-alone file can be sent to the decision-maker(s) who can explore the exact trend and forecast values with a mouse hover, while still seeing the overall pattern.

![energy widget](/images/energy_widget_screenshot.png)


## Documenting the Project

I like to have a script window (`.R`) and an R Markdown (.`Rmd`) window open at the same time when working on projects; once I have the code the way I want it, I transfer that to the `.Rmd` file. When I have steps or results and want to jot down my rationale for the methods I'm using, interpretations and ideas for future use, and so on, I do that in the `.Rmd` file as well. At the end, I aim for an `.Rmd` file that will allow another user to reproduce the entire analysis and have an understanding of why I chose certain methods or came to the conclusions I report. 

Even better is when it's a small project—I can make the code, documentation, and report all one file. Since decision makers want the answer first, I include all of the code except for results inside a single code chunk. The results I place in separate code chunks, placed where appropriate. For example, had the forecasting project above required a report, I'd structure it all inside an `.Rmd` as follows: 

<<(code/Hauteville_House_Power_Forecast_20101127.Rmd)

To run this file manually, just load it into R Studio and click the *Knit HTML* button in the Source window. A preview window will pop up, while the .html file is concurrently saved to the working directory. To run it programatically, the `rmarkdown` package allows you to run it from the console, a command line, or a "make" file (see Appendix 2) via the `render` function:

```
render("Code/Hauteville_House_Power_Forecast_20101127.Rmd", "html_document")
```

Note: as I like the entire `.Rmd` file to be self-contained, I often include the directory creation steps in the code. This will *fail* if you've already created the directories; what I do is comment those sections out if I'm running it locally for testing.  

`rmarkdown` (and `pandoc`, for exporting to a wide variety of file types) are included in the installation of RStudio. (If you don't use RStudio, install instructions are [here](https://github.com/rstudio/rmarkdown#installation).) 

<br>
<hr>

The entire analytics workflow for this project—from directory set-up through creating a forecast model and developing the final report—took only 50 lines of code.  

The rest of this book provides code snippets, examples, and full recipes for using R throughout the main portions of the analytics workflow: data acquisition through exploration and reporting. Predictive modeling (including forecasting) and more advanced analytics will be covered in Part 2 (forthcoming in late 2015 or early 2016). But we've seen in this chapter how much can be accomplished in just a few lines of R code; there are very few languages in which you really can do it all.
