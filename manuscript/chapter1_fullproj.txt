# Chapter 1: An entire project in a few lines of code

Learning R is an infamously difficult thing to do. However, learning to do *x* with R can be pretty simple. Most business intelligence analysts just need to accomplish a set of particular tasks, many of which can be done relatively easily in R.

Essentially, the technial part of the typical BI workflow is

1. Set up and develop project space  
2. Acquire data  
3. Wrangle data  
4. Perform analytics  
5. Develop report/data product  
6. Document the process  

This chapter shows how simple it can be do some advanced analytics in R, working through each of these six steps.

## The Analytics Problem

The data we'll use in this chapter contains 2+ million records of minute-scale power consumption in a single household for about 47 months, collected by Electricité de France and archived at the UCI Machine Learning Repository. The data set's page can be accessed [here](https://archive.ics.uci.edu/ml/datasets/Individual+household+electric+power+consumption) for more details, but for convience, the attributes of this dataset are as follows:

1. **`date`**: date in format dd/mm/yyyy
2. **`time`**: time in format hh:mm:ss
3. **`global_active_power`**: household global minute-averaged active power (in kilowatts)
4. **`global_reactive_power`**: household global minute-averaged reactive power (in kilowatts)
5. **`voltage`**: minute-averaged voltage (in volts)
6. **`global_intensity`**: household global minute-averaged current intensity (in amperes)
7. **`sub_metering_1`**: energy sub-metering No. 1 (in watt-hours of active energy). It corresponds to the kitchen, containing mainly a dishwasher, an oven and a microwave (hot plates are not electric but gas powered).
8. **`sub_metering_2`**: energy sub-metering No. 2 (in watt-hours of active energy). It corresponds to the laundry room, containing a washing-machine, a tumble-drier, a refrigerator and a light.
9. **`sub_metering_3`**: energy sub-metering No. 3 (in watt-hours of active energy). It corresponds to an electric water-heater and an air-conditioner.

Our purpose is to describe past monthly power usage and forecast potential usage for six months following the last full month of data in the dataset.


## Set up

First, we need to set up the working environment—this is generally easiest to do using the file system (see Appendix 1), but for sake of explanation and thoroughness, we'll do the entire process in this chapter within R. To ensure clarity and reproducibility, we need to set up a single directory for each project, with subfolders that contain and separate code, data, results, and so on. 

Open RStudio. 

R starts in the default directory, symbolized at the top of the console window as `~/`. 

Create the directory space with `dir.create`, the R equivalent to `mkdir`:

```
# Creates directory/ies, use recursive=TRUE in the first
# command to create the entire path at once
dir.create("~/BIWR/Chapter1/Code", recursive=T)
dir.create("~/BIWR/Chapter1/Data")
dir.create("~/BIWR/Chapter1/Results")
```

Normally, at this point we'd use the script window to write out the script, often using R Markdown to create the documentation as we go. For now, you either cut and paste each line into the console as we go, or open a new R Script window and put the code as-is into that. At the end of this chapter, we'll go through the entire process using an R Markdown file to document the project and create a final data product. 

Move into your new directory and load the R packages we'll use in this project:

```
# Set the working directory
setwd("~/BIWR/Chapter1")

# This package allows us to explore time values separate from date values
require(chron)

# This package will allow us to interpolate between missing time series values
require(zoo)

# This package provides functions for easy data wrangling
require(dplyr)

# This package provides automated forecasting of time series data
require(forecast)

# This package allows us to create publication-quality plots
require(ggplot2)

# This package allows creation of javascript widgets for use in webpages
require(htmlwidgets)

# This packages uses htmlwidgets to make time series widgets
require(dygraphs)
```


## Acquire Data


Download the data from a zipped flat file (semi-colon delimted .txt format) on the UCI Machine Learning Repository:

```
# Download the zip file into the data folder
download.file("http://archive.ics.uci.edu/ml/machine-learning-databases/00235/household_power_consumption.zip", destfile = "Data/household_power_consumption.zip")

# Unzip the data from the zip file into the Data folder
unzip("Data/household_power_consumption.zip", exdir="Data")

# Read the data into R 
# NAs are represented by blanks and ? in this data, so need to change
power = read.table("Data/household_power_consumption.txt", sep=";", header=T, 
  na.strings=c("?",""), stringsAsFactors=FALSE)
```


## Wrangling Data

We'll look at the structure of the data to see what's been read in:

```
# str gives you the structure of the dataset
str(power)

'data.frame':	2075259 obs. of  9 variables:
 $ Date                 : chr  "16/12/2006" "16/12/2006" "16/12/2006" "16/12/2006" ...
 $ Time                 : chr  "17:24:00" "17:25:00" "17:26:00" "17:27:00" ...
 $ Global_active_power  : num  4.22 5.36 5.37 5.39 3.67 ...
 $ Global_reactive_power: num  0.418 0.436 0.498 0.502 0.528 0.522 0.52 0.52 0.51 0.51 ...
 $ Voltage              : num  235 234 233 234 236 ...
 $ Global_intensity     : num  18.4 23 23 23 15.8 15 15.8 15.8 15.8 15.8 ...
 $ Sub_metering_1       : num  0 0 0 0 0 0 0 0 0 0 ...
 $ Sub_metering_2       : num  1 1 2 1 1 2 1 1 1 2 ...
 $ Sub_metering_3       : num  17 16 17 17 17 17 17 17 17 16 ...
```

The `Date` and `Time` variables were read in as characters, so we'll convert them to date and time classes, respectively, as well as create a new `DateTime` column:

```
# Convert data to Date object
power$Date = as.Date(power$Date, format="%d/%m/%Y")

# Create a DateTime object
power$DateTime = as.POSIXct(paste(power$Date, power$Time))

# Obtain the Month and Year for each data point
power$Month = format(power$Date,"%Y-%m")

# Add the first to each Y-m combo and convert back to Date
power_monthly$Month = as.Date(paste0(power_monthly$Month, "-01"))

# Verify the changes
str(power)

'data.frame':	2075259 obs. of  11 variables:
 $ Date                 : Date, format: "2006-12-16" "2006-12-16" "2006-12-16" ...
 $ Time                 : chr  "17:24:00" "17:25:00" "17:26:00" "17:27:00" ...
 $ Global_active_power  : num  4.22 5.36 5.37 5.39 3.67 ...
 $ Global_reactive_power: num  0.418 0.436 0.498 0.502 0.528 0.522 0.52 0.52 0.51 0.51 ...
 $ Voltage              : num  235 234 233 234 236 ...
 $ Global_intensity     : num  18.4 23 23 23 15.8 15 15.8 15.8 15.8 15.8 ...
 $ Sub_metering_1       : num  0 0 0 0 0 0 0 0 0 0 ...
 $ Sub_metering_2       : num  1 1 2 1 1 2 1 1 1 2 ...
 $ Sub_metering_3       : num  17 16 17 17 17 17 17 17 17 16 ...
 $ DateTime             : POSIXct, format: "2006-12-16 17:24:00" "2006-12-16 17:25:00" "2006-12-16 17:26:00" ...
 $ Month                   : Date, format: "2006-12-01" "2006-12-01" "2006-12-01" ...

# Get an overview of the variables
summary(power)
```

***Need to figure this part out--image?***

      Date                Time           Global_active_power Global_reactive_power
 Min.   :2006-12-16   Length:2075259     Min.   : 0.076      Min.   :0.000        
 1st Qu.:2007-12-12   Class :character   1st Qu.: 0.308      1st Qu.:0.048        
 Median :2008-12-06   Mode  :character   Median : 0.602      Median :0.100        
 Mean   :2008-12-05                      Mean   : 1.092      Mean   :0.124        
 3rd Qu.:2009-12-01                      3rd Qu.: 1.528      3rd Qu.:0.194        
 Max.   :2010-11-26                      Max.   :11.122      Max.   :1.390        
                                         NA's   :25979       NA's   :25979        
    Voltage      Global_intensity Sub_metering_1   Sub_metering_2   Sub_metering_3  
 Min.   :223.2   Min.   : 0.200   Min.   : 0.000   Min.   : 0.000   Min.   : 0.000  
 1st Qu.:239.0   1st Qu.: 1.400   1st Qu.: 0.000   1st Qu.: 0.000   1st Qu.: 0.000  
 Median :241.0   Median : 2.600   Median : 0.000   Median : 0.000   Median : 1.000  
 Mean   :240.8   Mean   : 4.628   Mean   : 1.122   Mean   : 1.299   Mean   : 6.458  
 3rd Qu.:242.9   3rd Qu.: 6.400   3rd Qu.: 0.000   3rd Qu.: 1.000   3rd Qu.:17.000  
 Max.   :254.2   Max.   :48.400   Max.   :88.000   Max.   :80.000   Max.   :31.000  
 NA's   :25979   NA's   :25979    NA's   :25979    NA's   :25979    NA's   :25979   
    DateTime                      Month          
 Min.   :2006-12-16 17:24:00   Min.   :2006-12-01    
 1st Qu.:2007-12-12 00:18:30   1st Qu.:2007-12-01  
 Median :2008-12-06 07:13:00   Median :2008-12-01  
 Mean   :2008-12-06 06:33:21   Mean   :2008-11-21                  
 3rd Qu.:2009-12-01 14:07:30   3rd Qu.:2009-12-01                  
 Max.   :2010-11-26 21:02:00   Max.   :2010-11-01   


We can see from the summary that there are about 26k missing values (`NAs`) in our primary variable, `Global_active_power`—about 1.25% of the ~2 million records. We can quickly get a table and graph of missing data over time by using setting a counting marker for missing values with `ifelse`, then using the `dplyr` package to group and summarize the data, and finally pull a ready-made R function from the web to create a calendar graph that shows the daily distribution of the missing values.  

```
# Use 'ifelse' to count each minute that is NA
power$Missing = ifelse(is.na(power$Global_active_power), 1, 0)

# Use 'dplyr' to group the data by Date
power_group_day = group_by(power, Date)

# Use 'dplyr' to summarize by our NA indicator 
# (where 1 = 1 minute with NA)
power_day_missing = summarise(power_group_day, Count_Missing = sum(Missing))

# Download the 'calendarHeat' function from revolutionanalytics.com
source("http://blog.revolutionanalytics.com/downloads/calendarHeat.R")

# Plot the calendar graph to view the missing data pattern
calendarHeat(power_day_missing$Date, power_day_missing$Count_Missing, 
  varname="Missing Data", color="w2b")
```

![missing](/images/biwr_01_power_missing.png)

You can view the the actual values of missing data on each day by exploring the `power_day_missing` data frame. 

The "recent" 4-5 day spans of missing data may be a little concerning since we want to perform automated forecasting. However, since we're aggregating to months and forecasting into months with very few missing values, we should be ok. But to make automatic forecasting work, we need to fill in those missing values. If we convert each missing value to 0, we'll definitely underestimate usage for those times. A reasonable first pass is to carry the last value forward, which we can do with the `na.locf` function in the `zoo` package. While other approaches are possible (and perhaps better, e.g., using some sort of mean or median value instead of the last value, or even a seasonal Kalman filter), we'll proceed with this option to keep the example simple. 

```
# Use 'zoo' to perform interpolation for missing time series values
power$Global_active_power_locf = na.locf(power$Global_active_power)

# Compare the original and interpolated distributions
# Reshape the two variables into 'long' form for ggplot
power_long = melt(power, id.vars= "DateTime", measure.vars=
  c("Global_active_power", "Global_active_power_locf"))

# Create density plot
density_plot = ggplot(power_long, aes(value, fill=variable, color=variable)) +
  geom_density(alpha=0.75) +
  facet_wrap(~variable)

# Display plot
density_plot

# Save density plot to Results folder
ggsave("Results/density_plot.png", width=6, height=4, dpi=600, units="in")
```

![power histograms](/images/biwr_01_power_histos.png)

The overall shape hasn't changed, though we can see a small spike at about 1 kW. This should be fine for forecasting. 

Now that we have a complete time series, we can determine total monthly use (kWh). While we're at it, we can calculate maximum demand for a given month (kW) over the period of record as an example of how to calculate multiple summaries at once. kWh measures use, i.e., how much energy is used, while kW measures demand. We're interested in usage, because that's what power companies use to charge us.

```
# Use 'dplyr' to group by month
power_group = group_by(power, Month)

# Use 'dplyr' to get monthly max demand and total use results
power_monthly = summarise(power_group, 
    Max_Demand_kW = max(Global_active_power_locf), 
    Total_Use_kWh = sum(Global_active_power_locf)/60)

# Remove partial months from data frame
power_monthly = power_monthly[2:47,]

# Convert Month to Date
power_monthly$Month = as.Date(paste0(power_monthly$Month, "-01"))

# Look at structure of the result
str(power_monthly)
```


## Analytics

### Explore data

```
# Create plot of total use by month
total_use_plot = ggplot(power_monthly, aes(Month, Total_Use_kWh)) +
    geom_line(col="blue", lwd=1) 

# Display plot
total_use_plot

# Save plot as hi-res png to Results subfolder
ggsave("Results/total_use_plot.png", width=6, height=4, dpi=600, units="in")
```

![total use plot](/images/total_use_plot.png)


### Forecast Total Usage

```
# Create a time series object of Total Usage
total_use_ts = ts(power_monthly$Total_Use_kWh, start=c(2007,1), frequency=12)

# Automatically obtain the forecast for the next 6 months
# using the 'forecast' package's forecast function
# See ?forecast for more details
total_use_fc = forecast(total_use_ts, h=6)

# View the forecast model results
summary(total_use_fc)

# "Sink" a copy of the model results into a text file in the Results folder
sink("Results/Forecast_Model.txt")
summary(total_use_fc)
sink()

# View the forecast plot
plot(total_use_fc)

# Save the plot to the Results folder
# Note that resolution is called 'res' here
png("Results/total_use_forecast.png", width=6, height=4, res=600, units="in")
plot(total_use_fc)
dev.off() 
```

![total_use_forecast](/images/total_use_forecast.png)


## Reporting

With the forecast summary and plot we have the essential pieces for addressing the problem. Now we need to create a "report" for decision-making; in this case, we'll again keep it simple and just produce an interactive browser app of the monthly trends and forecast results. 

### Create an Interactive .html Plot

```
# Create a data frame with the original data
# and space for the forecast details
use_df = data.frame(Total_Use = power_monthly$Total_Use_kWh, 
                    Forecast = NA, Upper_80 = NA, 
                    Lower_80 = NA, Upper_95 = NA, Lower_95 = NA)

# Create a data frame for the forecast details
# with a column for the original data
use_fc = data.frame(Total_Use = NA, Forecast = total_use_fc$mean, Upper_80 = 
                        total_use_fc$upper[,1], Lower_80 = total_use_fc$lower[,1], 
                    Upper_95 = total_use_fc$upper[,2], Lower_95 = total_use_fc$lower[,2])

# "Union" the two data frames into one
use_ts_fc = rbind(use_df, use_fc)

# Create a time series of the data and forecast results
total_use_forecast = ts(use_ts_fc, start=c(2007, 1), freq=12)

# Create the widget
energy_use_prediction_widget = dygraph(total_use_forecast, 
    main = "Predicted Monthly Electricty Use (kWh)",
    ylab = "Total kWh", width=900, height=500) %>% 
    dySeries(c("Total_Use"), label = "Actual kWh Usage") %>%
    dyEvent(date = "2008-08-01", "Went on vacation", labelLoc = "top") %>%
    dyRangeSelector(dateWindow = c("2008-09-15", "2011-06-01")) %>%
    dyLegend(width = 800)

# Display the widget in the Viewer window
# Hit the Zoom button for a pop-out 
energy_use_prediction_widget

# Save the widget as a stand-alone html file to Results folder
saveWidget(energy_use_prediction_widget, "energy_use_prediction_widget.html")
```

Opening the `energy_use_prediction_widget.html` from the Reports folder shows us the result. This stand-alone file can be sent to the decision-maker(s) who can explore the exact trend and forecast values with a mouse hover, while still seeing the overall pattern.

![energy widget](/images/energy_widget_screenshot.png)


## Documenting the Project

blah blah blah
