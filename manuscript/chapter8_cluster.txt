# Chapter 8: Pattern Discovery and Dimension Reduction: Proximity, Clusters, and Outliers

- Mapping multivariate relationships with non-metric multidimensional scaling (nMDS) and Principal Components Analysis (PCA)
- Grouping observations with hierarchical clustering 
- Identifying and describing group membership with kMeans and PAM
- Determining optimal numbers of groups with model-based clustering
- Identifying group membership with non-linear clusters
- Exploring outliers
- Finding associations in shopping carts


Humans like to put things in boxes. What things are most like each other? is one of the most common questions you face in the analysis of multivariate data. Technically, we can think of these tasks as dimension reduction--how can we take a mass of data and reduce it to something that provides insight? Techniques to take many variables and cluster observations or detect outliers have been around for decades, though it’s been only recently where desktop computing power was sufficient for more advanced similarity-analysis tool development. 

Non-metric multidimensional scaling (nMDS) is one such tool. It’s similar to Principal Components Analysis (PCA), but does not suffer from the assumption that the multivariate relationships must be linear. As a result, it has already replaced PCA as the primary descriptive and dimension reduction tool for multivariate data in some fields, and is moving rapidly into others. But although nMDS works better for nearly all applied problems and is easier to explain to business client, PCA is still widely used. 

Cluster analysis has advanced as well, with the development of techniques for clustering non-linear groupings by density via DBSCAN and similar tools, and being able to make inferential claims on cluster membership with model-based clustering. Still, the old standbys of kMeans, PAM, and hierarchical clustering all remain excellent first-order descriptive tools for multivariate data. 

If anyone has ever asked you for an "automatic outlier detection" data product, you probably realized that they probably don't understand analytics. What counts as an outlier is completely dependent on context. That said, sometimes you need to explore what might count as an outlier across enough variables that visualization becomes difficult, if not impossible. Other times, you might want to get a head-start on investigating a potential outlier or trend before it wrecks things. Multivariate outlier exploration and anomaly detection are two useful exploratory tools for these purposes. 

Finding things that are (really) similar or (really) different has been the common theme to the tools in this chapter. Sometimes, you just want to be purely empirical about it, and let the data itself give you information to explore based simply on common co-occurrences. Association analysis--more commonly called "shopping cart analysis"--is a great way to do this. 

 
## Mapping multivariate relationships

### Non-metric multidimensional scaling (nMDS)

Non-metric multidimensional scaling (nMDS) is a useful way to identify groups, outliers, and influential variables in multivariate space. In essence, nMDS collapses multiple dimensions into two or three so that relationships based on multivariate proximity can be visualized. nMDS models a similarity matrix in low dimensional space in much the same way as a map can be created from scattered GPS coordinates.

The data in this example we first saw in Chapter 7: nine financial ratios for 66 Spanish banks, 29 of which failed during the Spanish banking crisis (Serrano-Cinca 2001). 

The nMDS analysis is largely automated in `vegan` using the `metaMDS` function (see `?metaMDS` for all the details). This function defaults to expecting count data; for continuous data, setting the distance metric to euclidean is more appropriate. There are dozens of distance metrics (see `?vegdist` for the options available in `vegan`), but only two are really needed for most applications: Bray-Curtis for count data (the default) and Euclidean for continuous data. `metaMDS` automatically performs transformations when given count data. Manual transformations are sometimes needed if the variables are continuous and have considerably different scales (e.g., scale to mean 0/sd 1, see `?scale`).

```
require(vegan)

banks_mds = metaMDS(banks[,3:11], distance="euclidean", autotransform=FALSE, 
  noshare=FALSE, wascores=FALSE)
```

These options are set in this case to suppress warning messages and to ensure that the default transformations and measures are not used.

The plot that is built from this algorithm is the heart of nMDS analysis. Categorical information can be added as well. Note that axis values are meaningless in nMDS except as x/y coordinates; only spatial pattern matters for interpretation. Much like a map, points that lie close together on the plot are more similar, while points that are further away are more dissimilar:

```
par(mar=c(1.5,0,1,0))
banks.stress = round(banks_mds$textdollar$stress, digits=2)

# Set up color scheme
groups = ifelse(banks$\textdollar$Solvente == 0, "darkred", "blue")

# Create an empty plot
ordiplot(banks_mds, type="n", xaxt="n", yaxt="n", xlab=" ", ylab=" ", 
  display="sites")

# Add banks by number and color (solvency)
orditorp(banks_mds, display="sites", col=groups, air=0.01, cex=0.75)

# Add legend
legend("topleft", legend=c("Bankrupt", "Solvent"), bty="n", 
  col=c("darkred","blue"), title=paste("Stress: ",banks.stress), pch=21, 
  pt.bg=c("darkred", "blue"), cex=0.75)
```

![initial nmds](images/0815OS_05_nmds1.png)

You also can easily explore regions within the plot in more detail by using xlim and ylim to zoom in on an area of interest, such as the right-side cluster:

```
ordiplot(banks_mds, type="n", xlab=" ", ylab=" ", display="sites", 
  xlim=c(0,0.07), ylim=c(-0.1, 0.095))
orditorp(banks_mds, display="sites", col=groups, air=0.01, cex=0.75)
legend("topleft", legend=c("Bankrupt", "Solvent"), bty="n", col=c("darkred",
  "blue"), title=paste("Stress: ",banks.stress), pch=21, pt.bg=c("darkred", 
  "blue"), cex=0.75)
```

![zoomed nmds](images/0815OS_05_nmds2.png)

Essentially, nMDS creates a map of a similarity matrix. For example, a matrix of European inter-city distance (see `?eurodist`) creates an nMDS plot that closely reflects the actual distance between these cities:

```
par(mar=c(1.5,4,1,4))
euromap = metaMDS(eurodist, distance="euclidean", autotransform=FALSE, 
  noshare=FALSE, wascores=FALSE)
euromap$\textdollar$points[,1] = -euromap$\textdollar$points[,1]
ordiplot(euromap, display="sites", xaxt="n", yaxt="n", xlab=" ", ylab=" ", 
  type="t")
```

![euromap nmds](images/0815OS_05_nmds4.png)

nMDS does not use the absolute values of the variables, but rather their rank orders. While this loses some information contained in the raw data, the use of ranks omits some of the issues associated with using absolute distance (e.g., sensitivity to transformation and linearity), and as a result is a very flexible technique that works well with a variety of types of data. It's also particularly useful when there are a large number of predictors relative to the number of entities, where using actual values could easily lead to overfitting.

Diagnostics for nMDS results are also simple. A Shepard plot (also called a stress plot) shows the relationship between the similarity matrix and how well the nMDS algorithm represented that matrix in the plot. The greater the scatter around the (non-linear) regression line, the higher the stress, which would imply that the matrix similarities are not as well represented in the plot.

```
banks.stress = round(banks_mds$stress, digits=2)
banks.stress

0.03

stressplot(banks_mds, xaxt="n", yaxt="n")
```

![stress plot](images/0815OS_05_nmds3.png)

The stress value itself is a single value that represents how well the plot represents the underlying similarity matrix. It can be roughly interpreted as follows:

{width="narrow"}
| Stress Score | How well the plot represents the matrix |
| ------------ |:---------------------------------------:|
| < 0.05 | Excellent |
| > 0.05-0.10 | Very Good |
| > 0.10-0.15 | Good |
| > 0.15-0.20 | Fair |
| > 0.20-0.30 | Poor |
| > 0.30 | No representation |


Based on the diagnostics, the this plot represents the similarity matrix quite well, so we can be confident in the nMDS representation of this dataset.

Note: in general, stress values rise with the size of the dataset. Thus, interpreting the final stress score requires considering the size of the dataset in addition to considering the Shepard plot and the patterns revealed in the nMDS plot. When stress is high, sometimes it can be brought down by `metaMDS` options `k` and `trymax`, but the guidelines above should not be considered definitive—sometimes clear patterns can be seen even with relatively higher levels of stress.

The final configuration may differ depending on the noise in the data, the initial configuration, and the number of iterations, so running the nMDS multiple times and compare the interpretation from the lowest stress results is a good practice. `metaMDS` does this automatically, however, so multiple restarts are only necessary when running nMDS manually.

### Vector mapping influential variables over the nMDS plot

You can add vectors of the variables used to construct the matrix over the nMDS plot to explore how they influenced the final configuration. The length and direction of each vector gives a relative sense of each variable's influence on the "map":

```
banks.vectors = envfit(banks_mds, banks[,3:11])
ordiplot(banks_mds, type="n", xaxt="n", yaxt="n", xlab=" ", ylab=" ", 
  display="sites")
orditorp(banks_mds, display="sites", col=groups, air=0.01, cex=0.75)
plot(banks.vectors, col="gray40", cex=0.75)
legend("topleft", legend=c("Bankrupt", "Solvent"), bty="n", 
  col=c("darkred","blue"), title=paste("Stress: ",banks.stress), pch=21, 
  pt.bg=c("darkred", "blue"), cex=0.75)
```

![vectors nmds](images/0815OS_05_nmds5.png)

### Contour mapping influential variables over the nMDS plot

Since there is a clear left-right gradient, and the R8 variable (the ratio of Cost of Sales to Sales) seems to be one of the major reasons for that, a contour of that variable, in that variable’s scale values--in this case, the ratio--can be mapped in the nMDS plot. Other contours can be mapped as well; this example shows R8 and R4 contours: 

```
par(mar=c(1.5,0,1,0))
# Empty plot
ordiplot(banks_mds, type="n", xaxt="n", yaxt="n", xlab=" ", ylab=" ", 
  display="sites")
# Add contour of variable R8 (Cost of Sales:Sales ratio)
# Putting it in tmp suppresses console output
tmp = ordisurf(banks_mds, banks$\textdollar$R8, add=TRUE, col="gray30")
# Add contour of variable R4 (Reserves:Loans ratio)
tmp = ordisurf(banks_mds, banks$\textdollar$R4, add=TRUE, col="gray70")
# Plot nMDS solution
orditorp(banks_mds, display="sites", col=groups, air=0.01, cex=0.75)
legend("topleft", legend=c("Bankrupt", "Solvent"), bty="n", 
  col=c("darkred","blue"), title=paste("Stress: ",banks.stress), pch=21, 
  pt.bg=c("darkred", "blue"), cex=0.75)
```

![initial nmds](images/0815OS_05_nmds7.png)

While technically any number of contours can be plotted, in practice more than two renders the plot essentially unreadable.

### Principal Components Analysis (PCA)

PCA is often used as an exploratory technique in similar ways to nMDS, and in some cases where your data have similar scales and there are linear combinations between the variables, it can sometimes prove more powerful. If the assumptions can be justified, PCA has the added benefit of being able to provide dimension reduction through linear combinations of the original variables, in which case it can be used as input to other tools, such as regression.

```
banks_pca = princomp(banks[,3:11], cor=TRUE)
summary(banks_pca, loadings=TRUE)

Importance of components:
                          Comp.1    Comp.2    Comp.3     Comp.4     Comp.5      Comp.6
Standard deviation     2.1811785 1.5982195 1.0433711 0.62850818 0.33908817 0.248945162
Proportion of Variance 0.5286155 0.2838117 0.1209581 0.04389139 0.01277564 0.006885966
Cumulative Proportion  0.5286155 0.8124273 0.9333854 0.97727679 0.99005243 0.996938395
                            Comp.7       Comp.8       Comp.9
Standard deviation     0.161143044 0.0392016948 7.113040e-03
Proportion of Variance 0.002885231 0.0001707525 5.621704e-06
Cumulative Proportion  0.999823626 0.9999943783 1.000000e+00

Loadings:
   Comp.1 Comp.2 Comp.3 Comp.4 Comp.5 Comp.6 Comp.7 Comp.8 Comp.9
R1  0.163 -0.575        -0.190 -0.328  0.115         0.693       
R2        -0.506 -0.452  0.342  0.601 -0.207 -0.105              
R3  0.173 -0.566  0.112 -0.200 -0.254  0.143  0.146 -0.698       
R4  0.201 -0.110  0.831         0.445        -0.228              
R5  0.433  0.141 -0.114 -0.306  0.196         0.344  0.118 -0.715
R6  0.412  0.167 -0.254 -0.300               -0.790 -0.120       
R7  0.434  0.136        -0.308  0.205         0.404         0.693
R8 -0.413 -0.109        -0.570        -0.695                     
R9  0.424                0.451 -0.430 -0.648                     
```

The basic `biplot` function provides the essential details (e.g., `biplot(banks_pca, col=c("black", "gray40"))`) but a function by Vince Vu on Github does the same, only via `ggplot2` for a prettier image:

```
# require(devtools)
# install_github("vqv/ggbiplot")

require(ggbiplot)

ggbiplot(banks_pca, labels =  rownames(banks), ellipse=T, groups=
  as.factor(banks$\textdollar$Solvente)) + 
  theme_bw()
```

![pca biplot](images/0815OS_05_07.png)


Finally, variance plots often accompany PCA work; here's a way to do it with ggplot:

```
variances = data.frame(vars=banks_pca$\textdollar$sdev^2, var_exp = 
  vars/sum(vars),_cumsum = cumsum(vars), PCs=1:length(banks_pca$\textdollar$sdev))

ggplot(variances, aes(PCs, var_cumsum)) +
    scale_x_discrete(limits=c(1:9)) +
    ylab("Var. Explained (bars) and Cumulative (line) by PC") +
    geom_bar(aes(y=var_exp), stat="identity", fill="gray", alpha=.5) +
    geom_line() +
    theme_bw()
```

![pca variance](images/0815OS_05_08.png)

### nMDS for Categories: Correspondence Analysis

Correspondence analysis (CA) is a special case of nMDS, serving essentially the same purpose for categorical variables. It's very useful for visualizing relationships within contingency tables, particularly when mosaic plots are too noisy. There are several packages that can perform CA, but the aptly named `ca` package provides a good, simple way to do it. We'll use the `smoke` dataset built into `ca`, which is a fake dataset on smoking habits (none, light, medium, heavy) among senior managers (SM), junior managers (JM), senior employees (SE), junior employees (JE), and secretaries (SC).

```
require(ca)

smoke_ca = ca(smoke)
summary(smoke_ca)

Principal inertias (eigenvalues):

 dim    value      %   cum%   scree plot               
 1      0.074759  87.8  87.8  **********************   
 2      0.010017  11.8  99.5  ***                      
 3      0.000414   0.5 100.0                           
        -------- -----                                 
 Total: 0.085190 100.0                                 

Rows:
    name   mass  qlt  inr    k=1 cor ctr    k=2 cor ctr  
1 |   SM |   57  893   31 |  -66  92   3 | -194 800 214 |
2 |   JM |   93  991  139 |  259 526  84 | -243 465 551 |
3 |   SE |  264 1000  450 | -381 999 512 |  -11   1   3 |
4 |   JE |  456 1000  308 |  233 942 331 |   58  58 152 |
5 |   SC |  130  999   71 | -201 865  70 |   79 133  81 |

Columns:
    name   mass  qlt  inr    k=1 cor ctr    k=2 cor ctr  
1 | none |  316 1000  577 | -393 994 654 |  -30   6  29 |
2 | lght |  233  984   83 |   99 327  31 |  141 657 463 |
3 | medm |  321  983  148 |  196 982 166 |    7   1   2 |
4 | hevy |  130  995  192 |  294 684 150 | -198 310 506 |

plot(smoke_ca, arrows=c("F","T"), mass = c(TRUE, TRUE))
```

![ca nmds](images/0815OS_05_ca.png)
 
## Grouping observations with hierarchical clustering

Hierarchical clustering can be a useful way to summarize high-dimensional data, but results can be strongly dependent on the clustering method. The average, Ward’s, and centroid methods tend to be the best places to start. 

Hierarchical cluster analysis tools are built into the `base` installation, so no specific packages are required here. We'll continue to work with the Spanish banking data, so load it now if necessary. 

```
banks = read.csv("Data/quiebra.csv", header=T)
```

Typically, variables to cluster on are scaled (usually to mean 0 and sd 1), a distance matrix is created on the scaled data, and a clustering method is applied to the distance matrix. Since the set of descriptive variables in this data set are all on the same scale (i.e., a set of ratios), we won’t scale this data. We'll use Ward's clustering method here, which aims to create compact clusters by minimizing variance.

```
banks_dist = dist(banks[,3:11])
banks_hclust = hclust(banks_dist, method="ward.D")
plot(banks_hclust, hang=-1, labels=paste(banks$Banco, banks$Numero, sep="-"))
```

![basic hclust](images/0815OS_05_01.png)

See `?hclust` for a short overview of the clustering methods available. The method and approach you use are important--changes in either will change the result (see below for an example).  

### Plotting a cluster dendrogram with ggplot

The `NeatMap` package has a helper function—`draw.dendrogram`—that allows you to place an `hclust` object in `ggplot2`, where you can style or label it from there however you wish, e.g.:

```
require(NeatMap)
ggplot() +
    draw.dendrogram(banks_hclust) +
    scale_color_manual(name="Status: ",values=c("darkred", "blue"),
                        labels=c("Bankrupt ", "Solvent ")) +
    geom_text(aes(x=0.75, y=banks_hclust$\textdollar$order, 
      label=banks$\textdollar$Numero, color=factor(banks$\textdollar$Solvente)),
      size=4) +
    ggtitle("Cluster Dendrogram (Ward's) - Spanish Banking Crisis") +
    theme(panel.grid.major = element_blank(), panel.grid.minor = element_blank(),
          panel.border = element_blank(), panel.background = element_blank(),
          axis.title = element_blank(), axis.text = element_blank(),
          axis.ticks = element_blank(), legend.position="top")
```

![neatmap hclust](images/0815OS_05_08_old.png)

As we mentioned above, a small change in approach can complete alter the result. Say we had scaled the data first. 

```
banks_scaled = scale(banks[,3:11])
banks_dist = dist(banks_scaled)
banks_hclust = hclust(banks_dist, method="ward.D")
# Now rerun the ggplot commands as above
```

![neatmap hclust](images/0815OS_05_09.png)

The clear structure seen above in the unscaled data is now somewhat muddled. 

### Exploring hierarchical clustering of nMDS results

NeatMap's `draw.dendrogram3d` function allows you to plot an `hclust` object over your nMDS results, and explore it interactively in 3D in a new window; a screenshot of this view is shown below the code:

```
require(NeatMap)
banks_pos = banks_mds$\textdollar$points # nMDS results from previous recipe
draw.dendrogram3d(banks_hclust, banks_pos, labels=banks$\textdollar$Numero, 
  label.colors=banks$\textdollar$Solvente+3, label.size=1)
```

![neatmap nmds hclust](images/0815OS_05_10.png)

### How to partition the results of a hierarchical cluster analysis

The `cutree` function will cut a hierarchical cluster dendrogram into a user-supplied number of groups, which can be useful when combined with the results of a `clusGap`-based PAM or kMeans analysis. That analysis, above, suggested that two clusters is the best (mathematical) solution, so we’ll continue to use k=2 here. Note that since it is cutting the tree based on the clustering method’s distance values, results from `cutree` may not match the clusters obtained with `pam` or `kmeans`.

```
bank_clusters = cutree(banks_hclust, k=2)
table(bank_clusters)
aggregate(banks[,3:11], by=list(cluster=bank_clusters), median)

bank_clusters
 1  2
62  4

  cluster      R1      R2      R3     R4      R5      R6      R7      R8       R9
1       1 0.39265 0.26210 0.40725 0.0203  0.0042  0.1002  0.0044 0.87650  0.00955
2       2 0.36585 0.23295 0.38455 0.0152 -0.0380 -0.9390 -0.0397 1.30815 -0.01630

plot(banks_hclust, hang=-1, labels=paste(banks$\textdollar$Banco, banks$\textdollar$Numero, sep="-"), cex=0.9)
rect.hclust(banks_hclust, k=2)
```

![rect hclust](images/0815OS_05_14.png)


## Identifying and describing group membership with kMeans and PAM

kMeans and Partitioning Around Medoids (PAM) are two methods that will divide your observations into {k groups. kMeans works with quantitative data only, and can be susceptible to outliers, while PAM works with both quantitative as well as categorical data. As medoids are simply the “most representative” member of a cluster, outliers don’t really affect PAM. As a result, it is a more robust version of kMeans. If you have a large data set, look at the `clara` package, which performs PAM by iteratively sampling the data to speed up analysis time.

The `cluster` package contains the basic PAM function, so load that first. We’ll continue to use the Spanish banking data, so load it also, if necessary:

```
require(cluster)
banks = read.csv("Data/quiebra.csv", header=T)
```

To run either analysis, you start with the number of groups (*k*) in which you wish to partition the data. We know there are two natural categories in this data set—banks that went bankrupt and those that remained solvent, so we can use those factors to check the results of the analysis. Often you don’t have natural groups in your data, and you’re not sure how many groups there should be. That topic is briefly touched on below with clusGap and at more length in the next recipe on model-based clustering. 

To run PAM with k=2:

```
banks_pam = pam(banks[,3:11], k=2)
banks_pam


Medoids:
     ID     R1     R2     R3     R4      R5      R6      R7     R8      R9
[1,] 58 0.4074 0.2764 0.4397 0.0156  0.0043  0.0582  0.0046 0.8846  0.0095
[2,] 17 0.3740 0.2205 0.3938 0.0012 -0.0537 -1.0671 -0.0565 1.3713 -0.0231
Clustering vector:
 [1] 1 2 1 1 1 1 1 1 1 1 1 1 1 1 1 1 2 1 1 1 1 1 1 2 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1
[46] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1
```

To run kMeans with k=2:

```
banks_kmeans = kmeans(banks[,3:11], centers=2, nstart=50)
banks_kmeans

K-means clustering with 2 clusters of sizes 3, 63

Cluster means:
         R1        R2        R3         R4           R5         R6           R7        R8
1 0.3409333 0.2465000 0.3561333 0.01186667 -0.052733333 -1.2071667 -0.055200000 1.2808000
2 0.3952127 0.2674238 0.4173810 0.02471746  0.005095238  0.1117937  0.005436508 0.8728571
            R9
1 -0.017966667
2  0.009780952

Clustering vector:
 [1] 2 1 2 2 2 2 2 2 2 2 2 2 2 2 2 2 1 2 2 2 2 2 2 1 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2
[46] 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2

Within cluster sum of squares by cluster:
[1] 0.495665 3.595726
 (between_SS / total_SS =  57.3 %)
```

If you’re satisfied with your results, you can add them to the original data frame:

```
banks$\textdollar$pam2 = banks_pam$\textdollar$clustering
banks$\textdollar$km2 = banks_kmeans$\textdollar$cluster
```

PAM and kMeans work by assigning observations into a set of discrete categories, the number of which (*k*) is assigned by the user. Like `hclust`, there are several algorithms to choose from, so explore those options with `?kmeans` and `?pam`.

Both `pam` and `kmeans` objects have ready-to-use plotting functions that allow you to compare the results of the cluster with actual values on an nMDS plot. If you’re looking for a production plot, you’re probably better off doing it manually in `ggplot2`, but both of these functions work fine for quick validation work. 

For `pam` results, the `plot function provides two plots, the first of which has what we’re looking for. Note that the y-axis is reversed as compared to the other nMDS plots we’ve seen:

```
plot(banks_pam, which.plot=1, xlab="", ylab="")
```

![neatmap hclust](images/0815OS_05_11.png)

The `useful` package allows you to plot a kmeans object via a simplified ggplot:

```
plot(banks_kmeans, data=banks, class="Solvente", xlab="", ylab="")
```

![neatmap hclust](images/0815OS_05_12.png)

## How to choose an optimal number of clusters with bootstrapping

The `clusGap` function in the `cluster` package compares a set of observed cluster sizes with a bootstrapped sample to evaluate the difference between actual and expected for a given number of clusters. That distance is the Gap statistic; the mathematically optimal number of clusters according to this approach is the smallest number within one standard error (`SE.sim`) of the smallest gap’s value (`gap`). You can specify whether to use `kmeans` or `pam` in the `FUNcluster` option. 

```
banks_gap = clusGap(banks[,3:11], FUNcluster=pam, K.max=10, B=500)
banks_gap_df = as.data.frame(banks_gap$\textdollar$Tab)
ggplot(banks_gap_df, aes(x=1:nrow(banks_gap_df))) +
    scale_x_discrete(limits=c(1:10)) +
    xlab("Clusters") +
    geom_point(aes(y=gap), size=3) +
    geom_errorbar(aes(ymin=gap-SE.sim, ymax=gap+SE.sim))
```

![bootstrapped clusters](images/0815OS_05_13.png)


## Determining optimal numbers of groups with model-based clustering

Sometimes you actually need to be able to justify your cluster analysis results with more than verbal arguments and intuition. Model-based clustering lets you do formal inference on cluster grouping results using finite mixture density models.  

The `mclust` package provides a simple interface to model-based clustering, and we'll move away from the Spanish banking data and use the famous Wisconsin Breast Tissue database for this recipe. 

```
require(mclust)
breast_cancer = read.table("http://archive.ics.uci.edu/ml
  /machine-learning-databases/breast-cancer-wisconsin
  /breast-cancer-wisconsin.data", sep=",", header=F, col.names=c("Sample", 
  "Clump_Thickness", "Uniformity_Cell_Size", "Uniformity_Cell_Shape", 
  "Marginal_Adhesion", "Single_Epithelial_Cell_Size", "Bare_Nuclei", 
  "Bland_Chromatin", "Normal_Nucleoli", "Mitoses", "Class"))
  
breast_cancer$Diagnosis = ifelse(breast_cancer$Class==2,
  "benign", ifelse(breast_cancer$Class==4, 
  "malignant", "unk"))

bc_wisc =  breast_cancer[,2:10]
```

Running `mclust` is easy. Interpreting the results takes more work. 

```
bc_wisc_mc = Mclust(bc_wisc)
summary(bc_wisc_mc)

----------------------------------------------------
Gaussian finite mixture model fitted by EM algorithm
----------------------------------------------------

Mclust VEV (ellipsoidal, equal shape) model with 8 components:

 log.likelihood   n  df       BIC       ICL
      -5843.056 699 383 -14194.63 -14213.41

Clustering table:
  1   2   3   4   5   6   7   8
104  49 115 184  83  50  77  37
```



## Exploring outliers

Outliers are easy enough to explore when you have three or fewer dimensions, but after that you need ways to detect and explore multivariate outliers. `mvoutlier` is one package that makes this straightforward. We'll first use the Spanish banks data in the `quiebra` data frame:

```
library(mvoutlier)

quiebra_outliers = uni.plot(quiebra[,2:10], symb = T)
```

![](images/mvout_quiebra.png)

The `uni.plot` function provides the numeric and visual results automatically, so if you assign it to an object, you can also attach the outlier designation and distance measures to your data frame:

```
quiebra$mv_outlier = quiebra_outliers$outliers
quiebra$mv_md = quiebra_outliers$md
quiebra$mv_euclidean = quiebra_outliers$euclidean
```

Interactive plots are useful for multivariate outlier exploration. We can use the (newly open) `plotly` library's ability to make a ggplot of the Mahalanobis distance measure interactive, via the `ggplotly` function:

```
p1 = ggplot(quiebra, aes(mv_md, text = paste("Bank:", Numero, 
  "<br> Solvency: ", Output))) +
  xlab("Mahalanobis Distance") + 
  geom_histogram() + 
  facet_wrap(~mv_outlier, ncol=1, scales="free_y") 

ggplotly(p1)
```

![](images/mvout_ly.png)

The `uni.plot` function allows you to pass through plotting options, but not always easily (for example, you'll have to modify the function to change point colors). You can use point assignment via `pch`, however, to show a factor variable. Using the bike sharing data, we can plot the weather-based variables and use the point type to show the weather situation. 

```
uni.plot(bike_share_daily[,10:13], pch=as.numeric(bike_share_daily$weathersit))
```

![](images/mvout_bike.png)

A more recent outlier algorithm is the local outlier factor (lof), with an implementation in the `DMwR` package. You need to specify `k`, the number of neighbors to use when calculating the lof value. 

```
require(DMwR)

quiebra$lof = lofactor(quiebra[,2:10], k=4)

p_lof = ggplot(quiebra, aes(lof, text = paste("Bank:", Numero, 
 "<br> Solvency: ", Output))) +
 xlab("Local Outlier Factor") + 
 geom_histogram() 

ggplotly(p_lof)
```

![](images/lof.png)


## Finding associations in shopping carts

We haven't worked with a data set that's really amenable to association rules, so why not explore shopping cart analysis with a groceries dataset?

The `arules` package has a special function that allows you to read a data set directly in to R as a transaction object, for example, where you have the transaction identifier in one column and a particular item from that transaction in a second column. 

```
require(arules)
require(arulesViz)

basket_trans = read.transactions("http://dmg.org/pmml/pmml_examples/baskets1ntrans.csv", format = "single", sep = ",", cols = c("cardid", "Product"), rm.duplicates=TRUE)
```

As usual, `summary` provides details of the resulting object:

```
summary(basket_trans)

transactions as itemMatrix in sparse format with
 939 rows (elements/itemsets/transactions) and
 11 columns (items) and a density of 0.2708878 

most frequent items:
 cannedveg frozenmeal   fruitveg       beer       fish    (Other) 
       303        302        299        293        292       1309 

element (itemset/transaction) length distribution:
sizes
  1   2   3   4   5   6   7   8 
174 227 218 176  81  38  21   4 

   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. 
   1.00    2.00    3.00    2.98    4.00    8.00 

includes extended item information - examples:
      labels
1       beer
2 cannedmeat
3  cannedveg

includes extended transaction information - examples:
  transactionID
1        100047
2        100255
3        100256
```

We can see that there are only 11 distinct items in this data set (must be a very small store!) from 939 distinct transactions, so we can plot all items by frequency using the `itemFrequencyPlot` function. Larger item sets can use the `topN` value to cut off the plot to show just the 15 or 20 or so most common items.

```
itemFrequencyPlot(basket_trans, topN=11, type="absolute") #arules
```

![basket case](images/basket_freq.png)

The `apriori` function takes a transaction object and creates a rule set using default confidence and support levels of 0.8 and 0.1, respectively, and no more than 10 items on any one side of a rule. Accepting the defaults makes the call as simple as `basket_rules = apriori(basket_trans)`, but you will likely want to modify those to expand the rule set. To do so requires setting the `parameter` function with the details in a list. The `minlen` is set at 1, which means that items purchased by themselves will show up--if you want to exclude single-item purchases, set `minlen=2` here. Note that since there are a variety of possible target types (see `?ASparameter-classes`), you need to specify that you are looking for rules. 

```
basket_rules_custom = apriori(basket_trans, parameter = list(support = 0.01, confidence = 0.01, target="rules"))

Apriori

Parameter specification:
 confidence minval smax arem  aval originalSupport support minlen maxlen target
       0.01    0.1    1 none FALSE            TRUE    0.01      1     10  rules
   ext
 FALSE

Algorithmic control:
 filter tree heap memopt load sort verbose
    0.1 TRUE TRUE  FALSE TRUE    2    TRUE

Absolute minimum support count: 9 

set item appearances ...[0 item(s)] done [0.00s].
set transactions ...[11 item(s), 939 transaction(s)] done [0.00s].
sorting and recoding items ... [11 item(s)] done [0.00s].
creating transaction tree ... done [0.00s].
checking subsets of size 1 2 3 4 5 done [0.00s].
writing ... [845 rule(s)] done [0.00s].
creating S4 object  ... done [0.00s].
```

If you had run the `apriori` function with the defaults, you would have seen it created only 3 rules. By opening up the support and confidence levels, we now have 845 rules, which provides us with a much richer exploration space. 

I> *Confidence* is the probability that a transaction including the items on one
I> side of the rule also contains the items on the other side. 
I> *Support* is the proportion of transactions that contain that item or item set. 
I> Higher support = relevant to large proportion of transactions.  

Now we can look at the rules. A key concept for rule usefulness is *lift*. A lift value at or close to 1 suggests that the items or either side of the rule are independent, so are usually of little further interest. Lift values above 1 suggest that the presence of the items on one side of the rule set increase the chances of seeing items on the other side. Where items are all purchased or occur at once (e.g., as in a supermarket), the direction of the rule set is irrelevant. Where items are purchased or occur in a series (e.g., as in a web store, where orders for specific items are placed in order), the rule set is ordered from the first purchased (the `{rhs}`, right hand side), to those placed in the cart later (the `{lhs}`, left hand side). 

We'll look first at the rule sets that generate the greatest lift with the `inspect` function:

```
inspect(head(sort(basket_rules_custom, by="lift"), 10))

    lhs                                       rhs          support    confidence lift    
838 {cannedveg,frozenmeal,fruitveg,wine}   => {beer}       0.01171459 1.0000000  3.204778
802 {beer,cannedmeat,cannedveg,freshmeat}  => {frozenmeal} 0.01171459 1.0000000  3.109272
807 {beer,cannedveg,fish,freshmeat}        => {frozenmeal} 0.01064963 1.0000000  3.109272
817 {beer,cannedmeat,cannedveg,fish}       => {frozenmeal} 0.01277955 1.0000000  3.109272
801 {beer,cannedmeat,freshmeat,frozenmeal} => {cannedveg}  0.01171459 1.0000000  3.099010
806 {beer,fish,freshmeat,frozenmeal}       => {cannedveg}  0.01064963 1.0000000  3.099010
816 {beer,cannedmeat,fish,frozenmeal}      => {cannedveg}  0.01277955 1.0000000  3.099010
821 {beer,cannedmeat,frozenmeal,fruitveg}  => {cannedveg}  0.01064963 1.0000000  3.099010
609 {beer,freshmeat,frozenmeal}            => {cannedveg}  0.03088392 0.9666667  2.995710
833 {cannedveg,fish,frozenmeal,wine}       => {beer}       0.01490948 0.9333333  2.991126
```

We can also plot the rule set using a variety of garph types; accepting the default gives you a scatterplot of confidence, support, and lift, and setting `interactive` to `TRUE` allows you to explore particular subsets based on the combination of those three values. For example, we can select the rules with relatively high values of all three within the graph, and see those rules in the console once we've clicked 'inspect`: 

```
plot(basket_rules_custom, interactive = T)

Interactive mode.
Select a region with two clicks!

Number of rules selected: 3 
    lhs                       rhs          support   confidence lift     order
555 {beer,cannedveg}       => {frozenmeal} 0.1554846 0.8742515  2.718285 3    
556 {cannedveg,frozenmeal} => {beer}       0.1554846 0.8439306  2.704610 3    
554 {beer,frozenmeal}      => {cannedveg}  0.1554846 0.8588235  2.661503 3  
```

![](images/basket_rules.png)

If you wanted to find items that were purchased with, say, beer, you can create a rule set specifically for that item:

```
beer_rules = apriori(data=basket_trans, parameter=list(supp=0.01, conf = 0.01, minlen=2), 
  appearance = list(default="rhs", lhs="beer"))

inspect(head(sort(beer_rules, by="lift"), 10))inspect(head(sort(beer_rules, by="lift"), 10))

   lhs       rhs             support    confidence lift     
9  {beer} => {frozenmeal}    0.18104366 0.5802048  1.8040142
10 {beer} => {cannedveg}     0.17784878 0.5699659  1.7663299
8  {beer} => {fruitveg}      0.09478168 0.3037543  0.9539306
4  {beer} => {cannedmeat}    0.06389776 0.2047782  0.9425818
7  {beer} => {fish}          0.09052183 0.2901024  0.9328978
6  {beer} => {wine}          0.08200213 0.2627986  0.8628249
2  {beer} => {freshmeat}     0.05005325 0.1604096  0.8276075
1  {beer} => {dairy}         0.04792332 0.1535836  0.8147741
3  {beer} => {softdrink}     0.04792332 0.1535836  0.7837773
5  {beer} => {confectionery} 0.06815761 0.2184300  0.7431370
```

You can also create graph diagrams of a rule set, and move the nodes around to show the rule meanings. In this case, only two rules give positive lift, while the other eight show inverse associations. We can adjust the initial graph layout to show this relationship:

```
plot(beer_rules, method="graph", interactive=T)
```

![basket graph](images/basket_graph.png)

To expore the rules in greater depth, you can write them to a text file to view outside of R using the `write` function. 

```
write(basket_rules_custom, file = "basket_rules_custom.csv", sep = ",", row.names=FALSE)
```
