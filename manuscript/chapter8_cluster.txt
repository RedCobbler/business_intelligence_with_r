# Chapter 8: Pattern Discovery and Dimension Reduction

- Mapping multivariate relationships with non-metric multidimensional scaling (nMDS), Principal Components Analysis (PCA), and Correspondence Analysis (CA)
- Grouping observations with hierarchical clustering 
- How to partition the results of a hierarchical cluster analysis
- Identifying and describing group membership with kMeans and PAM
- Determining optimal numbers of groups with model-based clustering
- Identifying group membership with irregular clusters
- Variable selection in cluster analysis
- Error checking cluster results with known outcomes 
- Exploring outliers
- Finding associations in shopping carts


Humans like to put things in boxes. "What things are most like each other?" is one of the most common questions you face in the analysis of multivariate data. Technically, we can think of these tasks as dimension reduction---how can we take a mass of data and reduce it to something that provides insight? Techniques to take many variables and cluster observations or detect outliers have been around for decades, though it’s been only recently where desktop computing power was sufficient for more advanced similarity-analysis tool development. 

 
## Mapping multivariate relationships

Non-metric multidimensional scaling (nMDS) is one such tool. It’s similar to Principal Components Analysis (PCA), but does not suffer from the assumption that the multivariate relationships must be linear. As a result, it has already replaced PCA as the primary descriptive and dimension reduction tool for multivariate data in some fields, and is moving rapidly into others. But although nMDS works better for nearly all applied problems and is easier to explain to business clients, PCA is still widely used. 

### Non-metric multidimensional scaling (nMDS)

Non-metric multidimensional scaling (nMDS) is a useful way to identify groups, outliers, and influential variables in multivariate space. In essence, nMDS collapses multiple dimensions into two or three so that relationships based on multivariate proximity can be visualized. nMDS models a similarity matrix in low dimensional space in much the same way as a map can be created from scattered GPS coordinates.

The data in this example we first saw in Chapter 7---nine financial ratios for 66 Spanish banks, 29 of which failed during the Spanish banking crisis ([Serrano-Cinca 2001](http://www.tandfonline.com/doi/abs/10.1080/13518470122202)). 

The nMDS analysis is largely automated in the `vegan` package using the `metaMDS` function (see `?metaMDS` for all the details). This function defaults to expecting count data; for continuous data, setting the distance metric to euclidean is more appropriate. There are dozens of distance metrics (see `?vegdist` for the options available in `vegan`), but only two are really needed for most applications: Bray-Curtis for count data (the default) and Euclidean for continuous data. `metaMDS` automatically performs transformations when given count data. Manual transformations are sometimes needed if the variables are continuous and have considerably different scales (e.g., scale to mean 0/sd 1, see `?scale`).

```
require(vegan)

banks_mds = metaMDS(banks[,3:11], distance="euclidean", autotransform=FALSE, 
  noshare=FALSE, wascores=FALSE)
 
Run 0 stress 0.03123018 
Run 1 stress 0.03122783 
... New best solution
... procrustes: rmse 0.0007384291  max resid 0.004514075 
*** Solution reached
```

The plot that is built from this algorithm is the heart of nMDS analysis. Categorical information can be added as well. Note that axis values are meaningless in nMDS except as x/y coordinates; only spatial pattern matters for interpretation. Much like a map, points that lie close together on the plot are more similar, while points that are further away are more dissimilar:

```
# You may need to mess with par for the margins. Note: Clearing the 
# plots in RStudio (broom icon) returns par to defaults
# par(mar=c(1.5,0,1,0))

# Save the stress value for the legend
banks.stress = round(banks_mds$stress, digits=2)

# Set up color scheme
groups = ifelse(banks$Solvente == 0, "darkred", "blue")

# Create an empty plot
ordiplot(banks_mds, type="n", xaxt="n", yaxt="n", xlab=" ", ylab=" ", 
  display="sites")

# Add banks by number and color (solvency)
orditorp(banks_mds, display="sites", col=groups, air=0.01, cex=0.75)

# Add legend
legend("topleft", legend=c("Bankrupt", "Solvent"), bty="n", 
  col=c("darkred","blue"), title=paste0("Stress: ", banks.stress), 
  pch=21, pt.bg=c("darkred", "blue"), cex=0.75)
```

![](images/0815OS_05_nmds1.png)

You also can easily explore regions within the plot in more detail by using `xlim` and `ylim` to zoom in on an area of interest, such as the right-side cluster:

```
# Zoom in to the cluster
ordiplot(banks_mds, type="n", xlab=" ", ylab=" ", display="sites", 
  xlim=c(0,0.07), ylim=c(-0.1, 0.095))
  
orditorp(banks_mds, display="sites", col=groups, air=0.01, cex=0.75)

legend("topleft", legend=c("Bankrupt", "Solvent"), bty="n", col=c("darkred",
  "blue"), title=paste0("Stress: ",banks.stress), pch=21, pt.bg=c("darkred", 
  "blue"), cex=0.75)
```

![](images/0815OS_05_nmds2.png)

nMDS does not use the absolute values of the variables, but rather their rank orders. While this loses some information contained in the raw data, the use of ranks omits some of the issues associated with using absolute distance (e.g., sensitivity to transformation and linearity), and as a result is a very flexible technique that works well with a variety of types of data. It's also particularly useful when there are a large number of predictors relative to the number of entities, where using actual values could easily lead to overfitting.

If you need to describe it succinctly, say that nMDS creates a map of a similarity matrix. For example, a matrix of European inter-city distance (see `?eurodist`) creates an nMDS plot that closely reflects the distances and spatial direction between these cities:

```
# You may need to mess with par for the margins
# par(mar=c(1.5,4,1,4))

euromap = metaMDS(eurodist, distance="euclidean", autotransform=FALSE, 
  noshare=FALSE, wascores=FALSE)

euromap$points[,1] = -euromap$points[,1]

ordiplot(euromap, display="sites", xaxt="n", yaxt="n", xlab=" ", ylab=" ", 
  type="t")
```

![](images/0815OS_05_nmds4.png)


### Diagnostics for nMDS results

Diagnostics for nMDS results are also simple. A Shepard plot (also called a stress plot) shows the relationship between the similarity matrix and how well the nMDS algorithm represented that matrix in the plot. The greater the scatter around the (non-linear) regression line, the higher the stress, which would imply that the matrix similarities are not as well represented in the plot.

```
banks_stress = round(banks_mds$stress, digits=2)

banks_stress

0.03

stressplot(banks_mds, xaxt="n", yaxt="n")
```

![](images/0815OS_05_nmds3.png)

The stress value itself is a single value that represents how well the plot represents the underlying similarity matrix. It can be roughly interpreted as follows:

{width="narrow"}
| Stress Score | How well the plot represents the matrix |
| -------------------- |:---------------------------------------:|
| < 0.05 | Excellent |
| > 0.05-0.10 | Very Good |
| > 0.10-0.15 | Good |
| > 0.15-0.20 | Fair |
| > 0.20-0.30 | Poor |
| > 0.30 | No representation |


Based on the diagnostics, this plot represents the similarity matrix quite well, so we can be confident in the nMDS representation of this dataset.

Note: in general, stress values rise with the size of the dataset. Thus, interpreting the final stress score requires considering the size of the dataset in addition to considering the Shepard plot and the patterns revealed in the nMDS plot. When stress is high, sometimes it can be brought down by `metaMDS` options `k` and `trymax`, but the guidelines above should not be considered definitive—sometimes clear patterns can be seen even with relatively higher levels of stress.

The final configuration may differ depending on the noise in the data, the initial configuration, and the number of iterations, so running the nMDS multiple times and comparing the interpretations from the lowest stress results is a good practice. `metaMDS` does this automatically, however, so multiple restarts are only necessary when running nMDS manually.


### Vector mapping influential variables over the nMDS plot

You can add vectors of the variables used to construct the matrix over the nMDS plot to explore how they influenced the final configuration. The length and direction of each vector gives a relative sense of each variable's influence on the "map":

```
# par(mar=c(1.5,0,1,0))

banks_vectors = envfit(banks_mds, banks[,3:11])

ordiplot(banks_mds, type="n", xaxt="n", yaxt="n", xlab=" ", ylab=" ", 
  display="sites")

orditorp(banks_mds, display="sites", col=groups, air=0.01, cex=0.75)

plot(banks_vectors, col="gray40", cex=0.75)

legend("topleft", legend=c("Bankrupt", "Solvent"), bty="n", 
  col=c("darkred","blue"), title=paste0("Stress: ", banks.stress), 
  pch=21, pt.bg=c("darkred", "blue"), cex=0.75)
```

![](images/0815OS_05_nmds5.png)


### Contour mapping influential variables over the nMDS plot

Since there is a clear left-right gradient, and the R8 variable (the ratio of Cost of Sales to Sales) seems to be one of the major reasons for that, a contour of that variable, in that variable’s scale values---in this case, the ratio---can be mapped in the nMDS plot. Other contours can be mapped as well; this example shows R8 and R4 contours: 

```
# par(mar=c(1.5,0,1,0))

# Empty plot
ordiplot(banks_mds, type="n", xaxt="n", yaxt="n", xlab=" ", ylab=" ", 
  display="sites")
  
# Add contour of variable R8 (Cost of Sales:Sales ratio)
# Putting it in tmp suppresses console output
tmp = ordisurf(banks_mds, banks$R8, add=TRUE, col="gray30")

# Add contour of variable R4 (Reserves:Loans ratio)
tmp = ordisurf(banks_mds, banks$R4, add=TRUE, col="gray70")

# Plot nMDS solution
orditorp(banks_mds, display="sites", col=groups, air=0.01, cex=0.75)

legend("topleft", legend=c("Bankrupt", "Solvent"), bty="n", 
  col=c("darkred","blue"), title=paste0("Stress: ",banks.stress), 
  pch=21, pt.bg=c("darkred", "blue"), cex=0.75)
```

![](images/0815OS_05_nmds7.png)

While technically any number of contours can be plotted, in practice more than two renders the plot essentially unreadable.


### Principal Components Analysis (PCA)

PCA is often used as an exploratory technique in similar ways to nMDS, and in some cases where your data have similar scales and there are linear combinations between the variables, it can sometimes prove more powerful. If the assumptions can be justified, PCA has the added benefit of being able to provide dimension reduction through linear combinations of the original variables, in which case it can be used as input to other tools, such as regression.

```
banks_pca = princomp(banks[,2:10], cor=TRUE)

summary(banks_pca, loadings=TRUE)

Importance of components:
                          Comp.1    Comp.2    Comp.3     Comp.4     Comp.5      Comp.6
Standard deviation     2.1811785 1.5982195 1.0433711 0.62850818 0.33908817 0.248945162
Proportion of Variance 0.5286155 0.2838117 0.1209581 0.04389139 0.01277564 0.006885966
Cumulative Proportion  0.5286155 0.8124273 0.9333854 0.97727679 0.99005243 0.996938395
                            Comp.7       Comp.8       Comp.9
Standard deviation     0.161143044 0.0392016948 7.113040e-03
Proportion of Variance 0.002885231 0.0001707525 5.621704e-06
Cumulative Proportion  0.999823626 0.9999943783 1.000000e+00

Loadings:
   Comp.1 Comp.2 Comp.3 Comp.4 Comp.5 Comp.6 Comp.7 Comp.8 Comp.9
R1  0.163 -0.575        -0.190 -0.328  0.115         0.693       
R2        -0.506 -0.452  0.342  0.601 -0.207 -0.105              
R3  0.173 -0.566  0.112 -0.200 -0.254  0.143  0.146 -0.698       
R4  0.201 -0.110  0.831         0.445        -0.228              
R5  0.433  0.141 -0.114 -0.306  0.196         0.344  0.118 -0.715
R6  0.412  0.167 -0.254 -0.300               -0.790 -0.120       
R7  0.434  0.136        -0.308  0.205         0.404         0.693
R8 -0.413 -0.109        -0.570        -0.695                     
R9  0.424                0.451 -0.430 -0.648                     
```

The basic `biplot` function provides the essential details (e.g., `biplot(banks_pca, col=c("black", "gray40"))`) but a function in the `ggbiplot` does the same, only via `ggplot2` for a prettier image:

```
# devtools::install_github("vqv/ggbiplot")

require(ggbiplot)

ggbiplot(banks_pca, labels=rownames(banks), ellipse=T, 
  groups=as.factor(banks$Output)) + 
  theme_bw()
```

![](images/0815OS_05_07.png)

Finally, variance plots often accompany PCA work; here's a way to do it with ggplot:

```
# Get variances
vars = banks_pca$sdev^2

# Get proportion of variance
var_exp = vars/sum(vars)

# Get cumulative variance and make into data frame
variances = data.frame(vars = vars, var_exp = var_exp, 
  var_cumsum = cumsum(var_exp), PCs = 1:length(banks_pca$sdev))

# Plot variance explained and cumulative variance
ggplot(variances, aes(PCs, var_cumsum)) +
  scale_x_discrete(limits=c(1:9)) +
  ylab("Var. Explained (bars) and Cumulative (line) by PC") +
  geom_bar(aes(y=var_exp), stat="identity", fill="gray", alpha=.5) +
  geom_line() +
  theme_bw()
```

![](images/0815OS_05_08.png)


### nMDS for Categories: Correspondence Analysis

Correspondence analysis (CA) is a special case of nMDS, serving essentially the same purpose for categorical variables. It's very useful for visualizing relationships within contingency tables, particularly when mosaic plots are too noisy. There are several packages that can perform CA, but the aptly named `ca` package provides a good, simple way to do it. We'll use the `smoke` dataset built into `ca`, which is a fake dataset on smoking habits (none, light, medium, heavy) among senior managers (SM), junior managers (JM), senior employees (SE), junior employees (JE), and secretaries (SC).

```
require(ca)

smoke_ca = ca(smoke)

summary(smoke_ca)

Principal inertias (eigenvalues):

 dim    value      %   cum%   scree plot               
 1      0.074759  87.8  87.8  **********************   
 2      0.010017  11.8  99.5  ***                      
 3      0.000414   0.5 100.0                           
        -------- -----                                 
 Total: 0.085190 100.0                                 

Rows:
    name   mass  qlt  inr    k=1 cor ctr    k=2 cor ctr  
1 |   SM |   57  893   31 |  -66  92   3 | -194 800 214 |
2 |   JM |   93  991  139 |  259 526  84 | -243 465 551 |
3 |   SE |  264 1000  450 | -381 999 512 |  -11   1   3 |
4 |   JE |  456 1000  308 |  233 942 331 |   58  58 152 |
5 |   SC |  130  999   71 | -201 865  70 |   79 133  81 |

Columns:
    name   mass  qlt  inr    k=1 cor ctr    k=2 cor ctr  
1 | none |  316 1000  577 | -393 994 654 |  -30   6  29 |
2 | lght |  233  984   83 |   99 327  31 |  141 657 463 |
3 | medm |  321  983  148 |  196 982 166 |    7   1   2 |
4 | hevy |  130  995  192 |  294 684 150 | -198 310 506 |

plot(smoke_ca, arrows=c("F","T"), mass = c(TRUE, TRUE))
```

![](images/0815OS_05_ca.png)
 

## Cluster analysis

Although it's a relatively old tool, cluster analysis has advanced with the development of techniques such as clustering irregular groupings by density via DBSCAN, and being able to make inferential claims on cluster membership with model-based clustering. Still, the old standbys of kMeans, PAM, and hierarchical clustering all remain excellent first-order descriptive tools for multivariate data. 

### Grouping observations with hierarchical clustering

Hierarchical clustering can be a useful way to summarize high-dimensional data, but results can be strongly dependent on the clustering method. Ward’s, average, and centroid methods tend to be the best places to start. 

Hierarchical cluster analysis tools are built into the `base` installation, so no specific packages are required here. 

Typically, variables to cluster on are scaled (usually to mean 0 and sd 1), a distance matrix is created on the scaled data, and a clustering method is applied to the distance matrix. Since the set of descriptive variables in this data set are all on the same scale (i.e., a set of ratios), we don't need to scale this data. We'll use Ward's clustering method here with the Spanish banks dataset, which aims to create compact clusters by minimizing variance.

```
banks_dist = dist(banks[,3:11])

banks_hclust = hclust(banks_dist, method="ward.D")

plot(banks_hclust, hang=-1, labels=paste(banks$BANCO, row.names(banks), sep="-"))
```

![](images/0815OS_05_01.png)

See `?hclust` for a short overview of the clustering methods available. The method and approach you use are important---changes in either will change the result (see below for an example).  


### Plotting a cluster dendrogram with ggplot

The `NeatMap` package has a helper function (`draw.dendrogram`) that allows you to place an `hclust` object in `ggplot2`, where you can style or label it from there however you wish, e.g.:

```
require(NeatMap)

ggplot() +
  draw.dendrogram(banks_hclust) +
  scale_color_manual(name="Status: ", values=c("darkred", "blue"),
    labels=c("Bankrupt ", "Solvent ")) +
  geom_text(aes(x=0.75, y=banks_hclust$order, label=row.names(banks), 
    color=factor(banks$Output)), size=4) +
  ggtitle("Cluster Dendrogram (Ward's) - Spanish Banking Crisis") +
  theme(panel.grid.major = element_blank(), panel.grid.minor = element_blank(),
    panel.border = element_blank(), panel.background = element_blank(),
    axis.title = element_blank(), axis.text = element_blank(),
    axis.ticks = element_blank(), legend.position="top")
```

![](images/0815OS_05_08_old.png)

As we mentioned above, a small change in approach can completely alter the result. Say we had scaled the data first. 

```
banks_scaled = scale(banks[,3:11])

banks_dist = dist(banks_scaled)

banks_hclust = hclust(banks_dist, method="ward.D")

# Now rerun the ggplot commands as above
```

![](images/0815OS_05_09.png)

The clear structure seen above in the unscaled data is now somewhat muddled. 


### Exploring hierarchical clustering of nMDS results

`NeatMap`'s `draw.dendrogram3d` function allows you to plot an `hclust` object over your nMDS results, and explore it interactively in 3D in a new window; a screenshot of this view is shown below the code:

```
require(NeatMap)

banks_pos = banks_mds$points # nMDS results from above

draw.dendrogram3d(banks_hclust, banks_pos, labels=row.names(banks), 
  label.colors=banks$Output+3, label.size=1)
```

![](images/0815OS_05_10.png)


### How to partition the results of a hierarchical cluster analysis

The `cutree` function will cut a hierarchical cluster dendrogram into a user-supplied number of groups, which can be useful when combined with the results of a `clusGap`-based PAM or kMeans analysis (see the recipes later in this chapter). Since we know there are two natural clusters---bankrupt and solvent---we’ll use *k*=2 here. Note that since it is cutting the tree based on the clustering method’s distance values, results from `cutree` may not match the clusters obtained with `pam` or `kmeans`.

Using our original `hclust` object:

```
# Run cutree for 2 groups
bank_clusters = cutree(banks_hclust, k=2)

# View results
table(bank_clusters)

bank_clusters
 1  2
62  4

# Show variable medians for each group
aggregate(banks[,2:10], by=list(cluster=bank_clusters), median)

  cluster      R1      R2      R3     R4      R5      R6      R7      R8       R9
1       1 0.39265 0.26210 0.40725 0.0203  0.0042  0.1002  0.0044 0.87650  0.00955
2       2 0.36585 0.23295 0.38455 0.0152 -0.0380 -0.9390 -0.0397 1.30815 -0.01630

# Plot dendrogram
plot(banks_hclust, hang=-1, labels=paste(banks$BANCO, row.names(banks), sep="-"), 
   cex=0.9)

# Add cluster boxes to dendrogram
rect.hclust(banks_hclust, k=2)
```

![](images/0815OS_05_14.png)


### Identifying and describing group membership with kMeans and PAM

kMeans and Partitioning Around Medoids (PAM) are two methods that will divide your observations into *k* groups. kMeans works with quantitative data only, and can be susceptible to outliers, while PAM works with both quantitative as well as categorical data. As medoids are simply the “most representative” member of a cluster, outliers don’t really affect PAM. As a result, it is a more robust version of kMeans. If you have a really large data set, look at the `clara` package, which performs PAM by iteratively sampling the data to speed up analysis time.

The `cluster` package contains the basic PAM function, so load that first. We’ll continue to use the Spanish banking data.

```
require(cluster)
```

To run either analysis, you start with the number of groups (*k*) in which you wish to partition the data. We know there are two natural categories in this data set—banks that went bankrupt and those that remained solvent, so we can use those factors to check the results of the analysis. Often you don’t have natural groups in your data, and you’re not sure how many groups there should be. That topic is briefly touched on in the next recipe with `clusGap` and at more length later in this chapter in the recipes on  bootstrapping and model-based clustering. 

To run PAM with *k*=2:

```
banks_pam = pam(banks[,2:10], k=2)

banks_pam

Medoids:
     ID R1Liqui1 R2Liqui2 R3Liqui3 R4Autof R5RenEc R6RenFin  R7Apal R8CosVen
[1,] 58   0.4074   0.2764   0.4397  0.0156  0.0043   0.0582  0.0046   0.8846
[2,] 17   0.3740   0.2205   0.3938  0.0012 -0.0537  -1.0671 -0.0565   1.3713
      R9Cash
[1,]  0.0095
[2,] -0.0231
Clustering vector:
 [1] 1 2 1 1 1 1 1 1 1 1 1 1 1 1 1 1 2 1 1 1 1 1 1 2 1 1 1 1 1 1 1 1 1 1 1 1 1
[38] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1
Objective function:
   build     swap 
0.219742 0.219742 

Available components:
 [1] "medoids"    "id.med"     "clustering" "objective"  "isolation" 
 [6] "clusinfo"   "silinfo"    "diss"       "call"       "data"     
 ```

To run kMeans with *k*=2:

```
banks_kmeans = kmeans(banks[,2:10], centers=2, nstart=50)

banks_kmeans

K-means clustering with 2 clusters of sizes 63, 3

Cluster means:
   R1Liqui1  R2Liqui2  R3Liqui3    R4Autof      R5RenEc   R6RenFin
1 0.3952127 0.2674238 0.4173810 0.02471746  0.005095238  0.1117937
2 0.3409333 0.2465000 0.3561333 0.01186667 -0.052733333 -1.2071667
        R7Apal  R8CosVen       R9Cash
1  0.005436508 0.8728571  0.009780952
2 -0.055200000 1.2808000 -0.017966667

Clustering vector:
 [1] 1 2 1 1 1 1 1 1 1 1 1 1 1 1 1 1 2 1 1 1 1 1 1 2 1 1 1 1 1 1 1 1 1 1 1 1 1
[38] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1

Within cluster sum of squares by cluster:
[1] 3.595726 0.495665
 (between_SS / total_SS =  57.3 %)

Available components:

[1] "cluster"      "centers"      "totss"        "withinss"     "tot.withinss"
[6] "betweenss"    "size"         "iter"         "ifault" 
```

If you’re satisfied with your results, you can add them to the original data frame:

```
banks$pam2 = banks_pam$clustering

banks$km2 = banks_kmeans$cluster
```

PAM and kMeans work by assigning observations into a set of discrete categories, the number of which (*k*) is assigned by the user. Like `hclust`, there are several algorithms to choose from, so explore those options with `?kmeans` and `?pam`.

If you do have known outcome values, a table of actual vs. modeled clusters can give you a sense of each algorithm's performance on your data. 

```
table(banks$Output, banks$pam2)
   
     1  2
  0 26  3
  1 37  0
  
table(banks$Output, banks$km2)
   
     1  2
  0 26  3
  1 37  0
```

Both `pam` and `kmeans` objects have ready-to-use plotting functions that allow you to compare the results of the cluster with actual values on an nMDS plot. If you’re looking for a production plot, you’re probably better off doing it manually in `ggplot2`, but both of these functions work fine for quick validation work. 

For `pam` results, the `plot` function provides two plots, the first of which has what we’re looking for. Note that the y-axis is reversed as compared to the other nMDS plots we’ve seen:

```
plot(banks_pam, which.plot=1, xlab="", ylab="")
```

![](images/0815OS_05_11.png)

The `useful` package allows you to plot a kmeans object via a simplified ggplot:

```
require(useful)

plot(banks_kmeans, data=banks, class="Output", xlab="", ylab="")
```

![](images/0815OS_05_12.png)

### How to choose an optimal number of clusters with bootstrapping

The `clusGap` function in the `cluster` package compares a set of observed cluster sizes with a bootstrapped sample to evaluate the difference between actual and expected for a given number of clusters. That distance is the Gap statistic; the mathematically optimal number of clusters according to this approach is the smallest number within one standard error (`SE.sim`) of the smallest gap’s value (`gap`). You can specify whether to use `kmeans` or `pam` in the `FUNcluster` option. 

```
banks_gap = clusGap(banks[,2:10], FUNcluster=pam, K.max=10, B=500)

banks_gap_df = as.data.frame(banks_gap$Tab)

ggplot(banks_gap_df, aes(x=1:nrow(banks_gap_df))) +
  scale_x_discrete(limits=c(1:10)) +
  xlab("Clusters") +
  geom_point(aes(y=gap), size=3) +
  geom_errorbar(aes(ymin=gap-SE.sim, ymax=gap+SE.sim))
```

![](images/0815OS_05_13.png)

The two cluster solution is clearly the best option.


### Determining optimal numbers of clusters with model-based clustering

Sometimes you actually need to be able to justify your cluster analysis results with more than verbal arguments and intuition. Model-based clustering lets you do formal inference on cluster grouping results using finite mixture density models. Even better, it can pick up clusters that traditional kmeans or hierarchical clustering fail at, because of the flexibility of its clustering shapes. 

The `mclust` package provides a simple interface to model-based clustering. For the sake of example, we'll use the `diabetes` dataset that comes with the `mclust` package. 

```
require(mclust)
data("diabetes")

diabetes_mclust = Mclust(diabetes[,2:4])

summary(diabetes_mclust)
```
{lang="text"}
```
----------------------------------------------------
Gaussian finite mixture model fitted by EM algorithm 
----------------------------------------------------

Mclust VVV (ellipsoidal, varying volume, shape, and orientation) model with 3 components:

 log.likelihood   n df       BIC       ICL
      -2307.883 145 29 -4760.091 -4776.086

Clustering table:
 1  2  3 
82 33 30 
```

Running the main function, `Mclust`, is easy. Interpreting the results takes more work. There are currently 20 possible model types in this package, which allows for a huge amount of flexibility in cluster shapes, something essential for oddly-distributed data (which is exactly why we have used the `diabetes` dataset here). These are summarized below (for more details, run `?mclustModelNames`).

| Model Name | Model Type | 
| ---------- | ---------- |
| **Univariate** | | 
| "E"	|	equal variance (one-dimensional) | 
| "V"	|	variable variance (one-dimensional) |
| | | 
| **Multivariate** | |
| "EII"	|	spherical, equal volume |
| "VII"	|	spherical, unequal volume |
| "EEI"	|	diagonal, equal volume and shape |
| "VEI"	|	diagonal, varying volume, equal shape |
| "EVI"	|	diagonal, equal volume, varying shape |
| "VVI"	|	diagonal, varying volume and shape |
| "EEE"	|	ellipsoidal, equal volume, shape, and orientation |
| "EVE"	|	ellipsoidal, equal volume and orientation |
| "VEE"	|	ellipsoidal, equal shape and orientation |
| "VVE"	|	ellipsoidal, equal orientation |
| "EEV"	|	ellipsoidal, equal volume and equal shape |
| "VEV"	|	ellipsoidal, equal shape |
| "EVV"	|	ellipsoidal, equal volume |
| "VVV"	|	ellipsoidal, varying volume, shape, and orientation |
| | | 
| **Single Component**	|  	|
| "X"	|	univariate normal |
| "XII"	|	spherical multivariate normal |
| "XXI"	|	diagonal multivariate normal |
| "XXX"	|	ellipsoidal multivariate normal |


By looking at the BIC table as well as plotting the modeling results, we can see why the algorithm chose the **VVV** model type:

```
diabetes_mclust$BIC

Bayesian Information Criterion (BIC):
        EII       VII       EEI       VEI       EVI       VVI       EEE
1 -5863.923 -5863.923 -5530.129 -5530.129 -5530.129 -5530.129 -5136.446
2 -5449.518 -5327.719 -5169.399 -5019.350 -5015.884 -4988.322 -5010.994
3 -5412.588 -5206.399 -4998.446 -4899.759 -5000.661 -4827.818 -4976.853
4 -5236.008 -5208.512 -4937.627 -4835.856 -4865.767 -4813.002 -4865.864
5 -5181.608 -5202.555 -4915.486 -4841.773 -4838.587 -4833.589 -4882.812
6 -5162.164 -5135.069 -4885.752        NA -4848.623 -4810.558 -4835.226
7 -5128.736 -5129.460 -4857.097        NA -4849.023        NA -4805.518
8 -5135.787 -5135.053 -4858.904        NA -4873.450        NA -4820.155
9 -5150.374 -5112.616 -4878.786        NA -4865.166        NA -4840.039
        EVE       VEE       VVE       EEV       VEV       EVV       VVV
1 -5136.446 -5136.446 -5136.446 -5136.446 -5136.446 -5136.446 -5136.446
2 -4875.633 -4920.301 -4877.086 -4918.500 -4834.727 -4823.779 -4825.027
3 -4858.851 -4851.667 -4775.537 -4917.567 -4809.225 -4817.884 -4760.091
4 -4793.261 -4840.034 -4794.892 -4887.406 -4823.882 -4828.796 -4802.420
5        NA        NA        NA -4908.030 -4842.077        NA        NA
6        NA        NA        NA -4844.584 -4826.457        NA        NA
7        NA        NA        NA -4910.155 -4852.182        NA        NA
8        NA        NA        NA -4858.974 -4870.633        NA        NA
9        NA        NA        NA -4930.535 -4887.206        NA        NA

Top 3 models based on the BIC criterion:
    VVV,3     VVE,3     EVE,4 
-4760.091 -4775.537 -4793.261 

# Model comparison plot
plot(diabetes_mclust, what="BIC")
```

![](images/diabetes_BIC.png)

We can see that the **VVV** model is definitely the best among the top three. 

Now we can plot the clusters over a pairs plot of the variables:

```
# Cluster results 
plot(diabetes_mclust, what="classification")
```

![](images/diabetes_class.png)


We can explore the bivariate density of the original data through this package as well, with some simple code:

```
# Bivariate density (basic contour plot)
plot(diabetes_mclust, what="density")

# Bivariate density (perspective plot)
plot(diabetes_mclust, what = "density", type = "persp", 
  border = adjustcolor(grey(0.01), alpha.f = 0.15))
```

![](images/diabetes_density.png)

![](images/diabetes_persp_light.png)

Understanding the uncertainty associated with specific observations and their cluster assignment is a useful diagnostic tool as well.

```
# Cluster uncertainty plot
plot(diabetes_mclust, what="uncertainty")
```

![](images/diabetes_uncert.png)

This graph shows us *where* the uncertainty occurs, but it doesn't tell us which observations are the most uncertain. We can plot the uncertainty values in an interactive setting for more directed exploration. We'll use the `googleVis` package here:

```
# Add classification and uncertainty to data set
diabetes$mclust_cluster = diabetes_mclust$classification
diabetes$mclust_uncertainty = diabetes_mclust$uncertainty

# Plot the uncertainty as a histogram and table
require(googleVis)

diabetes_histogram = gvisHistogram(data.frame(diabetes$mclust_uncertainty), 
  options=list(width=800, height=300, legend="none",
  title="Distribution of Mclust Uncertainty Values"))

diabetes_table = gvisTable(diabetes, options=list(width=800, height=300))

HT = gvisMerge(diabetes_histogram, diabetes_table) 

plot(HT)
```

![](images/diabetes_interactive.png)


### Identifying group membership with irregular clusters

Sometimes the clusters we want to identify have clear patterns but consist of non-linear or irregular groupings---things we can identify readily by eye but cannot cluster with the usual methods. The DBSCAN algorithm was built for this purpose. We'll use a synthetic dataset from the `multishapes` package to demonstrate exactly what this means. 

```
data("multishapes", package="factoextra")

ggplot(multishapes, aes(x, y)) +
  geom_point(aes(shape=as.factor(shape), 
    color=as.factor(shape))) +
  scale_shape_discrete(name="Known\nGrouping", labels = 
    levels(as.factor(multishapes$shape))) +
  scale_color_discrete(name="Known\nGrouping", labels = 
    levels(as.factor(multishapes$shape))) +
  theme_bw()
```

![](images/dbscan_data.png)

The patterns are clear to your eye, but the methods we've explored above completely fail in identifying them. The `dbscan` function in the package of the same name *can* identify these groupings. 

The algorithm requires you choose a value for the &epsilon; (*epsilon*) parameter. The `kNNdistplot` function can help you choose this value; you should try a variety of values close to the bend in the resulting line to explore how changes in this value affect the outcome. *k* is usually taken to be the number of variables you have plus one, though you can also play with this option as a way to explore the sensitivity of any solution. 

```
require(dbscan)

# Estimate eps with kNN distance plot
kNNdistplot(multishapes[,1:2], k=3)

# Try an eps value of 0.15
abline(h=0.15, col="blue", lty=3)
```

![](images/dbscan_knn.png)

Once you have chosen `eps`, run it through the main `dbscan` function:

```
multishapes_dbscan = dbscan(multishapes[,1:2], eps = 0.15, minPts = 3)

multishapes_dbscan

DBSCAN clustering for 1100 objects.
Parameters: eps = 0.15, minPts = 3
The clustering contains 5 cluster(s) and 25 noise points.

  0   1   2   3   4   5 
 25 411 406 106 100  52 

Available fields: cluster, eps, minPts
```

We can add the modeled cluster assignment to the original data, and compare the DBSCAN assignment (columns) against the known groupings in the data (`multishapes$shape`, here designated in the rows):

```
multishapes$dbscan = multishapes_dbscan$cluster

table(multishapes$shape, multishapes$dbscan)

      0   1   2   3   4   5
  1   1 398   1   0   0   0
  2   0   0 400   0   0   0
  3   0   0   0 100   0   0
  4   1   0   0   0  99   0
  5  23  13   5   6   1   2
  6   0   0   0   0   0  50
```

We can see the outcome and the known values together graphically:

```
ggplot(multishapes, aes(x, y)) +
  geom_point(aes(color=as.factor(dbscan), shape=as.factor(shape))) +
  labs(color="DBSCAN\nGrouping", shape="Known\nGrouping") +
  theme_bw()
```

![](images/dbscan_results.png)


### Variable selection in cluster analysis

We often find that we have many, many variables, and are not sure of the extent to which they add value or noise to the clustering process. The `clustvarsel` package---created by the same folks who created `mclust`---provides a way to assess this.

We'll use the famous Wisconson Breast Cancer data set, which consists of nine diagnostic cell features measured on a 1-10 ordinal scale that was automatically computed from biopsy tissue images (see [this paper](http://dollar.biz.uiowa.edu/~street/research/hu_path95/hp95.pdf) for more details). Higher values are linked with higher chances of a tumor being malignant, and lower values are more likely from benign tissue.  

```
require(clustvarsel)

# Download the Wisconsin Breast Cancer data
breast_cancer = read.table("http://archive.ics.uci.edu/ml/
  machine-learning-databases/breast-cancer-wisconsin/
  breast-cancer-wisconsin.data", sep=",", na.strings = 
  c("NA", "?"), header=F, col.names=c("Sample", 
  "Clump_Thickness", "Uniformity_Cell_Size", 
  "Uniformity_Cell_Shape", "Marginal_Adhesion", 
  "Single_Epithelial_Cell_Size", "Bare_Nuclei", 
  "Bland_Chromatin", "Normal_Nucleoli", "Mitoses", "Class"))

# Label the outcomes
breast_cancer$Diagnosis = ifelse(breast_cancer$Class==2, "benign", 
  ifelse(breast_cancer$Class==4, "malignant", "unk"))

# Remove incomplete rows, then create data-only object
bc_wisc = na.omit(breast_cancer[,c(2:10, 12)])

bc_wisc_data = bc_wisc[,1:8]

# Perform variable selection (with parallel processing)
varsel_fwd = clustvarsel(bc_wisc_data, parallel = TRUE)

varsel_fwd
```
{lang="text"}
```
Stepwise (forward/backward) greedy search
----------------------------------------- 
              Variable proposed  Type of step  BIC difference  Decision
1                   Bare_Nuclei           Add      3577.27017  Accepted
2               Normal_Nucleoli           Add     -1189.74368  Accepted
3               Bland_Chromatin           Add       619.81296  Accepted
4               Bland_Chromatin        Remove       247.84644  Rejected
5          Uniformity_Cell_Size           Add        67.87812  Accepted
6               Bland_Chromatin        Remove       493.01606  Rejected
7   Single_Epithelial_Cell_Size           Add      1122.53709  Accepted
8          Uniformity_Cell_Size        Remove     -1393.18387  Accepted
9               Clump_Thickness           Add      -474.17028  Rejected
10              Bland_Chromatin        Remove      1978.99023  Rejected

Selected subset: Bare_Nuclei, Normal_Nucleoli, Bland_Chromatin,
  Single_Epithelial_Cell_Size
```

Based on BIC values, the algorithm selected only four of the nine variables that should be used for clustering. We'll now take those variables and run model-based clustering using `Mclust` on that reduced data set. 

```
# Create data frame of the date with only the chosen variables
bc_fwd = bc_wisc_data[,varsel_fwd$subset]

# Run Mclust on subsetted data frame
bc_mclust_fwd = Mclust(bc_fwd)

# Add clustering result to data
bc_wisc$mclust_fwd_cluster = bc_mclust_fwd$classification

# Review Mclust results
summary(bc_mclust_fwd)
```
{lang="text"}
```
----------------------------------------------------
Gaussian finite mixture model fitted by EM algorithm 
----------------------------------------------------

Mclust VII (spherical, varying volume) model with 2 components:

 log.likelihood   n df       BIC       ICL
      -4492.643 683 11 -9057.077 -9068.989

Clustering table:
  1   2 
399 284 
```

We will compare the results from this reduced clustering to a clustering using the full variable set in the next recipe. 


W> **Cluster analysis results are strongly dependent on the method used.**  
W> 
W> If you run all of the clustering algorithms in this chapter on the same dataset, you will find sometimes similar and sometimes very different outcomes. There are lots of ways you can cluster data, but no method can substitute for careful comparison of a variety of results against subject matter knowledge.  
W> 
W> For example, if you run the variable selection algorithm from the full set of variables, removing variables one-by-one *backwards*, you'll get different (and poorer) results than seen above with the default "forward" algorithm.  
W> 
W> Some sort of senstitivity analysis of cluster analysis results is *always* warranted.  


### Error-checking cluster results with known outcomes 

When you have known outcomes in your data (e.g., benign, malignant), the `mclust` package has two functions that help you evaluate your results. 

`classError` provides an overall error rate:

```
classError(bc_wisc$Diagnosis, bc_wisc$mclust_fwd_cluster)

$misclassified
 [1]   2   4   7  37  49  71  74  79  82 109 114 117 128 134 141 145 191 227
[19] 229 237 242 245 252 286 287 298 306 307 333 339 349 390 401 405 417 420
[37] 427 447 480 493 539 541 606 610 612 635 642

$errorRate
[1] 0.06881406
```

And `adjustedRandIndex` provides the adjusted Rand Index value, where 1 = perfect classification and 0 = random classification:

```
adjustedRandIndex(bc_wisc$Diagnosis, bc_wisc$mclust_fwd_cluster)

0.7426946
```

You can compare different clustering algorithms to see which performs better. 

```
# Run Mclust on the full variable set of the Wisconsin Breast Cancer data
bc_mclust = Mclust(bc_wisc_data)
bc_wisc$mclust_all = bc_mclust$classification

classError(bc_wisc$Diagnosis, bc_wisc$mclust_all)

$misclassified
  [1]   2   3   4   7  14  24  37  44  46  60  63  71  74  76  77  79  82  89
 [19]  92  94 107 109 113 114 115 117 118 120 124 128 134 141 144 145 150 159
 [37] 160 166 167 188 191 192 197 199 207 211 212 220 227 229 234 237 238 239
 [55] 242 245 252 258 262 269 270 283 286 287 290 292 295 296 298 306 307 311
 [73] 312 314 319 324 328 333 334 339 341 349 350 357 363 365 366 369 372 374
 [91] 376 389 390 391 392 394 395 397 401 403 404 405 406 408 412 414 416 417
[109] 420 427 428 429 430 440 447 467 471 480 491 493 526 528 537 538 539 541
[127] 548 553 559 563 564 565 570 585 599 606 610 612 622 635 636 642 645 658
[145] 659 672 679

$errorRate
[1] 0.2152269


adjustedRandIndex(bc_wisc$Diagnosis, bc_wisc$mclust_all)

[1] 0.3827008
```

It seems that the use of all of the variables contributes some noise to the classification as compared with the subset chosen by forward variable selection.


## Exploring outliers

On the flip side of the coin of similarity is the exploration of outliers. If anyone has ever asked you for an "automatic outlier detection" data product, you immediately realized that they probably don't understand analytics. What counts as an outlier is completely dependent on context. That said, sometimes you need to explore what might count as an outlier across enough variables that visualization becomes difficult, if not impossible. Other times, you might want to get a head-start on investigating a potential outlier or trend before it wrecks things. Or perhaps you need to understand the probabilities associated with unusual but possible events. Multivariate outlier exploration, extreme value analysis, and anomaly detection are useful exploratory tools for these purposes. 

Outliers are easy enough to explore with standard EDA methds when you have three or fewer dimensions, but after that you need ways to detect and explore multivariate outliers. `mvoutlier` and `lof` are two packages that make it fairly simple. But don't ever forget that what constitutes a *real* outlier is completely contextual; think of these tools as a way to explore, not as a way to define. 

### Identifying outliers with distance functions

`mvoutlier` is one package that makes multivariate outlier identification straightforward. We'll first use the Spanish banks data to illustrate some of it, and later will use the bike sharing data to illustrate other components.

```
require(mvoutlier)

quiebra_outliers = uni.plot(banks[,2:10], symb = T)
```

![](images/mvout_quiebra.png)

The `uni.plot` function provides the numeric and visual results automatically, so if you assign it to an object, you can also attach the outlier designation and distance measures to your data frame:

```
# Assign outlier grouping to main data
banks$mv_outlier = quiebra_outliers$outliers

# Add Mahalanobis distance results to main data
banks$mv_md = quiebra_outliers$md

# Add Euclidean distance results to main data
banks$mv_euclidean = quiebra_outliers$euclidean
```

Interactive plots are useful for multivariate outlier exploration---static plots like the one above show us that there are multivariate outliers, but they don't tell us which observations they are. Tables can, of course, but at the expense of time. We can use the (newly open-sourced) `plotly` library's ability to make a ggplot of the Mahalanobis distance measure interactive, via the `ggplotly` function:

```
require(ggplot2)
require(plotly)
  
p1 = ggplot(banks, aes(mv_md, text = paste("Bank: ",
    row.names(banks), "<br>Solvency: ", Output))) +
  xlab("Mahalanobis Distance") + 
  geom_histogram() + 
  facet_wrap(~mv_outlier, ncol=1, scales="free_y") 

ggplotly(p1)
```

![](images/mvout_ly.png)

The `uni.plot` function allows you to pass through plotting options, but not always easily---for example, you'll have to modify the function to change point colors. You can use point assignment via `pch`, however, to show a factor variable. 

For our second `mvoutlier` example, we'll use the bike sharing data from previous chapters. We can plot the weather-based variables and use the point type to show the weather situation relative to potential outlier observation. The values of the weather situation are mapped to the matching `pch` values, so `pch=1`, a circle, represents observations under clear weather, and so on, while the outlier status is mapped to red (outlier) or green (not an outlier).  

```
uni.plot(bike_share_daily[,10:13], pch=as.numeric(bike_share_daily$weathersit))
```

![](images/mvout_bike.png)

### Identifying outliers with the local outlier factor

A more recently-developed outlier algorithm is the local outlier factor (lof), with an implementation in the `DMwR` package. You need to specify `k`, which here refers to the number of neighbors to use when calculating the lof value. 

```
require(DMwR)

# Calculate the local outlier factor values
banks$lof = lofactor(banks[,2:10], k=4)

# Plot the lof results interactively
p_lof = ggplot(banks, aes(lof, text = paste("Bank: ",
    row.names(banks), "<br>Solvency: ", Output))) +
  xlab("Local Outlier Factor") + 
  geom_histogram() 

ggplotly(p_lof)
```

![](images/lof.png)


### Anomaly detection

Twitter has come up with an anomaly detection algorithm for vectors, or for even things like time series that have cyclical patterns. (It's not on CRAN, so install it from Github: `devtools::install_github("twitter/AnomalyDetection"`.) 

The vignette has a great built-in demo dataset---presumably tweets per minute---so we'll use that here. 

```
require(AnomalyDetection)
data(raw_data)

# Run the algorithm, include expected values and plotting features
raw_data_anomalies = AnomalyDetectionTs(raw_data, direction='both', 
  e_value=TRUE, plot=TRUE)

# Plot the time series and the anomalies
raw_data_anomalies$plot
```

![](images/anomaly.png)

The algorithm defaults to determining no more than 10% of the data will be classified as anomalous. Use `max_anoms=0.01`, for example, if you wish to change that to 1% (or any other percent you deem important). 

You can use the `threshold` option to identify values *above* the median, 95th, or 99th percentiles of daily maximums. There's no equivalent option for values *below* a threshold, unfortunately---using it on this data misses the low end anomalies. 

Finally, you can extract the data with the expected values by pulling it from the list, e.g., `raw_data_anomalies_df = raw_data_anomalies$anoms`. 


### Extreme value analysis

Extreme value analysis (EVA) is a different way to explore outliers; by acknowledging that outliers are real and can be incredibly useful for analysis, we can better assess the likelihood of "improbable" events. 

There are a lot of packages in R that can do EVA, though some remain pretty technical in their documentation, when they have documentation at all. A great entry point for EVA in R is through the `extremeStat` package, which wraps around other R packages to allow for semi-automated distribution fitting and return-interval analysis. 

We'll use data on economic damages in the United States caused by hurricanes between 1926 and 1995. The data are for each major hurricane, so we'll use `dplyr` to aggregate those to yearly sums and to insert 0 for years in which there was no major damage. 

```
require(dplyr) 

data("damage", package="extRemes")

hurricane_cost = damage %>% group_by(Year) %>% summarise(total_damage = sum(Dam))

yearz = data.frame(Year = seq(1926, 1995, 1))

hurricane_cost = full_join(yearz, hurricane_cost)

hurricane_cost$total_damage[is.na(hurricane_cost$total_damage)] = 0
```

Now that the data is complete, we can use the `distLfit` function from `extremeStat` to fit a variety of extreme-value-oriented probability models to our data, and view the results:

```
require(extremeStat)

hurr_fit = distLfit(hurricane_cost$total_damage)
```

![](images/eva_hist.png)

A standard histogram is returned from this function call, which isn't as helpful as a CDF plot to compare against the actual values. Use `distLplot` with `cdf=TRUE` to return a CDF plot:

```
distLplot(hurr_fit, cdf=TRUE, main="Total Annual Hurricane Damages
  (in billion USD)", xlab="Damages ($1B USD)")
```

![](images/eva_cdf.png)

Unlike many R packages, `summary` and other universal functions don't work with `extremeStat` result objects because they return lists. So, to see a summary of the result, use `distLprint`.

{lang="text"}
```
distLprint(hurr_fit)

----------
Dataset 'hurricane_cost$total_damage' with 70 values. 
  min/median/max: 0/1.1/74.4  nNA: 0
truncate: 0 threshold: 0. dat_full with 70 values: 0/1.1/74.4  nNA: 0
dlf with 16 distributions. In descending order of fit quality:
gam, kap, pe3, wei, ln3, gno, wak, gpa, gev, glo, exp, gum, ray, nor, 
  revgum, rice
gofProp: 1, RMSE min/median/max: 0.037/0.076/0.3  nNA: 0
 5 distribution colors: #3300FFFF, #00D9FFFF, #00FF19FF, #F2FF00FF, #FF0000FF
```

`distLfit` ranks the distribution models by goodness-of-fit, with the best fitting models at the top. You can evaluate the model list and their goodness of fit statistics with a call to the `$gof` portion of the list:
 
``` 
hurr_fit$gof

             RMSE        R2     weight1    weight2   weight3 weightc
gam    0.03687682 0.9831812 0.080260879 0.08360687 0.1513246     NaN
kap    0.03690034 0.9875113 0.080254559 0.08359936 0.1513110     NaN
pe3    0.04638276 0.9851915 0.077706869 0.08057171 0.1458311     NaN
wei    0.04791014 0.9728583 0.077296502 0.08008403 0.1449485     NaN
ln3    0.05935699 0.9641990 0.074221019 0.07642915 0.1383333     NaN
gno    0.06361586 0.9527421 0.073076767 0.07506933 0.1358721     NaN
wak    0.06965965 0.9435495 0.071452952 0.07313961 0.1323794     NaN
gpa    0.06965965 0.9435495 0.071452952 0.07313961 0.0000000     NaN
gev    0.08185683 0.9337443 0.068175873 0.06924516 0.0000000     NaN
glo    0.08242552 0.9309928 0.068023081 0.06906358 0.0000000     NaN
exp    0.11283198 0.8597058 0.059853627 0.05935507 0.0000000     NaN
gum    0.13746709 0.8141283 0.053234789 0.05148931 0.0000000     NaN
ray    0.14435908 0.7933789 0.051383083 0.04928876 0.0000000     NaN
nor    0.16469704 0.7654365 0.045918783 0.04279503 0.0000000     NaN
revgum 0.19498790 0.7189435 0.037780386 0.03312344 0.0000000     NaN
rice   0.29872851 0.7589475 0.009907879 0.00000000 0.0000000     NaN
```

As you probably noticed with the graphs, only the top five models are plotted, but this list will allow you to determine the extent to which other models may fit almost as well. The list of the actual model names that go with those abbreviations is below (also see `?prettydist` in the `lmomco` package).


| R abbreviation | Distribution | 
| -------------- | ----------------------|
| `aep4` | 4-p Asymmetric Exponential Power |
| `cau` | Cauchy |
| `emu` | Eta minus Mu |
| `exp` | Exponential |
| `texp` | Truncated Exponential |
| `gam` | Gamma | 
| `gep` | Generalized Exponential Poisson |
| `gev` | Generalized Extreme Value |
| `gld` | Generalized Lambda |
| `glo` | Generalized Logistic |
| `gno` | Generalized Normal |
| `gov` | Govindarajulu |
| `gpa` | Generalized Pareto |
| `gum` | Gumbel |
| `kap` | Kappa |
| `kmu` | Kappa minus Mu |
| `kur` | Kumaraswamy |
| `lap` | Laplace |
| `lmrq` | Linear Mean Residual Quantile Function |
| `ln3` | log-Normal3 |
| `nor` | Normal | 
| `pe3` | Pearson Type III |
| `ray` | Rayleigh | 
| `revgum` | Reverse Gumbel |
| `rice` | Rice |
| `sla` | Slash |
| `st3` | Student t (3-parameter) |
| `tri` | Triangular |
| `wak` | Wakeby |
| `wei` | Weibull | 


You can evaluate the quantiles of these models to help determine what "counts" as an outlier with the `distLquantile` function. Use the `probs` option to see the quantiles you want returned, whether low or high. You also need to use the `returnlist=TRUE` option to ensure you can work with the results after the call. 

```
# Calculate quantiles for different models
hurr_quant = distLquantile(hurricane_cost$total_damage, 
  probs=c(0.80, 0.90, 0.95, 0.99), returnlist=TRUE)

# Look at the top five models only
hurr_quant$quant[1:5,]

         80%      90%      95%      99%
gam 7.269448 14.90234 23.92810 47.89042
kap 6.949505 13.98193 22.79060 50.00316
pe3 7.159698 14.86357 24.03902 48.52204
wei 6.639476 13.45379 22.53004 52.15676
ln3 6.165669 11.94520 20.29897 53.86717

# Plot the cdf of top five models with their quantiles
distLplot(hurr_quant, cdf=T, main="Total Annual Hurricane Damages
  (in billion USD)", xlab="Damages ($1B USD)", qlines=TRUE)
```

![](images/eva_cdf_quant.png)

A great feature of this package is the easy of calculating and plotting return/recurrence interval estimates. Use the `distLextreme` function with your desired return levels set with the `RP` option. 

```
# Calcluate return intervals. 
hurr_returns = distLextreme(hurricane_cost$total_damage, RPs = c(2,10,50,100))

# View top five model results
hurr_returns$returnlev[1:5,]

         RP.2    RP.10    RP.50   RP.100
gam 0.9028442 14.90234 37.18940 47.89042
kap 1.0322659 13.98193 37.07409 50.00316
pe3 0.8707481 14.86357 37.57498 48.52204
wei 1.1854233 13.45379 37.94427 52.15676
ln3 1.4838727 11.94520 36.52829 53.86717


# Plot return intervals
distLextremePlot(hurr_returns, main="Total Annual Hurricane Damages
  (in billion USD)", ylab="Damages ($1B USD)")
```

![](images/eva_returns.png)


## Finding associations in shopping carts

Finding things that are (really) similar or (really) different is the common theme to the tools in this chapter. Sometimes, you just want to be purely empirical about it, and let the data itself give you information to explore based simply on common co-occurrences. Association analysis---more commonly called "shopping cart analysis"---is a great way to do this. 

We haven't worked with a data set that's really amenable to association rules, so why not explore shopping cart analysis with a groceries dataset?

The `arules` package has a special function that allows you to read a data set directly in to R as a transaction object, for example, where you have the transaction identifier in one column and a particular item from that transaction in a second column. 

```
require(arules)
require(arulesViz)

# Download data and convert to a transactions object
basket_url = "http://dmg.org/pmml/pmml_examples/baskets1ntrans.csv"

basket_trans = read.transactions(basket_url, format = "single", sep = ",", 
  cols = c("cardid", "Product"), rm.duplicates=TRUE)
```

As usual, `summary` provides details of the resulting object:

```
summary(basket_trans)
```
{lang="text"}
```
transactions as itemMatrix in sparse format with
 939 rows (elements/itemsets/transactions) and
 11 columns (items) and a density of 0.2708878 

most frequent items:
 cannedveg frozenmeal   fruitveg       beer       fish    (Other) 
       303        302        299        293        292       1309 

element (itemset/transaction) length distribution:
sizes
  1   2   3   4   5   6   7   8 
174 227 218 176  81  38  21   4 

   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. 
   1.00    2.00    3.00    2.98    4.00    8.00 

includes extended item information - examples:
      labels
1       beer
2 cannedmeat
3  cannedveg

includes extended transaction information - examples:
  transactionID
1        100047
2        100255
3        100256
```

We can see that there are only 11 distinct items in this data set (must be a very small store!) from 939 distinct transactions, so we can plot all items by frequency using the `itemFrequencyPlot` function. Larger item sets can use the `topN` value to cut off the plot to show just the 15 or 20 or so most common items.

```
itemFrequencyPlot(basket_trans, topN=11, type="absolute")
```

![basket case](images/basket_freq.png)

The `apriori` function takes a transaction object and creates a rule set using default confidence and support levels of 0.8 and 0.1, respectively, with no more than 10 items on any one side of a rule. Accepting the defaults makes the call as simple as `basket_rules = apriori(basket_trans)`, but you will likely want to modify those to expand the rule set. To do so requires setting the `parameter` function with the details in a list. The `minlen` default is set at 1, which means that items purchased by themselves will show up---if you want to exclude single-item purchases, set `minlen=2` here. Note that since there are a variety of possible target types (see `?ASparameter-classes`), you need to specify that you are looking for rules. 

```
basket_rules_custom = apriori(basket_trans, parameter = list(support = 0.01, 
  confidence = 0.01, target="rules"))
```
{lang="text"}
```
Apriori

Parameter specification:
 confidence minval smax arem  aval originalSupport support minlen maxlen target
       0.01    0.1    1 none FALSE            TRUE    0.01      1     10  rules
   ext
 FALSE

Algorithmic control:
 filter tree heap memopt load sort verbose
    0.1 TRUE TRUE  FALSE TRUE    2    TRUE

Absolute minimum support count: 9 

set item appearances ...[0 item(s)] done [0.00s].
set transactions ...[11 item(s), 939 transaction(s)] done [0.00s].
sorting and recoding items ... [11 item(s)] done [0.00s].
creating transaction tree ... done [0.00s].
checking subsets of size 1 2 3 4 5 done [0.00s].
writing ... [845 rule(s)] done [0.00s].
creating S4 object  ... done [0.00s].
```

If you had run the `apriori` function with the defaults, you would have seen it created only 3 rules. By opening up the support and confidence levels, we now have 845 rules, which provides us with a much richer exploration space. 

I> *Confidence* is the probability that a transaction including the items on one
I> side of the rule also contains the items on the other side. 
I> 
I> *Support* is the proportion of transactions that contain that item or item set. 
I> Higher support = relevant to large proportion of transactions.  

Now we can look at the rules. A key concept for rule usefulness is *lift*. A lift value at or close to one suggests that the items or either side of the rule are independent, so are usually of little further interest. Lift values above 1 suggest that the presence of the items on one side of the rule set increase the chances of seeing items on the other side. Where items are all purchased or occur at once (e.g., as in a supermarket), the direction of the rule set is irrelevant. Where items are purchased or occur in a series (e.g., as in a web store, where orders for specific items are placed in order), the rule set is ordered from the first purchased (the `{lhs}`, left hand side), to those placed in the cart later (the `{rhs}`, right hand side). 

We'll look first at the rule sets that generate the greatest lift with the `inspect` function:

```
inspect(head(sort(basket_rules_custom, by="lift"), 10))
```

![](images/basket_rules_list.png)

We can also plot the rule set using a variety of graph types; accepting the default gives you a scatterplot of confidence, support, and lift, and setting `interactive` to `TRUE` allows you to explore particular subsets based on the combination of those three values. For example, we can select the rules with relatively high values of all three within the graph, and see those rules in the console once we've clicked `inspect` in the graph: 

{lang="text"}
```
plot(basket_rules_custom, interactive = T)

Interactive mode.
Select a region with two clicks!

Number of rules selected: 3 
    lhs                       rhs          support   confidence lift     order
555 {beer,cannedveg}       => {frozenmeal} 0.1554846 0.8742515  2.718285 3    
556 {cannedveg,frozenmeal} => {beer}       0.1554846 0.8439306  2.704610 3    
554 {beer,frozenmeal}      => {cannedveg}  0.1554846 0.8588235  2.661503 3  
```

![](images/basket_rules.png)

If you wanted to find items that were purchased with, say, beer, you can create a rule set specifically for that item:

```
beer_rules = apriori(data=basket_trans, parameter=list(supp=0.01, 
  conf = 0.01, minlen=2), appearance = list(default="rhs", lhs="beer"))

inspect(head(sort(beer_rules, by="lift"), 10))

   lhs       rhs             support    confidence lift     
9  {beer} => {frozenmeal}    0.18104366 0.5802048  1.8040142
10 {beer} => {cannedveg}     0.17784878 0.5699659  1.7663299
8  {beer} => {fruitveg}      0.09478168 0.3037543  0.9539306
4  {beer} => {cannedmeat}    0.06389776 0.2047782  0.9425818
7  {beer} => {fish}          0.09052183 0.2901024  0.9328978
6  {beer} => {wine}          0.08200213 0.2627986  0.8628249
2  {beer} => {freshmeat}     0.05005325 0.1604096  0.8276075
1  {beer} => {dairy}         0.04792332 0.1535836  0.8147741
3  {beer} => {softdrink}     0.04792332 0.1535836  0.7837773
5  {beer} => {confectionery} 0.06815761 0.2184300  0.7431370
```

You can also create graph diagrams of a rule set, and move the nodes around to show the rule meanings. In this case, only two rules give positive lift, while the other eight show inverse associations. We can manually adjust the initial graph layout to show this relationship:

```
plot(beer_rules, method="graph", interactive=T)
```

![basket graph](images/basket_graph.png)

To explore the rules in greater depth, you can write them to a text file to view outside of R using the `write` function. 

```
write(basket_rules_custom, file = "basket_rules_custom.csv", 
  sep = ",", row.names=FALSE)
```
