# Chapter 6: Trends and Time

- Describing trends in non-temporal data
- Detecting autocorrelation
- Dealing with temporal data
- Plotting time series data
- Decomposing time series into components
- Evaluating quality with control charts
- Identifying possible breakpoints in a time series
- Looking for cycles in the noise

Dealing with temporal data presents its own set of unique challenges. R lessens that burden considerably—as long as your data are in the right form!

## Describing trends in non-temporal data

If you’re in the data exploration phase, and want to get a loose sense of the trend in the data, starting with a loess curve is probably the best first step. Quantile regression provides tools for evaluating trends in data with increasing or decreasing variance, or any data with a monotonic trend regardless of variance. Ordinary least squares is probably the last tool you want to use in the exploratory phase, though it can often be what you start with when doing model building.

We’ll continue to use the bike share data, and plot the results of this recipe with `ggplot2`. We’ll also need the `quantreg` package in order to plot the quantile regression trend lines:

```
require(ggplot2)
require(quantreg)
```

### Smoothed trends

To see a smoothed average trend, use `method="loess"` in the `geom_smooth` function:

```
ggplot(bike_share_daily, aes(x=atemp, y=cnt)) +
    xlab("Daily Mean Normalized Air Temperature") +
    ylab("Number of Total Bike Uses") +
    geom_point(col="gray50") +
    geom_smooth(method="loess") +
    theme_bw()
```

![loess trend](/images/biwr_05_loess.png)

### Quantile trends

To show trends based on quantiles, use the `stat_quantile` option, and choose which quantiles you want to show, either specifically (as below) or sequentially (e.g., `quantiles = seq(0.05, 0.95, by=0.05)`:

```
ggplot(bike_share_daily, aes(x=temp, y=casual)) +
    xlab("Daily Mean Normalized Temperature") +
    ylab("Number of Casual Bike Uses") +
    geom_point(col="gray50") +
    stat_quantile(aes(colour = ..quantile..), quantiles = c(0.05, 0.1, 0.25,
    0.5, 0.75, 0.9, .95)) +
    scale_colour_gradient2(midpoint=0.5, low="steelblue", mid="brown", 
    high="steelblue ") +
    theme_bw()
```

![quantile trend](/images/biwr_05_quant.png)

### Simple linear trends

Finally, to get a simple linear trend, use the `lm` method with `geom_smooth`:

```
ggplot(bike_share_daily, aes(x=atemp, y=cnt)) +
    xlab("Daily Mean Normalized Air Temperature") +
    ylab("Number of Total Bike Uses") +
    geom_point(col="gray50") +
    geom_smooth(method="lm") +
    theme_bw()
```

![linear trend](/images/biwr_05_linear.png)

If you’re exploring trends, it’s good habit to not start with an OLS/linear trend, as in real data exploration you usually don’t have any *a priori* knowledge about the functional relationship or trend. Only using this approach after you’ve tried the others helps prevent getting your thinking into a (linear) rut.

### Segmented linear trends

Sometimes you do want to see a linear trend, but know (or suspect) that there are changes in the trend, as seen above in the air temperature-based plots. We’ll explore ways to evaluate change points later, but for now we’ll just quickly plot a segmented linear trend line. We'll allow the `segmented` package to choose the optimal breakpoint for us, after we specify approximately where we think it is with the `psi` option:

```
require(segmented)
bike_segment = segmented(lm(cnt~atemp, data=bike_share_daily), ~atemp, psi=0.6)
bike_segment$psi

           Initial     Est.     St.Err
psi1.atemp     0.6 0.585762 0.01352518

psi = bike_segment$psi[2]
ggplot(bike_share_daily, aes(x=atemp, y=cnt, group = atemp > psi)) +
    xlab("Daily Mean Normalized Air Temperature") +
    ylab("Number of Total Bike Uses") +
    geom_point(col="gray50") +
    geom_smooth(method="lm") +
    theme_bw()
```

![segmented trend](/images/biwr_05_segment.png)

The `psi` option is pretty robust to starting values when you have a large sample size and there's at least a hint of a real break in the trend. For example, you can run the above code using a `psi` of 0.2—where there's clearly no change in trend—and the breakpoint estimate is exactly the same. However, with fewer data points, and/or a lot of variance, the choice of psi could change the results, so try a few different values. 


## Working with temporal data

Date and time formats are the bane of coders everywhere. No matter which coding language you use, you’ll eventually run into the need to convert dates or times and end up wanting to punch your screen. Try to avoid that—it’d be an expensive mistake, and you don’t want to give emergency departments any more business than they already have.

The informal consensus is that in R, `as.Date` is best for dates and `as.POSIXct` is best for times because these are the simplest ways to work with each data type. The `lubridate` package provides a more intuitive wrapper to `as.POSIXct` and can be very useful for date conversions between types. 

Finally, time series analysis requires specific data types or structures—we’ll explore one approach to that in the next recipe, but the CRAN Time Series task view is worth spending time exploring if you need to evaluate time series in any depth (http://cran.r-project.org/web/views/TimeSeries.html).

There just isn’t the space to cover all (or even a small subset) of date/time conversions and temporal math, so we’ll just illustrate a few very common date conversion needs in this recipe.

In addition to the `base` function `as.Date`, we’ll use the `lubridate` package in this recipe.

```
require(lubridate)
```

The ISO standard for a date format is 4-digit year, 2-digit month, and 2-digit day, separated by hyphens. We’ll often aim to convert to that format, but as we’ll see, it’s not always necessary as long as you specify the format type in the conversion process.  

Perhaps the most common date conversion is a character to a date. If it’s not in a clear year-month-day format, you’ll need to specify the format it’s in so that R can properly convert it to a date (a table of the major formats appears below).

```
india_natl_holidays = c("Jan 26, 2014", "Apr 13, 2014", "Aug 15, 2014", 
  "Oct 02, 2014", "Dec 25, 2014")
india_natl_holidays = as.Date(india_natl_holidays, format="%b %d, %Y")
str(india_natl_holidays)

Date[1:5], 
    format: "2014-01-26" "2014-04-13" "2014-08-15" "2014-10-02" "2014-12-25"
```

Another common date conversion need is to calculate age based on today’s date (or any reference date):

```
birthdays = c("20000101", "19991201", "19760704", "20140314")
ymd(birthdays)

"2000-01-01 UTC" "1999-12-01 UTC" "1976-07-04 UTC" "2014-03-14 UTC"

age = ymd(today()) - ymd(birthdays)
round(age/eyears(1), 1) # eyears is a lubridate function, not a typo!

14.9 15.0 38.4  0.7

age = ymd(20141201) - ymd(birthdays)
round(age/eyears(1), 0)

15 15 38  1
```

The basic thing to remember is that even when you don’t have a real day value (e.g., your data is binned into months), many functions in R generally expect one if you want to use date functions. See the next recipe for an example of this issue in action.

It’s useful to have a cheat-sheet of the major date and time formats; you can get a complete list with `?strptime` but the major ones are as follows:

***Major date formats***  

| Date step | Symbol | Format | Alternate symbols |
| --------- | ------ | ------ | ----------------- |
| Format Date (ISO) | `%F` | Y-m-d | `%x`, `%D` | y/m/d, m/d/y |
| Year | `%Y` | 4-digit year (2014) | `%y` | 2-digit year (14) |
| Month | `%m` | Decimal month (10) | `%b`, `%B` | Abbreviated month (Oct.), Full month (October) |
| Day | `%d` | Decimal date (1-31) | `%e` | Decimal date (01-31) | 
| Weekday | `%w` | Decimal weekday (0-6, starts with Sunday) | `%a`, `%A`, `%u` | Abbreviated weekday (Sun), Full weekday (Sunday), Decimal with Monday start (7) |
| Weekyear (ISO) | `%V` | Sunday start (0-53) | `%U`, `%W` | Sunday start (USA), Monday start (UK) | 
| Day of the year | `%j` | Decimal day of the year (1-366) |  |  | 


***Major time formats***

| Time step | Symbol | Format |
| --------- | ------ | ------ |
| Date and time | `%c` | a b e H:M:S Y |
| Time | `%T`| H:M:S |
| Time (short) | `%R` | H:M |
| Hours (24 hour clock) | `%H` |  |
| Hours (12 hour clock) | `%I` |  |
| Minutes | `%M` |  |
| Seconds | `%S` |  | 
| AM or PM | `%p` |  |
| Offset from GMT | `%z` |  | 
| Time zone | `%Z` |  |


## Calculate a mean or correlation in circular time (clock time)

Sometimes you want to know the average time at which something happens, or the relationship between times, but if the data occur near the end of a time period, the use of mean() will give you incorrect results. The `psych` package comes to the rescue with the `circadian.mean` and `circadian.cor` functions:

```
require(psych)
wake_time = c(7.0, 7.5, 6.5, 6.25, 7.0, 7.25, 7.25)
sleep_time = c(23.5, 0.5, 23.75, 0.25, 23.25, 23.5, 0.75)
wake_sleep = data.frame(wake_time, sleep_time)
circadian.mean(wake_sleep$sleep_time)

23.92808

circadian.cor(wake_sleep)

           wake_time sleep_time
wake_time  1.0000000  0.1524331
sleep_time 0.1524331  1.0000000
```

## Plotting time-series data

While having dates in the dataset can allow easy plotting via `ggplot2`, there are a few plotting functions that provide one-line insight into time series data. They may not be graphically elegant, but they provide a great way to understand your data more thoroughly.

We’ll use a dataset of 10 years of monthly births in the UK (2003-2012), acquired from the EU’s EuroStat.

```
UK_births = read.table("UK_births.csv", header=T, sep=",", stringsAsFactors=F)
```

First, we need to convert the data into a ts (time series) object:

```
UK_births_ts = ts(UK_births$Value, start = c(2003, 1), frequency = 12)
```

Then we should plot it; `ts` objects can be a little awkward to plot without tweaking the plot settings, so if all you need to do is look at it, you can simply use `plot(my_ts_object)`. But if you want to use the plot for more than just your own edification, you’ll need to add some decoration manually:

```
plot(UK_births_ts, xlab="Date", ylab="Count", main="Monthly Births in the UK 
  (2003-2012)", col="darkblue")
```

![time series](/images/biwr_05_ukbirths1.png)

### No day value but you want to plot it in ggplot?

R is a little particular about date values, and if your data aren’t in a format it recognizes, it can choke. Case in point is the UK births date, which contains a year field and a month field—which makes sense, seeing as how the data are monthly values. But you’ll have to tie yourself in knots to make those two work together as time elements in a ggplot… so it’s actually easier if you just gave each month a fake day (i.e., the 1st), mush ‘em together as a character via the `paste` function, and then specify that you want the final result to be in Date format.

```
require(ggplot2)

UK_births$Date = as.Date(paste("1", UK_births$MONTH, UK_births$TIME, sep=" "), 
  format="%d %B %Y")

ggplot(UK_births, aes(x=Date, y=Value)) +
    geom_line(col="darkblue") +
    ylab("Count") +
    theme_minimal()
```

![time series 2](/images/biwr_05_ukbirths2.png)


## Detecting autocorrelation

Autocorrelation can be present in non-temporal data as well as in time series; the `acf` function works the same for either type of data.

```
acf(UK_births_ts)
```

![](/images/autocorrelation.png)


## Plotting monthly patterns

If your data are monthly (or if you aggregate them to months), the `monthplot` function breaks out each month’s data and plots them separately, along with a horizontal line for each month’s mean value over the time span. A few manual tweaks helps show the information in this plot more clearly:

```
monthplot(UK_births_ts, main="Monthly Patterns in UK Births (2003-2012)", 
  xlab="Month", ylab="Count", col="darkblue", lwd=2, lty.base=2, lwd.base=1, 
  col.base="gray40")
```

![time series 3](/images/biwr_05_ukbirths3.png)

The `forecast` package contains another useful plot function that shows each year’s trend as a separate line, allowing you to discern whether there’s a seasonal or monthly effect at a glance:

```
require(forecast)
seasonplot(UK_births_ts, main="Seasonal Trends in UK Births (2003-2012)", 
  col=rainbow(10), year.labels=TRUE, year.labels.left=TRUE, cex=0.7, 
  cex.axis=0.8)
```

![time series 4](/images/biwr_05_ukbirths4.png)


## Decomposing time series into components

If you need to explore a time series in more depth, you'll probably want to decompose it into seasonal, trend, and random/remainder components.

We’ll continue with the UK monthly births data in its time series format, acquired from the EU’s EuroStat. Most functions we’ll use in this recipe are in the `base` installation’s `stats` package, but one is in the `forecast` package, so load that with the data:

```
require(forecast)
UK_births = read.table("UK_births.csv", header=T, sep=",", stringsAsFactors=F)
```

The `decompose` function uses a moving average for simplicity:

```
plot(decompose(UK_births_ts), col="darkblue")
```

![decomp of uk births](/images/biwr_05_decomp1.png)

We’ll explore the `forecast` package in Part 2 of this book, but more details on `ts` objects and the decomposition of time series can be found in the help files (`?ts`, `?decompose`) and in Rob Hyndman’s excellent online text, [Forecasting: Principles and Practice](https://www.otexts.org/fpp).


## Decomposing when the variance changes

Of course, simple time series decomposition only works correctly if the variance is more or less stable. When it’s not—as in the `AirPassengers` dataset, below—you have to transform the values before you decompose the series into its components. The `forecast` package also has a way to determine the best transformation (`lambda`) automatically with the `BoxCox.lambda` function:

```
data(AirPassengers)
plot(AirPassengers)
AirPassengers_lambda = BoxCox.lambda(AirPassengers)
plot(BoxCox(AirPassengers, AirPassengers_lambda))
plot(decompose(BoxCox(AirPassengers, AirPassengers_lambda)))
```

![decomp of air passengers](/images/biwr_05_decomp2.png)


## Evaluating quality with control charts

Industrial and businesses with quality control processes or activities often use control charts (aka “Shewhart charts”) to evaluate whether a process is moving “out of control” over time and thus requires intervention. The `qcc` package provides a way to plot all major control chart types simply and quickly.

As an example, we’ll create some fake data on hospital-acquired-infections (HAI) for a 14-month period to illustrate the creation of a u-chart, a control chart that plots changes in a rate over time:

```
require(qcc)
infections = c(6, 2, 5, 1, 3, 4, 2, 6, 3, 2, 4, 7, 1, 1, 4, 4, 1, 5, 2, 3, 5, 
  2, 3, 2, 4)
patient_days = c(985, 778, 1010, 834, 750, 729, 1002, 639, 985, 578, 976, 540, 
  829, 723, 908, 1017, 1097, 1122, 1234, 1022, 1167, 1098, 1201, 1045, 1141)
```

The `qcc` function takes care of everything:

```
infection_control = qcc(infections, sizes=patient_days, type="u",
  labels=month.abb[c(1:12, 1:12, 1:1)], axes.las=2, xlab="Month", ylab="", 
  title="Hospital Acquired Infections Rate\nu-chart for Jan 2012 - Jan 2014")
```

![qcc u-chart](/images/biwr_05_qcc1.png)


## Identifying possible breakpoints in a time series

We’ll explore intervention analysis in more detail in Part 2 of this book, but this is a useful place to demonstrate a simple way to identify changes in the statistical structure of a time series using the `strucchange` package.

We’ll look at annual United States CO<sub>2</sub> emissions between February 2001 and September 2014 (data from the US Energy Information Administration’s WEPS+ model). Load the `strucchange` package, the data, and convert the data to a time series object:

```
require(strucchange)
us_co2 = read.table("US_CO2_2001_2014.csv", header=T, sep=",")
us_co2_ts = ts(us_co2[,2], freq=12, start=c(2001,2))
```

`strucchange` will pick the optimal number of breakpoints as determined by BIC, which can be subsequently plotted over the time series. As there does not seem to be an indication of general rises or declines in mean tendency, we can use `~ 1` in the formula in place of a dependent variable to assess whether there is a step change (or changes) in the time series:

```
breakpoints(us_co2_ts ~ 1)

     Optimal 2-segment partition:
Call:
breakpoints.formula(formula = us_co2_ts ~ 1)
Breakpoints at observation number:
96
Corresponding to breakdates:
2009(1)

plot(us_co2_ts, main=expression(paste("Breakpoint in Monthly US"
  ~CO[2]~Emissions)), ylab="Million Metric Tons", col="darkblue", lwd=1.5)
lines(breakpoints(us_co2_ts ~ 1), col="darkgreen")
text(2008.8, 555, "Jan 2009", cex=0.75, col="darkgreen", pos=4, font=3)
```

![breakpoints diagnostics](/images/biwr_05_bps.png)


## Using spectral analysis to find periodicity

Spectral analysis provides a simple way to evaluate whether there are any cyclical patterns in your time series. The `stats` package’s `spectrum` function creates a periodogram, where the highest peak(s) in the plot represent(s) possible cycling times.

We’ll use the built-in annual sunspot dataset in this recipe.

```
data(sunspot.year)
```

To find the cycle times, look for a clear peak (or peaks) in the plot; below, the second plot is zoomed in to see the detail. The blue line shows the confidence interval for peak locations.

```
sun_spec = spectrum(sunspot.year)
plot(sun_spec)
```

![sunspots spectral](/images/biwr_05_spectral.png)

You can take the inverse of the frequency peak’s value to get a sense for cycle timings. In this data, we can see the highest peak is at about 0.1, suggesting a ~10 year cycle (1/0.1=10).

You can also extract the `$freq` and `$spec` values from the spectrum object into a data frame as x and y, respectively, if you want to create a pretty plot with ggplot.

## Fourier transforms, wavelets, and other decomposition

STUFF HERE?

## Exploring relationships between time series: cross-correlation

STUFF HERE

