# Chapter 6: Trends and Time

- Describing trends in non-temporal data
- Regression for continuous, count, and binary data (GLMs)
- Regression for non-linear data (GAMs)
- Working with temporal data
- Calculate a mean or correlation in circular time (clock time)
- Plotting time-series data
- Detecting autocorrelation
- Plotting monthly patterns
- Decomposing time series into components
- Finding cycles in the noise: Using spectral analysis to identify periodicity
- Evaluating quality with control charts
- Identifying possible breakpoints in a time series
- Exploring relationships between time series: cross-correlation
- Basic forecasting

Dealing with temporal data presents its own set of unique challenges. R lessens that burden considerably—as long as your data are in the right form.

## Describing trends in non-temporal data

If you’re in the data exploration phase, and want to get a loose sense of the trend in the data, starting with a loess curve is probably the best first step. Quantile regression provides tools for evaluating trends in data with increasing or decreasing variance, or any data with a monotonic trend regardless of variance. Ordinary least squares is probably the *last* tool you want to use in the exploratory phase, though it can sometimes be what you start with when doing model building.

We’ll continue to use the bike share data, and plot the results of this recipe with `ggplot2`. We’ll also need the `quantreg` package in order to plot the quantile regression trend lines:

```
require(ggplot2)
require(quantreg)
```

### Smoothed trends

To see a smoothed average trend, use `method="loess"` in the `geom_smooth` function:

```
ggplot(bike_share_daily, aes(x=atemp, y=cnt)) +
    xlab("Daily Mean Normalized Air Temperature") +
    ylab("Number of Total Bike Uses") +
    geom_point(col="gray50") +
    geom_smooth(method="loess") +
    theme_bw()
```

![loess trend](images/biwr_05_loess.png)

### Quantile trends

To show trends based on quantiles, use the `stat_quantile` option, and choose which quantiles you want to show, either specifically (as below) or sequentially (e.g., `quantiles = seq(0.05, 0.95, by=0.05)`:

```
ggplot(bike_share_daily, aes(x=temp, y=casual)) +
    xlab("Daily Mean Normalized Temperature") +
    ylab("Number of Casual Bike Uses") +
    geom_point(col="gray50") +
    stat_quantile(aes(color = ..quantile..), quantiles = c(0.05, 0.1, 0.25,
    0.5, 0.75, 0.9, .95)) +
    scale_color_gradient2(midpoint=0.5, low="steelblue", mid="brown", 
    high="steelblue ") +
    theme_bw()
```

![quantile trend](images/biwr_05_quant.png)

### Simple linear trends

Finally, to get a simple linear trend, use the `lm` method with `geom_smooth`:

```
ggplot(bike_share_daily, aes(x=atemp, y=cnt)) +
    xlab("Daily Mean Normalized Air Temperature") +
    ylab("Number of Total Bike Uses") +
    geom_point(col="gray50") +
    geom_smooth(method="lm") +
    theme_bw()
```

![linear trend](images/biwr_05_linear.png)

If you’re exploring trends, it’s good habit to not start with an OLS/linear trend, as in real data exploration you usually don’t have any *a priori* knowledge about the functional relationship or trend. Only using this approach after you’ve tried the others helps prevent starting your thinking into a (linear) rut.

### Segmented linear trends

Sometimes you do want to see a linear trend, but know (or suspect) that there are changes in the trend, as seen above in the air temperature-based plots. We’ll explore ways to evaluate change points later, but for now we’ll just quickly plot a segmented linear trend line. We'll allow the `segmented` package to choose the optimal breakpoint for us, after we specify approximately where we think it is with the `psi` option:

```
require(segmented)
bike_segment = segmented(lm(cnt~atemp, data=bike_share_daily), ~atemp, psi=0.6)
bike_segment$psi

           Initial     Est.     St.Err
psi1.atemp     0.6 0.585762 0.01352518

psi = bike_segment$psi[2]
ggplot(bike_share_daily, aes(x=atemp, y=cnt, group = atemp > psi)) +
    xlab("Daily Mean Normalized Air Temperature") +
    ylab("Number of Total Bike Uses") +
    geom_point(col="gray50") +
    geom_smooth(method="lm") +
    theme_bw()
```

![segmented trend](images/biwr_05_segment.png)

The `psi` option is pretty robust to starting values when you have a large sample size and there's at least a hint of a real break in the trend. For example, you can run the above code using a `psi` of 0.2—where there's clearly no change in trend—and the breakpoint estimate is exactly the same. However, with fewer data points, and/or a lot of variance, the choice of `psi` could change the results, so try a few different values. 


## Regression for continuous, count, and binary data (GLMs)

{width="90%"}
| Response type | Response error distribution | Predictor type | Model type | Code |
| ------------- | --------------------------- | -------------- | ---------- | ---- |
| Continuous | Normal | Continuous | Regression | `lm(y ~ x1 + x2 ... + xn)` |
| Continuous | Normal | Categorical | ANOVA | `lm(y ~ x1 + x2 ... + xn)` |
| Continuous | Normal | Mixed | ANCOVA | `lm(y ~ x1 + x2 ... + xn)` |
| Binary | Binomial | Mixed | Logistic regression | `glm(y ~ x1 + x2 ... + xn, family=binomial(link=logit))` |
| Count | Poisson | Mixed | Mixed | Poisson regression | `glm(y ~ x1 + x2 ... + xn, family=poisson(link=log))` |
|  | Overdispersed Poisson |  Mixed | Negative binomial regression | `glm.nb(y ~ x1 + x2 ... + xn)` | 
| Rate | Poisson | Mixed |  Mixed | Poisson regression | `glm(y ~ x1 + x2 ... + xn, offset=log(t), family=poisson(link=log))` |
|  | Overdispersed Poisson |  Mixed | Negative binomial regression | `glm.nb(y ~ x1 + x2 ... + xn, offset=log(t))` | 
| Counts with lots of 0s | Overdispersed Poisson |  Mixed | Zero-inflated Poisson regression | `require (pscl)` |
| | | | `zeroinfl(y ~ x1 + x2 ... + xn, dist="poisson")` | 
| Counts where 0 is not possible and mean < 5 | Overdispersed Poisson |  Mixed | Zero-truncated Poisson regression | `require (gamlss.tr)` |
| | | | `gamlss(y ~ x1 + x2 ... + xn, family=PO)` | 



### Assessing model fit



## Regression for non-linear data (GAMs)



## Working with temporal data

Date and time formats are the bane of coders everywhere. No matter which coding language you use, you’ll eventually run into the need to convert dates or times and end up wanting to punch your screen. 

Time series analysis requires specific data types or structures—we’ll explore different types of those structures below, and the CRAN Time Series task view is worth spending time exploring if you need to evaluate time series in any depth (http://cran.r-project.org/web/views/TimeSeries.html).

The informal consensus is that in R, `as.Date` is best for dates and `as.POSIXct` is best for times because these are the simplest ways to work with each data type. The `lubridate` package provides a more intuitive wrapper to `as.POSIXct` and can be very useful for date conversions between types. 

There just isn’t the space to cover all (or even a small subset) of date/time conversions and temporal math, so we’ll just illustrate a few very common date conversion needs in this recipe.

In addition to the `base` function `as.Date`, we’ll use the `lubridate` package in this recipe.

```
require(lubridate)
```

The ISO standard for a date format is 4-digit year, 2-digit month, and 2-digit day, separated by hyphens. We’ll often aim to convert to that format, but as we’ll see, it’s not always necessary as long as you specify the format type in the conversion process.  

Perhaps the most common date conversion is a character to a date. If it’s not in a clear year-month-day format, you’ll need to specify the format it’s in so that R can properly convert it to a date (a table of the major formats appears below).

```
india_natl_holidays = c("Jan 26, 2014", "Apr 13, 2014", "Aug 15, 2014", 
  "Oct 02, 2014", "Dec 25, 2014")
india_natl_holidays = as.Date(india_natl_holidays, format="%b %d, %Y")
str(india_natl_holidays)

Date[1:5], 
    format: "2014-01-26" "2014-04-13" "2014-08-15" "2014-10-02" "2014-12-25"
```

Another common date conversion need is to calculate age based on today’s date (or any reference date):

```
birthdays = c("20000101", "19991201", "19760704", "20140314")
ymd(birthdays)

"2000-01-01 UTC" "1999-12-01 UTC" "1976-07-04 UTC" "2014-03-14 UTC"

age = ymd(today()) - ymd(birthdays)
round(age/eyears(1), 1) # eyears is a lubridate function, not a typo!

14.9 15.0 38.4  0.7

age = ymd(20141201) - ymd(birthdays)
round(age/eyears(1), 0)

15 15 38  1
```

The basic thing to remember is that even when you don’t have a real day value (e.g., your data is binned into months), many functions in R generally expect one if you want to use date functions. See the next recipe for an example of this issue in action.

It’s useful to have a cheat-sheet of the major date and time formats; you can get a complete list with `?strptime` but the major ones are as follows:

***Major date formats***  

{width="90%"}
| Date step | Symbol | Format | Alternate symbols | Alternate format |
| --------- | ------ | ------ | ----------------- | ---------------- |
| Format Date (ISO) | `%F` | Y-m-d | `%x`, `%D` | y/m/d, m/d/y |
| Year | `%Y` | 4-digit year (2014) | `%y` | 2-digit year (14) |
| Month | `%m` | Decimal month (10) | `%b`, `%B` | Abbreviated month (Oct.), Full month (October) |
| Day | `%d` | Decimal date (1-31) | `%e` | Decimal date (01-31) | 
| Weekday | `%w` | Decimal weekday (0-6, starts with Sunday) | `%a`, `%A`, `%u` | Abbreviated weekday (Sun), Full weekday (Sunday), Decimal with Monday start (7) |
| Weekyear (ISO) | `%V` | Sunday start (0-53) | `%U`, `%W` | Sunday start (USA), Monday start (UK) | 
| Day of the year | `%j` | Decimal day of the year (1-366) |  |  | 


***Major time formats***

{width="90%"}
| Time step | Symbol | Format |
| --------- | ------ | ------ |
| Date and time | `%c` | a b e H:M:S Y |
| Time | `%T`| H:M:S |
| Time (short) | `%R` | H:M |
| Hours (24 hour clock) | `%H` |  |
| Hours (12 hour clock) | `%I` |  |
| Minutes | `%M` |  |
| Seconds | `%S` |  | 
| AM or PM | `%p` |  |
| Offset from GMT | `%z` |  | 
| Time zone | `%Z` |  |


## Calculate a mean or correlation in circular time (clock time)

Sometimes you want to know the average time at which something happens, or the relationship between times, but if the data occur near the end of a time period, the use of mean() will give you incorrect results. The `psych` package comes to the rescue with the `circadian.mean` and `circadian.cor` functions:

```
require(psych)
wake_time = c(7.0, 7.5, 6.5, 6.25, 7.0, 7.25, 7.25)
sleep_time = c(23.5, 0.5, 23.75, 0.25, 23.25, 23.5, 0.75)
wake_sleep = data.frame(wake_time, sleep_time)
circadian.mean(wake_sleep$sleep_time)

23.92808

circadian.cor(wake_sleep)

           wake_time sleep_time
wake_time  1.0000000  0.1524331
sleep_time 0.1524331  1.0000000
```

## Plotting time-series data

While having dates in the dataset can allow easy plotting via `ggplot2`, there are a few plotting functions that provide one-line insight into time series data. They may not be graphically elegant, but they provide a great way to understand your data more thoroughly.

We’ll use a dataset of 10 years of monthly births in the UK (2003-2012), acquired from the EU’s EuroStat.

```
library(eurostat)
demo_fmonth = get_eurostat('demo_fmonth')
```

R can be a little particular about date values, and if your data aren’t in a format it recognizes, it can choke. Case in point is using `ggplot2` to plot this data, which contains a year field and a month field but not a day field—which makes sense, seeing as how the data are monthly values. But you’ll have to tie yourself in knots to make those two work together as time elements in a ggplot, so it’s actually easier if you just gave each month a fake day (i.e., the 1st), mush ‘em together as a character via the `paste` function, and then specify that you want the final result to be in `Date` format.

```
demo_fmonth$date = as.Date(paste(substr(demo_fmonth$time, 1, 4), 
    substr(demo_fmonth$month, 2, 3), "01", sep="-"))
```

Now we can use `dplyr` to filter and arrange the data...

```    
library(dplyr)
UK_births = filter(demo_fmonth, geo == "UK" & month != 'TOTAL' & 
    month != 'UNK' & date >= '2003-01-01' & date <= '2012-12-01')
UK_births = arrange(UK_births, date)
```

...and then we can convert the data into a `ts` (time series) object:

```
UK_births_ts = ts(UK_births$values, start=c(2003, 1), frequency=12)
```

The `frequency` option is related to how many time steps make up a "whole" unit in your data; for example, if your data is hourly and you expect seasonality on the daily level, frequency equals 24. Obviously, the above setting tells R that the data are monthly and we expect annual "seasonality." 

Once we have a `ts` object, we can plot it; if all you need to do is look at it, you can simply use `plot(my_ts_object)`. But if you want to use the plot for more than just your own edification, you’ll need to add some decoration manually:

```
plot(UK_births_ts, xlab="Date", ylab="Count", main="Monthly Births in the UK 
  (2003-2012)", col="darkblue")
```

![time series](images/biwr_05_ukbirths1.png)

And we can use `ggplot2` to make it prettier:

```
require(ggplot2)
ggplot(UK_births, aes(x=Date, y=Value)) +
    geom_line(col="darkblue") +
    ylab("Count") +
    theme_minimal()
```

![time series 2](images/biwr_05_ukbirths2.png)


## Detecting autocorrelation

Autocorrelation can be present in non-temporal data as well as in time series; the `acf` function works the same for either type of data.

```
acf(UK_births_ts)
```

![autocorrelation](images/autocorrelation.png)

What's often underappreciated is that autocorrelated data, temporal or not, will make standard confidence intervals *too small*. In other words, if you want to plot a trend and use the default confidence intervals, you'll underestimate the actual uncertainty associated with a trend line or data points across time. 


## Plotting monthly patterns

If your data are monthly (or if you aggregate them to months), the `monthplot` function in the `base` installation breaks out each month’s data and plots them separately, along with a horizontal line for each month’s mean (the default) or median (`base = median`) value over the time span. A few manual tweaks helps show the information in this plot more clearly:

```
monthplot(UK_births_ts, main="Monthly Patterns in UK Births (2003-2012)", 
  xlab="Month", ylab="Count", col="darkblue", lwd=2, lty.base=2, lwd.base=1, 
  col.base="gray40")
```

![time series 3](images/biwr_05_ukbirths3.png)

You can plot out the seasonal, trend, or random components in a `monthplot` with the `choice` option, e.g., `choice = "seasonal"`). 

The `forecast` package contains another useful plot function that shows each year’s trend as a separate line, allowing you to discern whether there’s a seasonal or monthly effect at a glance:

```
require(forecast)
seasonplot(UK_births_ts, main="Seasonal Trends in UK Births (2003-2012)", 
  col=rainbow(10), year.labels=TRUE, year.labels.left=TRUE, cex=0.7, 
  cex.axis=0.8)
```

![time series 4](images/biwr_05_ukbirths4.png)


## Decomposing time series into components

If you need to explore a time series in more depth, you'll probably want to decompose it into seasonal, trend, and random/remainder components.

### Additive decomposition

We’ll continue with the UK monthly births data in its time series format. Most functions we’ll use in this recipe are in the `base` installation’s `stats` package, but one is in the `forecast` package, so make sure it's loaded with the data.

The `decompose` function uses a moving average for simplicity:

```
plot(decompose(UK_births_ts), col="darkblue")
```

![decomp of uk births](images/biwr_05_decomp1.png)




### Decomposing when the variance changes

Of course, simple time series decomposition only works correctly if the variance is more or less stable. When it’s not—as in the `AirPassengers` dataset, below—you have to transform the values before you decompose the series into its components. 

There are two ways to do this: by using `type = "multiplicative"` in the `decompse` function as above, or by using the `BoxCox.lambda` function in the `forecast` package to determine an optimal transformation:

Look at the data to see the change in variance over time:
```
data(AirPassengers)
plot(AirPassengers)
```

![air passengers](images/air_passengers.png)

Use the `BoxCox.lambda` function and view the results:

```
require(forecast)
AirPassengers_lambda = BoxCox.lambda(AirPassengers)
plot(BoxCox(AirPassengers, AirPassengers_lambda))
plot(decompose(BoxCox(AirPassengers, AirPassengers_lambda)))
```

![decomp of air passengers](images/biwr_05_decomp2.png)

![decomp of air passengers second plot](images/biwr_05_decomp3.png)


## Finding cycles in the noise: Using spectral analysis to identify periodicity

Spectral analysis provides a simple way to evaluate whether there are any cyclical patterns in your time series. The `base` installation's `spectrum` function creates a periodogram, where the highest peak(s) in the plot represent(s) possible cycling times. It de-trends by default, so whether you're using data with a trend or not doesn't matter.

We’ll use the built-in annual sunspot dataset in this recipe.

```
data(sunspot.year)
```

To find the cycle times, look for a clear peak (or peaks) in the plot; below, the second plot is zoomed in to see the detail. The blue line shows the confidence interval for peak locations.

```
sun_spec = spectrum(sunspot.year)
plot(sun_spec)
```

![sunspots spectral](images/biwr_05_spectral.png)

You can also extract the `$freq` and `$spec` values from the spectrum object into a data frame as x and y, respectively, to get the exact frequency where the spectrum is maximized (or if you want to create a pretty plot with `ggplot2`).

```
# Make a data frame
sun_spec_df = data.frame(sun_spec$freq, sun_spec$spec)

# Determine value of "freq" at maximum "spec" 
sun_spec_df[which.max(sun_spec_df[,2]),]

   sun_spec.freq sun_spec.spec
27          0.09      55820.99
```

When frequency is 1, you can take the inverse of the frequency peak’s value to get a sense for cycle timings. This data is annual, and since the the highest spectral peak is at a frequency of 0.09, it suggests that there is a ~11 year cycle (1/0.09=11).


## Evaluating quality with control charts

Industrial and businesses with quality control processes or activities often use control charts (aka “Shewhart charts”) to evaluate whether a process is moving “out of control” over time and thus requires intervention. The `qcc` package provides a way to plot all major control chart types simply and quickly.

As an example, we’ll create some fake data on hospital-acquired-infections (HAI) for a 25-month period to illustrate the creation of a u-chart, a control chart that plots changes in a rate over time:

```
require(qcc)
infections = c(6, 2, 5, 1, 3, 4, 2, 6, 3, 2, 4, 7, 1, 1, 4, 4, 1, 5, 2, 3, 5, 
  2, 3, 2, 4)
patient_days = c(985, 778, 1010, 834, 750, 729, 1002, 639, 985, 578, 976, 540, 
  829, 723, 908, 1017, 1097, 1122, 1234, 1022, 1167, 1098, 1201, 1045, 1141)
month_name = month.abb[c(1:12, 1:12, 1:1)]
```

We'll use 1,000 patient days for the rate baseline. The `qcc` function takes care of everything:

```
infection_control = qcc(infections, sizes=patient_days/1000, type="u",
  labels=month_name, axes.las=2, xlab="Month", ylab="", digits=2,
  title="Hospital Acquired Infections Rate per 1,000 Patient Days\n
  u-chart for Jan 2012 - Jan 2014")
```

![qcc u-chart](images/qcc.png)

You can remove the components of the resulting R object for use or plotting elsewhere (e.g., with `ggplot2`).

```
infection_control_qcc = data.frame(Month = month_name, infection_control$limits, 
    Rate = (infections / patient_days)*1000)
```


## Identifying possible breakpoints in a time series

We’ll explore intervention analysis in more detail in Part 2 of this book, but this is a useful place to demonstrate a simple way to identify changes in the statistical structure of a time series using the `strucchange` package.

We’ll look at total monthly United States CO_2 emissions between February 2001 and June 2015 ([data from the US Energy Information Administration](http://www.eia.gov/beta/MER/index.cfm?tbl=T12.01#/?f=M&start=200101&end=201506&charted=0-1-13)). Load the `strucchange` package, download and filter the data, and convert the data to a time series object:

```
require(strucchange)
US_co2 = read.table("http://www.eia.gov/totalenergy/data/browser/csv.cfm?tbl=T12.01", sep=",", header=T)
US_co2 = filter(US_co2, Column_Order == 14 & YYYYMM >= 200102 & substr(YYYYMM, 5, 7) != 13)
US_co2_ts = ts(US_co2[,3], freq=12, start=c(2001,2))
```

`strucchange` will pick the optimal number of breakpoints as determined by BIC, which can be subsequently plotted over the time series. As there does not seem to be a clear indication of general rises or declines in mean tendency, we can use `~ 1` in the formula in place of a dependent variable to assess whether there is a step change (or changes) in the time series:

```
# Obtain breakpoints estimate
breakpoints(US_co2_ts ~ 1)

     Optimal 2-segment partition:
Call:
breakpoints.formula(formula = us_co2_ts ~ 1)
Breakpoints at observation number:
96
Corresponding to breakdates:
2009(1)

# Plot the time series
plot(US_co2_ts, main=expression(paste("Breakpoint in Monthly US"
  ~CO[2]~Emissions)), ylab="Million Metric Tons", col="darkblue", lwd=1.5)
# Plot the line at the optimal breakpoint
lines(breakpoints(US_co2_ts ~ 1), col="darkgreen")
# Plot a 90% confidence interval
lines(confint(breakpoints(US_co2_ts ~ 1), level=0.90), col="darkgreen")
# Add breakpoint location text
text(2008.8, 555, "Jan 2009", cex=0.75, col="darkgreen", pos=4, font=3)
```

![breakpoints diagnostics](images/biwr_05_bps.png)



## Exploring relationships between time series: cross-correlation

Cross-correlation is extremely useful for helping identify leading and lagging indicators. The `ccf` function makes this easy. 

We'll use data from the US Bureau of Labor Statistics to demonstrate cross-correlation. 

```
# Load dplyr to subset the BLS data
require(dplyr) 

# Download CPI time series data from US BLS
CPI_xport = read.table("http://download.bls.gov/pub/time.series/cu/
    cu.data.14.USTransportation", header=T, sep="\t", strip.white=T)

# CPI for motor fuel, 2000-2014
fuel = filter(CPI_xport, series_id == "CUUR0000SETB" &
    year >= 2000 & year <= 2014 & period != "M13")
              
# CPI for airline fare, 2000-2014              
fare = filter(CPI_xport, series_id == "CUUR0000SETG01" &
    year >= 2000 & year <= 2014 & period != "M13")
```

Since ccf assumes that the time series detrended, we'll do that first. 

```
# Create time series
fuel_ts = ts(fuel$value, start=c(2000, 1), freq=12)
fare_ts = ts(fare$value, start=c(2000, 1), freq=12)

# CCF *requires* detrended data
# Get detrended values of each time series
fuel_detrend = na.omit(decompose(fuel_ts)$random)
fare_detrend = na.omit(decompose(fare_ts)$random)
```

An ACF plot shows pattern of each and both combinations for 18 months

```
acf(ts.union(fuel_detrend, fare_detrend))
```

![acf for ccf](images/acf_ccf.png)

You can combine these views into a single plot with the `ccf` function:

```
# Calculate CCF with +/- 4 year span
ccfvalues = ccf(fuel_detrend, fare_detrend, lag.max=48)
ccfvalues
```

![ccf](images/ccf_plot.png)

We can see that the maximum occurs just to the left of zero, we can infer that fuel costs lead airline fares slightly. Since it's hard to read values off a plot, we can use a small function can find the maximum correlation value:

```
# Function to find CCF
Max_CCF = function(a, b) {
    d = ccf(a, b, plot = FALSE, lag.max = length(a)-5)
    cor = d$acf[,,1]
    abscor = abs(d$acf[,,1])
    lag = d$lag[,,1]
    res = data.frame(cor,lag)
    absres = data.frame(abscor,lag)
    absres_max = res[which.max(absres$abscor),]
    return(absres_max)
}

# Determine maximum CCF value
Max_CCF(fuel_detrend, fare_detrend)
          cor         lag
163 0.5126287 -0.08333333
```

The maximum cross-correlation value is 0.51, which occurs between airline fares (at time 0) and fuel prices in the prior month (i.e., as 1/12 = 0.08333333).


## Basic forecasting

We saw a forecasting as a portion of the Chapter 1 example. We’ll explore the `forecast` package more detail in Part 2 of this book, but for sake of keeping ideas together, we'll replay that forecasting example here. An excellent companion to doing forecasting is understanding it: for that, we recommend Rob Hyndman’s excellent online text, *[Forecasting: Principles and Practice](https://www.otexts.org/fpp)*.

While this code is the same as in Chapter 1, to keep this chapter self-contained, here's the essential code to get the data and prep it for forecasting:

```
require(forecast)
require(dplyr)

# Download data and load into R
download.file("http://archive.ics.uci.edu/ml/machine-learning-databases/00235
    /household_power_consumption.zip", destfile = 
    "Data/household_power_consumption.zip")
unzip("Data/household_power_consumption.zip", exdir="Data")
power = read.table("Data/household_power_consumption.txt", sep=";", header=T, 
  na.strings=c("?",""), stringsAsFactors=FALSE)
  
# Convert and create date objects
power$Date = as.Date(power$Date, format="%d/%m/%Y")
power$Month = format(power$Date,"%Y-%m")
power$Month = as.Date(paste0(power$Month, "-01"))

# Interpolate missing values and summarize monthly total power use
power$Global_active_power_locf = na.locf(power$Global_active_power)
power_group = group_by(power, Month)
power_monthly = summarise(power_group, 
    Total_Use_kWh = sum(Global_active_power_locf)/60)

# Remove partial months from data frame
power_monthly = power_monthly[2:47,]

# Create a time series object of Total Usage
total_use_ts = ts(power_monthly$Total_Use_kWh, start=c(2007,1), frequency=12)
```

The main functinon of the `forecast` package is `forecast`; you can also be more specific by calling `forecast.ets` or `auto,arima` for explicitly forecasting the time series based on the expontential smoothing model family or ARMIA models, respectively.

Once you have a time series object, forecasting is a breeze:

```
# Automatically obtain the forecast for the next 6 months
total_use_fc = forecast(total_use_ts, h=6)

# View the forecast model results
summary(total_use_fc)
plot(total_use_fc)
```

![forecast model output](images/forecast_model_output.png)

![total_use_forecast](images/total_use_forecast.png)
